<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
<channel>
	<title>computers are bad</title>
	<link>https://computer.rip</link>
	<description>A newsletter on technology and its problems and also whatever else</description>

 	<item>
		<title>2024-12-04 operators on the front</title>
		<pubDate>Wed, 04 Dec 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-12-04-operators-on-the-front.html</link>
		<guid>https://computer.rip/2024-12-04-operators-on-the-front.html</guid>
		<description><![CDATA[ <p>At the very core of telephone history, there is the telephone operator. For a
lot of people, the vague understanding that an operator used to be involved is
the main thing they know about historic telephony. Of course, telephone
historians, as a group, tend to be much more inclined towards machinery than
people. This shows: websites with information on, say, TD-2, seldom tell you
much about the operators as people.</p>
<p>Fortunately, telephone operators have merited more than just a bit of
discussion in the social sciences. It was a major field of employment, many
ideas in management were tested out in the telephone companies, and moreover,
telephone operators were women.</p>
<p>It wasn't always that way. The first central exchange telephone system, where
you would meaningfully "place a call" to a specific person, arose in 1877. It
was the invention not of Bell, but of Edwin Holmes, a burglar alarm
entrepreneur. His experience with wiring burglar alarms back to central
stations for monitoring may have made the idea of a similarly-wired telephone
exchange more obvious to Holmes than to those in other industries. Holmes
initially staffed his telephone exchange the same way he had staffed his alarm
and telegraph businesses: with boys. The disposition of these young and
somewhat unruly staff members became problematic when they spoke direct with
customers, though, so Holmes tried something else. He hired Emma Nutt, the
first female telephone operator.</p>
<p>Women telephone operators were a hit. Holmes quickly hired more and customers
responded positively. The matter probably had little to do with their gender,
but rather with the cultural norms expected of young men and young women at the
time, but it takes a certain perspective to differentiate the two (for example,
it cannot be ignored that the switch from boys to women as telephone operators
also involved the other change implied by the terms: most women telephone
operators were hired as young adults, not at 12 or 13 as telegraph boys often
were). The way the decision was reported at the time, and sometimes still
today, is simply that women were better for the job: calmer, more friendly,
more professional, more obedient.</p>
<blockquote>
<p>With her extreme youth, her gentle voice, musical as the woodsy voices of a
summer day, her always friendly way of answering calls, she is a sensible
little thing, tranquilly serene through all the round of jollies, kicks and
nerve-racking experiences which are the result of a day's labor. She likes her
place, she knows her work, and she is prepared with quick-witted, instinctive
readiness for every emergency which comes her way. [1]</p>
</blockquote>
<p>Alexander Graham Bell was very much aware of the goings-on at Holmes' company,
which AT&amp;T would purchase in 1905. So the Holmes Telephone Despatch Co., the
first telephone exchange, became the pattern for many to follow. During the
last decades of the 19th century, the concept of exchange telephone service
rapidly spread, as did the role of the operator. Virtually all of these new
telephone workers were women, building a gender divide in the telephone
industry that would persist for as long as the operator.</p>
<p>Operators stood out not just for being women, but also for their constant
direct interaction with customers. To a telephone user, the operator was part
of the machine. The operator's diminished humanity was not unintentional.  The
early telephone industry was obsessed with the appearance of order and
reliability. The role of fallible humans in such a core part of the system
would undermine that appearance, and with the social mores of the time, the use
of women would do so even more. The telephone companies were quick to emphasize
to customers that operators were precisely trained, tightly managed, and a
model of efficiency. The virtues of telephone operators, as described by the
Bell companies, reflect the semi-mechanical nature of their professional
identities: a good operator was fast, precise, efficient, reliable.</p>
<p>Within the Bell System, new operators attended training schools that taught
both the technical skills of telephony (operation of the exchange, basic
understanding of telephone technology, et) and the behavior and manner expected
from operators. Operators were not expected to bring their personalities to the
workplace: they followed a detailed standard practice, and any deviation from
it would be seen as inefficiency. In many companies, they were identified by
number.</p>
<p>There was, of course, a tension underlying the role of the operator: operators
were women, chosen for their supposed subservience and then trained to follow
an exact procedure. At the same time, operators were women, in the workforce in
a time when female employment remained unusual. The job of telephone operator
was one of few respectable professions available to women in the late 19th and
early 20th centuries, alongside nursing. It seemed to attract the more
ambitious and independent-minded women, and modern studies have noted that
telephone operators were far more likely to be college-educated heads of
households than women in nearly any other field.</p>
<p>A full examination of telephone operators, their role in the normalization of
working women, the suffrage movement, and etc., would require a much better
education in the liberal arts than I have. Still, I plan to write a few
articles which will lend some humanity to the telephone industry's first, and
most important, switching system: first, I will tell you of a few particularly
famous telephone operators. Second, I plan to write on the technical details of
the work of operators, which will hopefully bring you to appreciate the unusual
and often very demanding career---and the women that took it up.</p>
<p>We will begin, then, with one of my favorite telephone operators: Susie Parks.
Parks grew up in Kirkland, Washington, at the very turn of the 20th century.
After a surprising amount of family relocation for the era, she found herself
in Columbus, New Mexico. At age 17, she met a soldier assigned to a nearby
camp, and they married. He purchased the town newspaper, and the two of them
worked together operating the press. Columbus was a small town, then as well as
now, and Parks wore multiple hats: she was also a telephone operator for the
Columbus Telephone Company.</p>
<p>The Columbus Telephone Company seems to have started as an association around
1914, when the first telephone line was extended from Deming to a series of
telephones located along the route and in Columbus itself. An exchange must
have been procured by 1915, when Henry Burton moved to Columbus to serve as the
young telephone company's full-time manager. Burton purchased land for the
construction of a new telephone office and brought on his sister as the first
operator.</p>
<p>Rural telephone companies were a world apart from the big-city exchanges of the
era. Many were staffed only during the day; emergency service was often
provided at night by dint of the manager living in a room of the telephone
office. Operators at these small exchanges had wide-ranging duties, not just
connecting calls but giving out the time, sending help to those experiencing
emergencies, and troubleshooting problematic lines.</p>
<p>By 1916, Susie Parks sat at a 75-line common battery manual exchange. Unlike
the long line multiple boards used at larger exchanges, this one was compact, a
single cabinet. When a nearby lumberyard burned in January and the fire damaged
the telephone office, Parks stepped in with a handy solution: the telephone
exchange was temporarily moved to the newspaper office, where she lived.</p>
<p>The temporary relocation of the exchange would prove fortuitous. Unknown to
Parks and everyone else, in February or March, Mexican revolutionary Pancho
Villa sent spies into Columbus. His army was in a weakened state, traveling
between temporary camps in northern Mexico and attempting to gather the
supplies to resume their campaign against the Federal forces of President
Carranza.</p>
<p>The exact reason for Villa's attack on Columbus remains disputed; perhaps they
hoped to capture US Army weaponry from the nearby fort or perhaps they intended
to destroy an ammunition depot to deter US advance into Mexican territory. We
also don't know if Villa directed his spies to locate the communications
facilities in Columbus, but it's said that they failed to identify the
telephone exchange because of its temporary relocation. The spies were
evidently not that good at their jobs anyway, as they significantly
under counted the number of US infantry stationed at Columbus.</p>
<p>On March 9th, 1916, Villa's army mounted what might be considered the most
recent land invasion of the United States [2]. Almost 500 of Villa's men moved
into downtown Columbus in the early morning, setting fire to buildings and
looting homes. Susie Parks awoke to screams, gunfire, and the glow of a burning
town. A day before, her husband had left town for the homestead the two worked.
Parks was alone with their baby, and bullets flew through the modest building.</p>
<p>At the nearby infantry camp, two machine gun units came together to mount a
hasty defense. While much more formidable than Villa had believed, they were
nonetheless outnumbered and caught off guard, some of them barefoot as they
advanced towards town with a few M1909s.</p>
<p>Susie Parks was barefoot, too, as gunfire shattered a window of the newspaper
office. Keeping her head down, she maneuvered in the dark, knowing that a light
would no doubt attract the attention of the raiders. Parks found her way to the
exchange and, cord in hand, tried their few long distance leads. El Paso was no
good: Villa's forces had cut the line. The line to the north, though, to
Deming, had escaped damage. The Deming operator must have had her own fright as
Parks described the violence around her. In short order, the message was passed
to Captain A. W. Brock of the National Guard.</p>
<p>Somewhere along the way, a bullet or at least a fragment hit her in the throat.
Unsure if she would survive, she hid her baby under the bed. According to most
accounts, she stayed with the switchboard, keeping a low profile until the
battle ended. According to her son, in an obituary, she took up a rifle of her
own and made way for the Army camp. I suspect there are elements of the truth
in both: she probably did get a gun, but I think she was more intending to
defend the baby than the soldiers, who were apparently able to take care of
themselves.</p>
<p>The Battle of Columbus ended as quickly as it began, and the exact order of
events is told in different ways. Villa may have already given the order to
retreat, seeing his substantial losses against the increasingly organized
machine gunners from the Columbus camp. In a version more complimentary to
our hero, it was the arrival of Brock's company, spotted coming into town,
that lead to the withdrawal. In any case, the sunrise appearance of the
National Guard in Columbus decisively ended the invasion.</p>
<p>It began a series of campaigns against Villa, culminating in the assignment of
General John Pershing to oversee a six-month "Punitive Expedition." They didn't
find Villa, but they did prove out the use of air support and truck transport
for a wide-ranging expedition through northern Mexico. The experience gathered
in the expedition would be invaluable in the First World War soon to follow.</p>
<p>Susie Parks is remembered as a hero. Charlotte Prince, a former first lady of
the New Mexico Territory, and the Daughters of the American Revolution presented
her with a gold watch and silverware set at a celebration in Columbus's small
theater. General Pershing, on his arrival to begin the Punitive Campaign, paid
her a visit to commend her for keeping her post through a raging battle [3].</p>
<p>The original Columbus telephone exchange, and other memorabilia of the Columbus
Telephone Company and Susie Parks, are on display in the top floor of the
Telephone Pioneer Museum of New Mexico.</p>
<p>Parks had set a high standard for her fellow telephone operators, not just in
New Mexico but beyond.</p>
<p>The next year, the United States would enter the First World War. Major General
Fred Funston, an accomplished military leader and veteran of the
Spanish-American War, was favored to lead the US Army into Europe. By bad luck,
he died of a heart attack just a couple of months before the declaration of
war. Funston was replaced by the General he had sent into Mexico. John Pershing
traveled to France as commander of the American Expeditionary Forces.</p>
<p>Upon Pershing's arrival, he found Europe in disarray. Communications in France
were tremendously more difficult than the high standard the Army maintained at
home. There were both technical and organizational challenges: telephone lines
and exchanges had been damaged by fighting, and the Army Signal Corps lacked
the personnel to improve service.</p>
<p>The idea to dispatch American telephone operators to Europe likely originated
in the Army Signal Corps and AT&amp;T, with whom they already maintained a close
relationship. But I like to think that Pershing remembered the bravery of Susie
Parks when he signed on to the plan, cabling the US to send send "a force of
Woman telephone operators."</p>
<p>At the time, women had been admitted to the military only as nurses, and those
nurses were kept far from the front. There was substantial doubt about the
fortitude of these women, especially as they would be called on to staff
exchanges near combat. The Secretary of War allowed the plan to go forward only
on the condition that men would be hired preferentially and women would be
carefully selected and closely supervised.</p>
<p>Operators were selected in by the Army Signal Corps in cooperation with AT&amp;T.
It was initially thought that they would be found among the staff of the many
Bell operating companies, but the practicalities of the AEF (which was
headquartered and primarily fought in France in collaboration with French
units) required that operators speak both French and English fluently. There
were few French-speaking telephone operators, so AT&amp;T expanded their search,
hiring women with no telephone experience as long as they were fluent in French
and passed AT&amp;T's standardized testing process for telephone proficiency.
These recruits were sent to the Bell System's operator training schools, and
all selectees attended the Army Signal Corps' training center at what is now
Fort Meade.</p>
<p>The first unit of the Signal Corps Female Telephone Operators Unit [4]
consisted of 33 operators under the leadership of Chief Operator Grace Banker,
who had learned French at Barnard College before finding work at AT&amp;T as an
instructor in an operator training school. Their 1918 journey to France was a
long and difficult one, as transport ships were in short supply early in the
war and subject to German attack. The ferry crossing of the English Channel,
not a long voyage by any means, turned into a 48-hour ordeal as the ship was
stuck in dense fog in a vulnerable position. Despite the cold and damp
conditions, the operators waited two days on deck in preparation to take to the
life boats if necessary. Two men on the ship died; at one point French forces
mistook its faint outline for an attacker and surrounded it. As Banker would
tell the story later, her operators were in good spirits.</p>
<p>Their cheerful disposition in the face of the harsh journey served as good
preparation for the conditions the operators faced in the field. There was
hardly a barracks or telephone exchange for the women that wasn't plagued by
leaks, rats, fleas, or disarray as the AEF scrambled to find facilities for
their use. The simple mechanics of the telephone system required that exchanges
be located fairly close to concentrations of command staff and, thus, fairly
close to the fighting. The operators were constantly in motion, moving from
camp to camp, and ever closer to the front.</p>
<p>Banker's first unit of 33 women quickly proved themselves invaluable, providing
faster and more reliable telephone service as they leveraged their French to
handle all allied traffic and developed directories and route guides to keep up
with the rapid work of the Signal Corps' men in building out new telephone
lines. The Female Telephone Operators had proven themselves, and Pershing
called for more. A few months later, hundreds more were in France or on their
way. Despite the War Department's concern about the willingness of women to
work in wartime conditions, telephone operators turned out to be as ready to
fight as anyone: when AT&amp;T solicited applications from among the Bell
companies, they were swamped by thousands of postcard forms.</p>
<p>While some sectors the military were clear that the women operators were
brought to Europe for their technical proficiency, there remained a clear
resistance to recognition of their work as part of the military art. "Even
telephone operators were persistently told that their presence and their
girlish American voices would benefit the war effort by comforting home-sick
soldiers and lifting their morale" [5]. The operators were, at times, regarded
in the same stead as the women "morale volunteers" fielded by organizations
like the YWCA.</p>
<p>The military was so quick to categorize them as such that, shortly after their
arrival, the YWCA was made responsible for their care. Operators were
accompanied by YWCA chaperones, furnished to protect their moral virtues from
the soldiers they worked alongside. Despite their long shifts at the exchanges,
the YWCA expected them to attend military dances and keep up appearances at
social functions. Many of the women associated with the AEF, telephone
operators and nurses alike, took to cutting their hair short---no doubt a
practical decision given the poor housing and inconsistent access to washrooms,
but one that generated complaints from the Army and the YWCA.</p>
<p>In September of 1918, the AEF and French troops---a quarter million men in
all---took on their first great offensive. The logistics of supporting and
organizing such a large fighting force proved formidable, and the Signal Corps
relied on the telephone to coordinate a coherent assault. The thunder of
artillery was heard over the chatter of telephone calls. For the duration of
the offensive, a system of field phones and hastily laid long-distance
connections, known as the "fighting lines," fell under the control of Grace
Banker and five operators she hand-picked to move up to the front with her.
They donned helmets and coats, toted gas masks, and took up their positions at
temporary exchanges, some of them in trenches. Infantry orders, emergency calls
for supply, and even artillery fire control passed through their plugboards as
the allies took Saint-Mihiel.</p>
<p>As a reward, they moved forward once again, taking up a new "telephone office"
at the allied advance headquarters in Bar-le-Duc. There, they camped in old
French army buildings and weathered German bombing as they provided 24/7
telephone service for the Meuse-Argonne offensive. Military service was
demanding, but still subject to the "scientific management" trend of the time
and the particular doctrine of the Bell System. Their long shifts were
carefully supervised, subject to performance evaluations and numerical scoring.
There was a certain subtext that the women operators had to perform better than
the Signal Corps' men who they had replaced.</p>
<p>Fighting ended in November of 1918, although many of the operators were
assigned to various post-war duties in Europe (including Grace Banker's
assignment to the French residence of President Wilson) during 1919. The first
33 operators had spent 20 months in France before they returned to the United
States, where Banker would complain of the low stakes of civilian work.</p>
<p>After the war, the Female Telephone Operators received numerous commendations.
Major General H. L. Rogers of the Signal Corps spoke of their efficiency and
the quality of the telephone service under their watch. The Chief Signal
Officer reported that "a large part of the success of the communications of
this Army is due to... a competent staff of women operators." Pershing
personally signed letters of commendation to a number of the operators,
referring to their "exceptionally meritorious and conspicuous services."
Operators who had worked near the front received ribbons and clasps for their
involvement in the offensives. Grace Banker, for her own part, was awarded the
Army's Distinguished Service Medal. Of 16,000 officers of the Signal Corps in
the First World War, only 18 received such an honor.</p>
<p>Considering the decorations these women wore on their Signal Corps jackets as
they returned to the United States, it is no wonder that modern accounts often
style them as the "first women soldiers." The female nurses of the Red Cross,
while far more numerous, were never as close to the front or as involved in
combat operations as the operators. The operators were unique in the extent to
which they considered themselves---and they were often seen by others---to be
members of the Army.</p>
<p>After the war, they would learn at the same time as many of their commanding
officers they were not. Earlier, the Army had quietly determined them to be
contracted civilian employees. None other than General Pershing himself had
ordered them to be inducted into to the army in his original letter to
headquarters, and recruiting materials explicitly used the terms "enlistment"
and "regular Army," even introducing the term "women soldiers." But even before
the first 33 shipped out for France, Army legal counsel had determined that
military code prohibited the involvement of women. None of the women were told;
instead, they were issued uniforms.</p>
<p>450 members of the Female Telephone Operators Unit worked 12-hour shifts,
handling 150,000 telephone calls per day, often not only making connections but
serving as interpreters between French and American officers. The Signal Corps'
male telephone operators, more experienced in the Army, were of such noticeably
poorer performance that they were restricted to night shifts---and even then,
only in safe territory well behind the front. Two operators, Corah Bartlett and
Inez Crittenden, died in the service of the United States and were buried in
France with military honors. Years later, it was noted that because of their
critical role in military logistics, the operators were among the first
Americans to reach the combat theater and among the last to leave.</p>
<p>They were discharged as civilians---or rather, they were not discharged at all.
Because of the Army's legal determination, the women received no Army papers
and were deemed ineligible for veteran's benefits or even to receive the
Victory Medal which the Signal Corps had promised them.</p>
<p>Despite its recognition of their exceptional service, the military was slow to
admit women's role outside of wartime exigency, or even in it. The United
States as a whole was even slower to recognize the work of the telephone
operators. Despite the introduction of 24 bills to congress, starting in 1927,
it was not until 1977 that the operators were declared regular members of the
Army and granted military benefits. By the time the act was put into effect in
1979, only 33 operators lived to receive their discharge papers and the Victory
Medal.</p>
<p>AEF telephone operator Olive Shaw, who tirelessly lobbied for military
recognition of her fellow women, was the first burial at the new Massachusetts
National Cemetery in 1980. Her wartime uniform, fitted as always with the brass
devices of the Signal Corps and the letters "U.S.," was presented to congress
as evidence of their rightful role as veterans in 1977 and cited again, in
2024, when all of the members of the Army Signal Corps Female Telephone
Operators Unit were awarded the Congressional Gold Medal. It is now on display
at the National World War I Museum in Kansas City.</p>
<p>The Female Telephone Operators Unit laid the groundwork for the induction of
women during World War II---the Women's Army Auxiliary Corps and the United
States Navy's Women's Reserve, or WAVES, which is remembered today for its
exceptional contributions in the fields of cryptography and computer science.
It is fitting, of course, that the achievements of the WAVES would be
exemplified by another Grace, Rear Admiral Grace Hopper.</p>
<blockquote>
<p>"Women's work," far from being frivolous, was now defined as essential to the
war effort, and the U.S. military found itself in the uncomfortable position
of being dependent on female labor to meet the structural needs of the war
economy. Ironically, then, it was the logic of sex segregation in the
civilian economy that compelled the U.S. government to grant women entry into
the armed services, the ultimate masculine preserve. [5]</p>
</blockquote>
<p>[1] "A Study of the Telephone Girl," <em>Telephony</em> magazine (1905).</p>
<p>[2] A 1918 conflict at Nogales, AZ, involving similar combatants, might also
lay claim to that description. I will argue in favor of the Battle of Columbus,
which was an unprovoked invasion, as compared to the Battle of Ambos Nogales
which was more of a border security conflict in reaction to years of rising
tensions.</p>
<p>[3] Parks was an interesting figure for the rest of her life. She and her
husband continued to move around, buying the <em>Clackamas News</em> in Oregon. Her
husband's condition declined, a result of surgical complications and a morphine
addiction, and they split up. During the Second World War, Parks found herself
back in wartime service, as a sheet metal worker at the Seattle-Tacoma
Shipbuilding Company. In 1981, the <em>Deming Headlight</em>, closest newspaper to
Columbus, reprinted her obituary from the <em>Seattle Post-Intelligencer</em>. It
recounts a half dozen careers, two husbands, and 36 grandchildren.</p>
<p>[4] The members of the Female Telephone Operators Unit are frequently referred
to as the "hello girls," but this is a more generic term for telephone
operators that would also come to refer to other groups, be used as the title
of works about telephone operators, etc. I prefer to stick to something a
little more precise.</p>
<p>[5] Susan Zeiger, "In Uncle Sam's Service: Women Workers with the American
Expeditionary Force, 1917-1919" (2019).</p> ]]></description>
	</item>

 	<item>
		<title>2024-11-23 cablesoft</title>
		<pubDate>Sat, 23 Nov 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-11-23-cablesoft.html</link>
		<guid>https://computer.rip/2024-11-23-cablesoft.html</guid>
		<description><![CDATA[ <p>As an American, I often feel an intense jealousy of Ceefax, one of several
commercially successful teletext services in the UK and Europe. "Teletext" is
sometimes a confusing term because of its apparent relation to telecom-industry
technologies like the teletypewriter and telegram, but it refers specifically
to a "broadcast text" technology usually operated over TV networks. Teletext
could be considered the first form of "interactive television," a reimagining
of traditional television as a more WWW-like service that allows viewers to
navigate through information and retrieve content on-demand.</p>
<p>Despite many, many attempts, interactive television was never particularly
successful in the US. Nor, I believe, did it fare well in Europe after the
retirement of teletext. It was an artifact of a specific time and place; once
PC ownership and internet access expanded they handily filled the niche of
interactive text. That feels a little surprising, televisions being a big
screen that so many consumers already had in their home, but offerings like MSN
TV sucked to use compared to PCs. The technology for interacting with PC
software from a couch honestly still isn't quite there [1], and it was even
worse in the '90s.</p>
<p>Despite its general failure to launch, Interactive TV was, for a time, a big
field with big ideas. For example, you've heard of MPEG-4, but what about
MHEG-5? That's the <em>Multimedia Hypermedia</em> Expert Group's effort towards an
object-oriented, television-native hypermedia environment, and it's exactly as
terrible and fascinating as that description would lead you to believe. But I'm
not going to talk about that today. Here's what's on my mind: what if I told
you that MSN TV was Microsoft's <em>second</em> attempt at interactive television?</p>
<p>In 1994, Microsoft formed a partnership with two cable television carriers to
launch Cablesoft. It was such a big hit that Microsoft spent most of its brief
life trying not to talk about it.</p>
<p>You might remember the days when a television with a standard QAM tuner could
often pick up on-demand content being watched by other people. And that's the
basic interactive television model at work: as a cable customer with on-demand
features, your STB presents a set of menus to select a program. When you start
it, a video server at the head end selects an unused digital television channel
and plays back the content on it. Your STB is sent a command to tune to the
correct channel, and all of your playback control actions (play/pause etc) are
sent to the video server. Today the video downlink is always encrypted, but in
the heyday of CATV on-demand encryption was inconsistent and some providers
didn't use it at all, leaving these downlink channels viewable by anyone with a
tuner not configured to hide them.</p>
<p>On-demand isn't particularly exciting, and perhaps only barely counts as
"interactive television," but it is the most substantial thing to come out of
US efforts. The late '80s and early '90s saw plenty of interactive television
ideas, which tended to envision the TV as the main way consumers would get
information. Because of the limitations of the technology, this exciting world
was mostly text. Text on the TV. Nothing would really catch on, even to the
trial phase, until interactive TV started to become synonymous with multimedia.</p>
<p>The 1990s were an exciting time in computer multimedia. Larger storage devices
(namely CD-ROMs), faster processors, and better display controllers all meant
that computers were becoming more and more practical for audio and video. In
the video game industry, the period famously lead to a surge of "full motion
video" (FMV) games that used live-action cutscenes or elaborate pre-rendered 3D
scenes. Of course, most enthusiasts of video game history also wince at the
thought of figuring out which exact version of Apple QuickTime will work right
with any given title.</p>
<p>Despite its surging popularity, computer multimedia was also in its infancy.
Audio and video encoding were dominated by proprietary systems like QuickTime
and RealMedia. SOver time, these products and their underlying codecs would
largely converge into the relatively consistent and consumer-friendly ecosystem
of media formats we use today (i.e. everything is MPEG and consumers don't care
about the rest, mostly, as long as they don't want h.265 in a web browser on
Linux or something wild like that).</p>
<p>Some of that convergence happened because of vendors actively contributing to
standardization and promoting licensing pools, but some of it also happened
because one of the biggest players in the PC software industry saw Apple's
success with QuickTime and didn't want to fall behind. Microsoft developed a
major focus on multimedia, leading to their own family of codecs, containers,
and protocols, some of which remain in common use today. What's more, Microsoft
had a long-running fascination with the television distribution industry, which
it tended to view as the future of media delivery due to its very high capacity
compared to telephone lines. Microsoft itself, and its executives as
individuals, had a variety of interests in cable TV starting in the '90s.
Perhaps most prominently, Paul Allen was controlling owner of Charter for a
decade, and Microsoft invested a billion dollars in Comcast in 1997 to support
their effort to pivot towards data.</p>
<p>So, the application of Microsoft technology to cable television was inevitable.
Microsoft brought Tele-Communications Inc (TCI, now part of Comcast) and Time
Warner (somehow not part of Comcast <em>yet</em>) on board as CATV partners and set
about building Microsoft Media for CATV. Or, perhaps, TCI and Time Warner
formed a joint initiative to develop an interactive TV platform and selected
Microsoft as a partner. The history is a little fuzzy, but somehow, these three
companies ended up in high-level talks about a new, standard platform for
technology-enabled TV. Cablesoft, as they called it, would include an
electronic program guide, TV shopping, and, most importantly, on-demand
streaming media.</p>
<p>I'm trying not to say this over and over again, but Microsoft and Bill Gates
and Paul Allen were all kind of obsessed with streaming media and on-demand
delivery in the 1990s. It's hard to keep track of all the failed ventures they
either launched or invested in, there were several each year. If you read into
the history of the TV distribution industry Paul Allen especially just keeps
popping up in weird places. It's fascinating to me because our modern
experience shows that they were very much right, in that on-demand streaming
delivery via computers would become the dominant way media is distributed. But
they were also pathologically ahead of their time; Paul Allen was basically
trying to do Netflix in 1993 and all of these efforts just <em>sucked.</em> The
infrastructure was simply not there and the many companies trying to build it
tripped over each other as often as they made progress.</p>
<p>To be fair, Microsoft was not the only faction making repeated stabs at
streaming media, and by 1994 investors were already starting to tire of it. A
1994 <em>News Tribune</em> (Tacoma) article on Cablesoft's announcement captures the
attitude with this spectacular quote from industry newsletter editor Denise
Caruso: "Anybody who bothers to get excited about another interactive TV trial
at this point deserves everything they get in terms of disappointment." In
1994! All These Goddamned Streaming Services is a complaint pretty much as old
as computer multimedia. I wonder what Denise Caruso would have to say about
Tubi.</p>
<p>As you know, things didn't get a whole lot better. In 1996, the <em>Boston
Globe's</em> "FastTrack" technology column began: "Interactive television is the
Loch Ness Monster of the information age---much talked about but rarely seen."
And it's not hard to find these quotes, those are like the first two search
results in the archive I use. The consumer internet was barely a thing and
industry commentators were already rolling their eyes at each new streaming
service.</p>
<p>The difference, of course, is this: back in the '90s, these streaming
multimedia efforts were collapsing fast, generally before they signed up actual
consumers. Now they collapse very slowly, after producing about a hundred
original TV series that none of us will ever hear of. Say what you will of
Cablesoft, at least they didn't make <em>Tall Girl</em> and <em>Tall Girl 2.</em></p>
<p>So what did they make? Look, Cablesoft didn't get very far, and there's not a
lot of historical information about them. You have to be careful not to confuse
Cablesoft with CableSoft, a completely separate company that was working on the
exact same thing at pretty much the same time (CableSoft had spun off of
television technology giant General Instrument and thus had a considerable
advantage, but it didn't work out for them either) [2].</p>
<p>By early 1994, Microsoft was already involved in other interactive TV ventures,
leading to a somewhat critical interview of future Microsoft CTO and cookbook
author Nathan Myhrvold by the <em>Seattle Post-Intelligencer.</em> "In the long run,"
he opined, "it's very likely there will be some form of a smart TV... it's not
very input intensive, you don't have a keyboard for your TV." 100% correct!</p>
<p>But then the interviewer, Jim Erickson, asks something along the line of
"what's with these three different interactive TV things that Microsoft is
doing at once?" Myhrvold answers that "there is more uniformity and more
synergy than may meet the eye with the series of things that we have done so
far," which sounds like a comedy sketch of a Google exec explaining the
difference between Duo and Allo. Erickson digs a little deeper, asking what's
going on with Cablesoft, prompting Myhrvold to say "it's a funny thing to give
status on something you never announced and never admitted to." And that is a
very interesting response indeed.</p>
<p>The Wikipedia article is an absolute stub, giving us just one tantalizing
factoid that has my practically foaming at the mouth: "...a custom version of
the Windows NT operating system known as NTAS, which was essentially a series
of fine-tuning efforts to drive ATM switches." We'll get back to that. But the
Wikipedia article also says that Cablesoft was announced in 1994, which isn't
wrong, but is a little misleading. As far as I can tell, Microsoft "announced"
Cablesoft in March 1994 only under duress. Rumors of Cablesoft started to swirl
about nine months earlier, in 1993, and the media did not look on it very
positively. The most widely published article quoted then-chairman of Apple
John Sculley accusing Microsoft of an anticompetitive move to corner the
interactive TV market.</p>
<p>There is, of course, nothing more quintessentially Microsoft than an
anticompetitive move to capture a market that would never actually emerge.</p>
<p>The first widespread mention of Cablesoft running under the headline "Big
Software-Cable Deal Criticized" does a bit to explain Microsoft's odd cageyness
about Cablesoft, repeatedly denying that any final deal had been signed and
even downplaying the likelihood of the product launching. TCI and Time Warner
refused to talk about it. Charmingly, a Phil Rowe of Battle Creek, Michigan
wrote in to the editor of the <em>Battle Creek Enquirer</em> that Microsoft, TCI, and
AT&amp;T (I think Rowe was just confused? AT&amp;T had its own interactive television
efforts going on) would soon monopolize the interactive TV market, and that to
hold them off, Battle Creek should swiftly franchise <a href="https://computer.rip/2024-10-26-buy-payphones-and-retire.html">wireless
cable</a>.</p>
<p>It seems that Cablesoft died under the same cloud that it emerged. No one is
really that clear on what happened. A trial program apparently launched where
TCI and Microsoft employees in the Puget Sound area could try it out. It must
not have lasted very long, by 1995 an article about Microsoft's antitrust woes
listed Cablesoft as one of the ventures that Microsoft had abandoned due to the
scrutiny. "Everyone backed off," an anonymous Microsoft employee told another
reporter. "They were all afraid that this thing would be regulated out of
existence."</p>
<p>Cablesoft didn't make much of a contribution to the business, but was it
technically significant? And what about that customized version of Windows NT?
Denise Caruso comes up again: an archived version of her personal website is
the Wikipedia article's main source. She wrote:</p>
<blockquote>
<p>Code-named Tiger and now called the Microsoft Media Server, the innovative
design is based on a version of the Windows NT operating system, called NTAS,
that uses standard PCs and cutting-edge ATM (asynchronous transfer mode)
networking products to deliver video, audio, animation and information services
into the home.</p>
</blockquote>
<p>Streaming media was difficult in 1999; it was <em>very</em> difficult in 1993, when
Microsoft's efforts began. Hard disks were slow, and head contention meant that
it was very hard to serve multiple video streams from one disk. Networks were
slow and, worse, had high levels of latency and jitter compared to what we are
used to today. Feasibly providing real-time unicast streams to a large set of
users would require some sort of large, very high performance storage
system---or, in a strategic move that has repeatedly revolutionized the server
side, a lot of consumer hardware and a clever system of coordination.</p>
<p>Microsoft technical report 96-09 describes the Tiger Video Fileserver. It was
released after the Cablesoft project had faltered and never mentions it by
name, but it clearly describes the head-end equipment for an on-demand video
streaming system. Its authors, including Myhrvold, include a half dozen people
with long careers in distributed systems and high performance storage.</p>
<p>A Tiger fileserver consists of multiple consumer PCs running Windows NT. Each
of these nodes, called "cubs," has multiple hard disks. Files are separated
into blocks (64kB-1MB) which are distributed across disks in the cluster; there
are no constraints on the nature of the files except that they must be at the
same playback bitrate. This constraint exists because the entire Tiger system
operates on a synchronized timeslot schedule, consisting of block service times
which are equal to the time required to read one block from disk, plus some
margin for transient events and error recovery.</p>
<p>When a viewer requests a video, a controller node allocates the viewer to slots
in a schedule of block service times and cubs. This is done such that each
successive block of a given video will be handled by a different disk, and such
that no one disk will be needed by more than one viewer in a given block
service time. In other words, video playback consists of a series of cubs each
delivering a single block of the file in order, and each disk retrieves only
one block at a time. Because the block service time (and thus rate at which the
schedule is executed) is appreciably shorter than the time viewers spent
playing back back that same block, the cubs are able to support multiple
viewers and still deliver blocks on time.</p>
<p>Because disks were not fast enough to reliably perform two block reads within a
block service time, and aggregation of multiple viewers into one logical stream
remained an elusive challenge, Tiger used a simple hack to avoid noisy neighbor
problems: the controller ensured that there was only one viewer of a given file
at a given time index. In practice, if two user were to hit "play" on the same
movie at the exact same time, the controller would slightly delay the beginning
of streaming to one of the viewers in order to introduce a time offset.
Combined with the striping of each file across multiple nodes, this naturally
distributed load to allow a large number of simultaneous viewers of the same
media without having to create additional replicas of the media.</p>
<p>The controller determines and distributes the schedule in advance, and each cub
is permitted (and expected) to retrieve blocks early as its I/O allows. But
cubs are required to <em>send</em> that block in the correct schedule slot, so that
storage buffering occurs only at the cub level and the outgoing network stream
is in perfect realtime. When the file is distributed across cubs and disks,
extra copies of each block are stored for redundancy, in case of a disk or cub
failure. Extra slack in the block service time allows a failed block retrieval
to be moved to a different cub. Secondary blocks are allocated by organizing
the cubs into a ring. Each primary block has secondary copies stored on one or
more cubs "to the right," and each cub is responsible for monitoring the
liveness of its neighbor "to the left" and assuming its schedule if required.</p>
<p>The physical layout of files on each disk is optimized for fault load; the
"primary" copy of each block is stored on the (physical) outer part of the disk
surface while the inner part of the disk surface is used for secondary (backup)
copies. Because the outer surface of the disk moves physically faster, it can
be read more quickly. By placing primary blocks on the outer half, the disk's
normal "primary" workload runs at nearer the disk's maximum read speed, leaving
spare time for retrieval of secondary blocks when block sizes and bitrates are
optimized for the disk's average read speed. When primary blocks are lost due
to disk failure, they are automatically restored to a new disk as soon as one
is available.</p>
<p>Besides real-time streaming of files, Tiger also supported ad-hoc read and
write operations. These were performed on an opportunistic basis, sent to cubs
as "extra" jobs to execute when they were ahead on carrying out scheduled
reads. When viewers fast-forwarded or rewound playback, these opportunistic
jobs were used to jump-start playback at other points in the file, with the
caveat of reduced reliability.</p>
<p>During the course of normal video playback, individual blocks will come from an
arbitrary sequence of different cubs. There are several approaches to the
network design, and Tiger supports UDP over both Ethernet and ATM, but ATM is
preferred.  ATM is Asynchronous Transfer Mode, a network protocol that
originated in the telephone industry as part of the ISDN stack. Unlike
Ethernet, ATM was designed for real-time data streams, and uses prescheduled
time-division muxing to provide guaranteed-bandwidth virtual circuits over a
switched fabric. This made ATM inherently more suited to streaming media than
Ethernet, a difference that Ethernet only made up for with quality of service
protocols and, mostly, just getting to be so fast that streaming media mostly
worked out despite having only intermittent, opportunistic access to the
network media.</p>
<p>Microsoft further enhanced ATM for the Tiger application by introducing the ATM
"funnel," a multipoint-to-point networking mode that allows many cubs to send
packets into a single virtual circuit. ATM subdivides packets into multiple
frames, meaning that if two cubs were to send packets too close together, they
may become interleaved (both violating the design of IP-over-ATM and
complicating the work of the viewer). To resolve this problem, Tiger uses a
token-passing scheme where each cub transmits its block and then passes a token
to the next cub in the schedule for that viewer. The implementation of this
token-passing ATM variant is one of two customizations to the NT kernel involved
in Tiger.</p>
<p>The other will be familiar to readers in the modern high-performance networking
industry: Tiger implemented a basic form of kernel-bypass networking in which
the network interface read the file block directly from the read buffer via
DMA. Tiger thus required some special kernel-mode code to implement this
DMA-to-UDP mode and ensure that video data passed over the bus only twice, once
from the disk controller to memory, and once from memory to the network
controller.</p>
<p>These kernel features, which I believe were less modifications than device
drivers, seem to be the "customizations" that Caruso referred to. To my
frustration, the connection to the term "NTAS" seems to be mistaken. I cannot
find any instance of Microsoft using it in relation to Cablesoft or Tiger.
Most likely it arose from confusion with the branding of NT 3.1's server
edition as NT Advanced Server; NT 3.1 must have been the basis for the Tiger
system since it was released in 1993 and had considerable emphasis on
performance as a network file server to compete with Netware.</p>
<p>The technical report describes a model Tiger deployment: Five Gateway Pentium
133MHz machines served as cubs, each with 48MB of RAM, three 2GB Seagate
drives, and an OC-3 ATM adapter. Larger than usual 2KB sectors were used on the
hard disks for better throughput when handling large files (this is an
interesting detail since support for non-default sector sizes was apparently
rather cantankerous in the PCs and storage interfaces of the time). A Gateway
486/66 machine served as the controller for the cluster. Ten 486/66s, each
attached to the ATM network by 100Mbps fiber, served as test clients.</p>
<p>The controller, on the other hand, used 10Mbps ethernet to communicate with the
five cubs, while the cubs communicated among themselves using the ATM network.
The paper explains this by noting that the 486-based controller was very slow
compared to the Pentium-based cubs, perhaps no benefit was seen in the added
cost of extending the ATM network to the controller.</p>
<p>This system, with a total of 15 data drives, stored a little over 6 hours of
media at 6Mbps. 0.75MB blocks were used, for a block playback time of roughly
one second. Based on calculations, the OC-3 interfaces were the bottleneck of
the system, allowing the five cubs to provide a total of 68 simultaneous
streams. The 486-based viewer machines actually weren't fast enough to decode
that many streams, so some requested videos and simply discarded the packets,
while others actually checked the received blocks for correctness. Based on
this sampling method, a lost or late block rate of only 0.02% was observed.
Performance data collected from the cubs indicated that, with faster network
controllers, they would have kept up with additional viewers.</p>
<p>Despite Cablesoft's failure to ever reach the market, Tiger appears to have
included novel work in several areas: scheduling real-time demand across a
cluster, distributed storage with striping and replication for fault-tolerance
and performance. At the bottom line, Tiger demonstrated the use of a cluster of
commodity PCs to perform a task that, at the time, was seen as requiring
specialized and costly high-performance machines from SGI or Oracle.</p>
<p>In the end, it wasn't enough. Caruso again:</p>
<blockquote>
<p>Unlike the PC business, where it has ultimate leverage over all the links of a
relatively short chain, Microsoft has no native influence in the large and
existing infrastructures it wants to penetrate (cable networks, telcos and
content providers)---except that it is Microsoft.     </p>
</blockquote>
<p>Cablesoft ended so quietly that we will probably never know for sure, but I
think that Caruso's larger argument in her article about Cablesoft is right:
licensing software for STBs and cable headends would never bring in Bill Gates
money. Microsoft's real play was for revenue share: wrestling matches and hotel
pornography had proven that on-demand content could move a lot of money, and as
the operator of the technical platform, Microsoft stood to impose royalties.
5% of pay-per-view revenue <em>is</em> the kind of thing that motivates Microsoft.
When Microsoft went to major cable operators to try to standardize them on its
platform, the cable oligopoly saw the software monopoly coming in for a slice
of the pie. They probably thought they could do it just fine on their own.</p>
<p>And they turned out to be right. Interactive TV would die off as a buzzword in
the US market, but not that much later, in the '00s, General Instrument
successor Motorola would partner with the manufacturers of some of those
specialized costly machines and quietly introduce video on demand to the
American cable consumer as a standard offering of digital TV packages.</p>
<p>I think they met more success because they didn't try as hard: '00s on-demand
infrastructure involved some formidable machines, like the Broadbus (later
Motorola) B-1 that served thousands of simultaneous video streams by storing
the entire active video library in RAM. But no one was talking about hypertext
or smart TVs or the TV as the center of digital family life. The aspects of
interactive TV that were familiar to the television industry, electronic
program guides and more convenient pay-per-view, did just fine once they became
technically feasible.</p>
<p>What of Tiger?</p>
<p>As part of NT4 in 1996, Microsoft introduced NetShow. NetShow would later be
known as Microsoft Media Server, and then Windows Media Server, before fading
into obscurity. It was a real-time media server that used a proprietary
protocol to deliver Advanced Streaming Format media, competing directly with
streaming pioneer RealNetworks. The details are fuzzy enough that I have a hard
time saying this for sure, and Tiger as a distributed system definitely didn't
make it past Cablesoft, but it does seem very likely that NetShow is a
descendent of Tiger. Over time, the Windows Media family moved over to industry
standard protocol RTSP, and then the streaming server was merged into IIS.
Perhaps nothing of Tiger survived into the 21st century. But, you know how
Microsoft is. Maybe there's a bit of Tiger inside of Windows to this day.</p>
<p>[1] As a dedicated computer-on-TV person, I use a ThinkPad Trackpoint Keyboard
II as the "remote control." It's the best I've found so far but still large and
clunky compared to a traditional remote. More recently, my husband also added
an HDMI-CEC adapter that allows the TV remote to control the computer via a
daemon that generates keyboard events. This is pretty slick for applications
like Plex and Steam Big Picture that were designed with television use in mind,
but in the web browser the experience leaves much to be desired. We're
basically facing all the same struggles as similar dweebs did thirty years ago.</p>
<p>[2] And neither should be confused with Cablesoft Ltd., an English company that
developed software for the telephone and television industry during the same
years, or CableSoft, a company that made a stock quotation and analytics
product a couple of years later.</p> ]]></description>
	</item>

 	<item>
		<title>2024-11-09 iron mountain atomic storage</title>
		<pubDate>Sat, 09 Nov 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-11-09-iron-mountain-atomic-storage.html</link>
		<guid>https://computer.rip/2024-11-09-iron-mountain-atomic-storage.html</guid>
		<description><![CDATA[ <p>I have quipped before about "underground datacenters," and how they never
succeed. During the late decades of the Cold War and even into the '00s, the
military and (to a lesser extent) the telecommunications industry parted ways
with a great number of underground facilities. Missile silos, command bunkers,
and hardened telephone exchanges were all sold to the highest bidder or---often
in the case of missile silos---offered at a fixed price to the surrounding land
owner. Many of them ended up sealed, the new owner only being interested in the
surface property. But others...</p>
<p>There are numerous examples of ex-defense facilities with more ambitious
owners. There ought to be some commercial interest in a hardened, underground
facility, right? After all, the investment to build them was substantial.
Perhaps a data center?</p>
<p>There are several ways this goes wrong. First, there are not actually that many
data center clients who will pay extra to put their equipment underground.
That's not really how modern disaster recovery plans work. Second, and probably
more damning, these ventures often fail to anticipate the enormous cost of
renovating an underground facility. Every type of construction is more
expensive when you do it underground, and hardened facilities have thick,
reinforced concrete walls that are difficult to penetrate. Modernizing a former
hardened telecom site or, even worse, missile site for data center use will
likely cost more than constructing a brand new one. Indeed, the military knows
this: that's why they just sold them, often at rock-bottom prices.</p>
<p>Even if these "secure datacenters" almost never succeed (and rarely even make
it to a first paying client), they've provided a lot of stories over the years.
CyberBunker, one of the less usual European examples (a former NATO  facility),
managed to become entangled in cybercrime and the largest DDoS attack ever
observed at the time, all while claiming to be an independent nation. They were
also manufacturing MDMA, and probably lying about most of the equipment being
in a hardened facility to begin with.</p>
<p>So that's obviously a rather extreme example, sort of a case study in the
stranger corners of former military real estate and internet crime. But just
here in New Mexico I know of at least two efforts to adopt Atlas silos as
secure datacenters or document storage facilities, neither of which got off the
ground (or under the ground, as it were). It seems like a good idea until, you
know, you actually think about it. You might recall that I wrote about a
<a href="https://computer.rip/2024-04-05-the-life-of-one-earth-station.html">secure data center claiming to be located in a hardened facility with CIA
and/or SDI
ties</a>. That
building doesn't even appear to have been hardened at all, and they still went
bankrupt.</p>
<p>What if I told you that they were all barking up the wrong tree? If you really
want to make a business out of secure underground storage, you need something
bigger and with better access. You need a mine.</p>
<p>It will also be very important to make this play in the <em>early</em> Cold War, when
there was a much clearer market for hardened facilities, as evidenced by the
military spending that period building them rather than selling them off. The
1980s and on were just too late.</p>
<p>There are actually several rather successful businesses built on the premise of
secure, hardened storage, and they are distinctively pre-computer. The best
known of them is diversified "information management" firm Iron Mountain. You
know, with the shredding trucks. And an iron mountain, or rather, an iron mine
by that name.</p>
<p>Like most of the large underground facilities that are still commercially
successful today, the story of Iron Mountain involves mushrooms. Efficient
cultivation of mushrooms requires fairly tightly controlled conditions that are
not dissimilar to those you already find underground: cool temperatures, high
humidity, and low light. Culinary mushrooms are widely produced in large caves
and former mines, which often provide more convenient access, having been
designed for bulk transportation of ores.</p>
<p>This might be a little surprising, because we tend to think of underground
mines as being small, cramped spaces. That's true of many mines, often for
precious metals, for example. But there are also mines that extract ores that
are present over large areas and have relatively low value. This requires an
efficient method of removing a very large quantity of rock. Modern mines employ
some very clever techniques, like "block caving," where a very large rock mass
is intentionally collapsed into a prepared chamber that it can be scooped out
of like the bottom of a hopper. One of the most common methods, though, and one
that has been in use for a very long time, is "room and pillar" mining.</p>
<p>The idea is pretty simple: you excavate a huge room, leaving pillars to hold up
the material above. Depending on the economics, geology, etc., you might then
"retreat," digging out the pillars behind you as you work your way out of the
room. This causes the room to collapse, ideally away from the working area, but
not always. Retreat mining is dangerous and doesn't always produce ore of that
much value, so a lot of mines didn't do it. They just left the rooms, and their
pillars: huge underground chambers, tens and hundreds of thousands of square
feet, empty space. Many were dug back into a hill or mountain, providing direct
drive-in access via adits. Most, almost all, successful underground storage
facilities are retired room and pillar mines.</p>
<p>In the first-half of the 20th century, mushroom cultivation was the most common
application of underground space. That's what led "Mushroom King" Herman Knaust
to purchase the disused Mt. Thomas iron mine near Livingston, NY in 1936.
Knaust's company, Knaust's Cavern Mushrooms, was the largest producer of
culinary mushrooms in the world. Ten crops of mushrooms were produced each year
in the Livingston mine, and as Knaust continued to grow his operations, the
mushroom mine became one of the area's principal employers. Knaust dubbed it
Iron Mountain.</p>
<p>By 1950, things had changed. Knaust had at least two motivations for his pivot:
first, the US mushroom industry was rapidly weakening, upset by lower-cost
mushrooms imported from Europe and Asia. Second, WWII had come to a close, and
the Cold War was beginning.</p>
<p>In 1952, Knaust told reporters of his experience working with refugees from
Europe resettled in New York. Most of them had lost everything to bombing, and
they told Knaust how they had attempted to hide their most valuable
possessions, and their paperwork, in safer places. The Germans, Knaust read,
had come up with the best hiding place of all: disused mines. During the course
of the war, the Nazi administration stored valuables ranging from gold bullion
to original works of art in former mines throughout their occupied territories.
Some of them were large-scale logistical operations, with rail access and
organized archival systems.</p>
<p>Now, in the age of nuclear weapons, Knaust thought that this kind of protection
would be in more demand than ever. In 1951, he renovated the old mine and
installed new ventilation equipment. Most importantly, he bought a 28-ton vault
door secondhand from a failed bank. A generator and a security force of armed
former police officers rounded out Knaust's new venture: Iron Mountain Atomic
Storage.</p>
<p>The bank vault door was mostly for show, and Knaust's description of the mine
as "A-bomb proof" and "radiation proof" somewhat stretch the science. But
Knaust was a born marketer; his version of nuclear alarmism drew the attention
of corporate America like the Civil Defense Administration's pamphlets gripped
the public. The entrance to his mine was a sturdy stone block building with
iron bars over the windows and "Atomic Storage Vaults" inscribed at the top. He
was sure to tell reporters of his estate, home to the world's only
mushroom-shaped swimming pool. Over the years of newspaper coverage, the bank
vault door at the front of the mine got heavier and heavier.</p>
<p>In the event of a nuclear attack, Knaust reasoned, banks could lose their
records of deposits. Insurers could lose track of their policies. Companies of
all kinds could lose inventory sheets. The resulting economic chaos would be as
destructive as the bomb that started it. If we were to shelter lives, we also
needed to shelter information. By the time Iron Mountain Atomic Storage was
open for business he had already signed up the first customers, who shipped
copies of their records for storage in individual vaults constructed within
chambers of the mine. The East River Savings Bank, of New York, proudly
described how each of their branches microfilmed new deposits daily for
transport to the Mountain.</p>
<p>Iron Mountain sold its services to individuals as well. For $2.50 a year, a
consumer or small business could pack records into a tin can for storage in the
mine. The cans could be stored or retrieved with local agents stationed in
major cities, who sealed them in the presence of the customer and shipped them
to the mine by courier.</p>
<p>By 1954, Iron Mountain boasted over 400 customers, mostly large corporations
and institutions. It was a surprising hit with newspapers: about 150 rented
space to store their archives. Recordak, a subsidiary of Kodak that provided
microfilming services, set up a branch office at the mine with representatives
who could convert records to microfilm or turn them back into full-sized
versions on demand. The consumer part of the business was reorganized in
partnership with the Railway Express Agency, one of the descendants and
ancestors of our modern American Express and Wells Fargo. Individuals and small
businesses could deposit records with any Railway Express agent and request
records sent back to their nearest train station.</p>
<p>They quickly faced competition. Perhaps the most interesting was Western States
Atomic Storage Vaults, who purchased a disused South Pacific Coast Railroad
tunnel in the Santa Cruz mountains. The railroad right of way, near Zayante,
California, received a similar conversion to caged storage units. At least a
half dozen underground storage companies would be organized between 1950 and
1960.</p>
<p>The atomic storage industry was not always an easy one. Iron Mountain had a
slow start, signing up few customers after their initial set of banks and
newspapers. The Cuban Missile Crisis gave a considerable boost to sales,
though, and revenue almost doubled in 1963 alone. Iron Mountain's inventory
expanded from paper and microfilm records to original works of art, and they
purchased a second mine, a former limestone mine nearby at Kingston, NY, to
expand. They added office and dormitory facilities at both sites, to protect
both their extensive staff of clerks and representatives of their customers in
the event of nuclear war. "What good are the records if everyone in the firm is
blown up," Iron Mountain's executive vice president offered.</p>
<p>Inside of Iron Mountain, behind the vault door, steel doors with combination
locks protected individual vault suites ranging from closets to hundreds of
square feet. Racks held boxes and cans of individual records deposited by
smaller customers. The whole facility was maintained at 70 degrees Fahrenheit
and 50% humidity, a task greatly eased by the surrounding earth.</p>
<p>They were dedicated to the privacy of their customers but also had a hard time
passing up an opportunity for promotion, telling reporters of some of the
publishers and television companies that stored their archives at Iron
Mountain, and hinting at a "major New York City art museum" that leased space
for its collection. Individual customers included doctors, stamp collectors,
and "a whole lot of people who aren't talking as long as the outside world
lasts."</p>
<p>The mid-1960s were the apex of the Cold War in the popular consciousness, and
Iron Mountain's luck would not last through the broader decline in planning for
all-out Soviet attack. Now called Iron Mountain Security Storage, quotes for
newspaper articles shifted their focus towards civil unrest. In a world of
campus riots, president James Price said, universities were moving their
academic records underground.</p>
<p>Iron Mountain's good (for business) and bad (for society)  mood must have been
infectious, because competitors flourished even in 1970. Bekins, the moving and
storage company, purchased 200 acres in the Diablo mountains of California.
They intended to open the first underground storage facility specifically built
for that purpose, and plans included a hotel and heliport for convenient
customer access. The July 11th, 1970 edition of The Black Panther's <em>Black
Community News Service</em> contains perhaps them most blunt assessment of the
Bekins plan, one that would prove prescient.</p>
<blockquote>
<p>The possibility of World War III is not as much an immediate threat to the
life and well being of America's greedy capitalists as is the strong
probability of the more severe "political consequences" that might be meted
out by the masses, the people, for selfish crimes committed against them.</p>
</blockquote>
<p>A year later, the plan had expanded to shelter for 1,000 "executives and office
workers" for up to 30 days, an airport that could serve business jets, and a
computer and communications center. Bekins said that it would help corporations
survive a nuclear war, but was even more important in the event of rioting or
terrorism. Bekins specifically called out unrest at UC Berkeley, and damage it
had caused to academic records, as evidence of the need.</p>
<blockquote>
<p>Blaine L. Paris, number one stooge manager of Bekins Company.., acknowledged
that the hideaway, hideout survival shelter's main draw is the widespread
fear, on the business executive level, of bombings, the random tossing of
molotov cocktails, possibilities of kidnapping...</p>
</blockquote>
<p>Some large companies, Bekins said, were planning to set up alternate corporate
headquarters in the facility as soon as it opened. It would be something like
the Mount Weather of the corporate world. Two years passed, and Bekins
reimagined the facility as a regular-use business park rather than a
contingency site, but still underground. Joseph Raymond, Bekins' director of
Archival Services, quipped that employees might be more productive underground
where they'd be "free from the distractions of the surface."</p>
<p>Bekins had bad news coming. The era of atomic storage had come to an end.
Corporate fear of popular revolution proved insufficient to fund the ten
million dollar project. The Bekins facility would never break ground. Iron
Mountain, quietly and under cover of their typical boosterism, had run out of
money.</p>
<p>In 1971, a group of investors formed Schooner Capital and bought them out.
Their strategy: to focus on business records management and compliance, and
largely drop the "underground" and "atomic" part. Beginning in 1978, Iron
Mountain built dozens of new storage facilities that were normal, above-ground
warehouses. At the same time, they shifted the focus of their sales from
security to "information management." New legal requirements and tax
regulations meant that records retention had become a complex and costly part
of many businesses; Iron Mountain offered to outsource the entire matter. Their
clerks collected records from businesses, filed them away, and destroyed them
when retention was no longer required.</p>
<p>Iron Mountain remains the largest company in the business today. Most US cities
have an expansive Iron Mountain warehouse somewhere on their outskirts, and
their mobile shredding trucks are a regular sight in business districts. Still,
a certain portion of the Cold War attitude remains. Unshredded records are said
to be transported in unmarked vehicles, to avoid attracting attention. Iron
Mountain facilities are not exactly hidden, but their locations are not well
publicized, and they continue to use armed guards. Distinctive red "Restricted
Area" signs surround each one.</p>
<p>And they still have plenty underground.</p>
<p>When you look into the history of Iron Mountain, you will see frequent
reference to the Corbis Collection. The story of Corbis would easily make its
own article, but the short version is that Corbis was founded by Bill Gates as
a sort of poorly-thought-out electronic picture frame company. Over the span of
decades, they amassed one of the world's largest private collections of
historic photographs and media, and then collapsed into an influencer marketing
firm. It is often noted that the Corbis collection, of over 15 million
photographs spanning 150 years, is stored at Iron Mountain. This isn't quite
correct, but it's wrong in an interesting enough way to make it worth
unpacking.</p>
<p>In the 1950s, the Northeast Pennsylvania Industrial Development Commission
(NPIDC) formed a task force to investigate opportunities for the reuse of the
state's growing number of abandoned coal mines. Coal is mined almost entirely
by the room and pillar method, and while there are practical challenges in
reusing coal mines in particular, the amount of space involved was
considerable. The NPIDC's first proposal was right in line with the cold war:
they proposed that the Civil Defense Administration use the mines to store
their stockpiles of equipment and supplies.</p>
<p>The Civil Defense Administration wasn't interested, they were worried that
firedamp (flammable coal gases) would make the mines dangerous and high
humidity would cause stored equipment to rust. Still, the idea rattled around
the state of Pennsylvania for years, and sometime around 1953 one such mine
near Boyers, PA was purchased by the newly formed National Storage Company.
National Storage became one of Iron Mountain's key competitors.</p>
<p>Iron Mountain has become as large as it is by following a fine American
economic tradition: monopolization. It outlasted its erstwhile atomic storage
competitors by buying them. Western States Atomic Storage Vaults and their
railroad tunnel, National Storage and their coal mine, and at least two other
similar ventures became part of Iron Mountain in the 1990s.</p>
<p>It is the former National Storage facility in Boyers that holds the Corbis
collection. It has a notable neighbor: the largest tenant at Boyers is the
United States Office of Personnel Management, which famously holds both
clearance investigation files and federal employee retirement records down in
the old mine. In 2014, the Washington Post called the Boyers mine a
<a href="https://www.washingtonpost.com/sf/national/2014/03/22/sinkhole-of-bureaucracy/">"sinkhole of
bureaucracy"</a>,
describing the 600 OPM employees who worked underground manually processing
retirement applications. These employees, toiling away in a literal paperwork
mine, were the practical result of a decades-long failed digitization program.</p>
<p>Underground storage is still a surprisingly large business. Some readers may be
familiar with "SubTropolis," an extensive limestone mine near Kansas City,
which offers 55 million square feet of underground space. SubTropolis has
never particularly marketed itself as a hardened or secure facility. Instead,
it offers very cost-effective storage space with good natural climate control.
Tenants include refrigerated logistics companies and the National Archives.
There are a number of facilities like it, particularly in parts of the eastern
United States where the geography has been amenable to room and pillar mining.</p>
<p>That's the irony of Iron Mountain: their original plan was a little too
interesting. Iron Mountain continues to operate multiple underground
facilities, both their own and those they have acquired. Some of them,
including Boyers, even have datacenters. The clients are mostly media
companies, with original materials they cannot easily duplicate, and legacy
government and financial records that would be too costly to digitize. Sony
Music stores their studio masters with Iron Mountain, a big enough
operation that some of Iron Mountain's underground sites have small recording
studios to allow for restoration without removing the valuable originals from
safekeeping. Miles of film are stored alongside miles of pension accounts. No
one talks about nuclear war. The bigger fear is fire, which is more difficult
to contain and fight in these old mines than in purpose-built archival
warehouses.</p>
<p>There are only so many masters to store, and the physical volume of corporate
records is quickly declining. Atomic vaults hit a limit to their growth. The
total inventory of underground corporate storage facilities in the United
States today is much the same as it was in the 1960s, with more closing than
opening. Offsite records storage is shrinking overall, and Iron Mountain is
effectively in the process of a pivot towards (above-ground) datacenters and
services.</p>
<p>Still, when you read about Mark Zuckerberg's 5,000 square foot bunker in
Hawaii, or Peter Thiel's planned underground project in New Zealand, one can't
help but wonder if the predictions of Bekins, and the Black Panthers, were just
ahead of their time.</p>
<hr />
<p>I hope you enjoy this kind of material on Cold War defense and culture. It's
one of my greatest interests besides, you know, anything underground. For those
of you who <a href="https://ko-fi.com/jbcrawford">support me on Ko-Fi</a>, in the next day
or two my supporter newsletter EYES ONLY will be a short followup to this
piece. It will discuss underground storage facilities of a slightly different
kind: the records vaults constructed by the Church of Scientology and the
Latter-Day Saints, and the extent to which these facilities also reflect Cold
War concerns.</p>
<p>I am also working on something about waste-to-energy facilities that will
probably be an EYES ONLY article, as a companion to an upcoming CAB article on
the history of an experimental Department of Energy biomass power plant in
Albany, Oregon. But first, I will write something about computers. I have to
every once in a while.</p> ]]></description>
	</item>

 	<item>
		<title>2024-10-26 buy payphones and retire</title>
		<pubDate>Sat, 26 Oct 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-10-26-buy-payphones-and-retire.html</link>
		<guid>https://computer.rip/2024-10-26-buy-payphones-and-retire.html</guid>
		<description><![CDATA[ <blockquote>
<p>PAYPHONES at High Volume</p>
<p>Existing sites! Earn BIG $$. Money Back Guarantee!</p>
</blockquote>
<p>Dropshipping AliExpress watches, AI-generated SEO spam websites... marginally
legal and ethical passive income schemes, that serve to generate that income
mostly for their promoters, can feel like a modern phenomenon. The promise of
big money for little work is one of the fundamental human weaknesses, though,
and it has been exploited by "business coaches" and "investment promoters" for
about as long as the concept of invesstment has existed. We used to refer mostly
to the "get rich quick" scheme, but fashions change with the time, and at the
moment "passive income" is the watchword of business YouTubers and Instagram
advertising.</p>
<p>And what income is more passive than vending machine coin revenue? Automated
vending has had a bit of a renaissance, with social media influencers buying
old machines and turning them into a business. The split of their revenue
between vending machine income and social media sponsorship is questionable,
but it's definitely brought some younger eyes to an industry that is as rife
with passive income scams as your average spam folder. Perhaps it's the
enforcement efforts of the SEC, or perhaps today's youth just need a little
more time to advance their art, but I haven't so far seen a vending machine
hustle quite as financialized as the post-divestiture payphone industry.</p>
<p>For much of the history of the telephone system, payphones were owned and
operated by telephone carriers. As with the broader telephone monopoly, there
were technical reasons for this integration. Payphones, more specifically
called coin operated telephones, were "dumb" devices that relied on the
telephone exchange for control. In the case of a manual exchange, you would
pick up a payphone and ask the operator for your party---and they would advise
you of the price and tell you to insert coins. The coin acceptor in the
payphone used a simple electrical signaling scheme to notify the operator of
which and how many coins you had inserted, and it was up to the operator to
check that it was correct and connect the call. If coins needed to be returned
after the call, the operator would signal the phone to do so.</p>
<p>With the introduction of electromechanical and then digital exchanges, coin
control became automated, but payphones continued to use specialized signaling
schemes to communicate with the coin control system. They had to be connected
to special loops, usually called "coin lines," with the equipment to receive
and send these signals. The payphone itself was a direct extension of the
telephone system, under remote control of the exchange, much like later devices
like line concentrators. It was only natural that they would be operated by the
same company that operated the control system they relied on.</p>
<p>Well, a lot of things have changed about the payphone industry. The 1968
Carterfone decision revolutionized the telephone industry by allowing the
customer to connect their own device. Coin operated telephones in the
traditional sense were unaffected, but Carterfone opened the door to a whole
new kind of payphone.</p>
<p>In 1970, burglar alarm manufacturer Robotguard blazed the trail into a new
telephone business. They imported a Japanese payphone that was a little
different from the American models of the time: it implemented coin payment
internally. Robotguard connected the payphone through one of their burglar
alarm autodialers, a device that was already fully compliant with telephone
industry regulations, and then hooked it up to a Southwestern Bell telephone
line in a department store in in St. Louis. By inserting a dime, the phone was
enabled and you could make a local call (the autodialer was used, in part, to
limit dialing to 7 digits to ensure that only local calls were made).</p>
<p>Robotguard had done their homework, consulting the same law firm that
represented Carterfone in the 1968 case. They believed the scheme to be legal,
since the modified Japanese payphone behaved, to the telephone company, just
like any other customer-owned phone. The New York Times quotes Southwestern
Bell, whose attitude is perhaps best described as resignation:</p>
<blockquote>
<p>Spokesmen for the Southwestern Bell Telephone Company, the operating company
in that area, acknowledge that the equipment is in the store, that it is
working as described and that it appears completely legal. There is nothing
they can do about it at this time, they say.</p>
</blockquote>
<p>There was, indeed, nothing that they could do about it. Robotguard had
introduced the Customer-Owned Coin-Operated Telephone, or COCOT, to the United
States. Payphones were now a competitive business.</p>
<p>Despite a certain air of inevitability, COCOTs had a slow start. First, there
would indeed be an effort by telephone companies to legally restrict COCOTs.
This was never entirely successful, but did result in a set of state
regulations (and to a lesser extent, federal regulations related to
long-distance calls) that made the payphone business harder to get into. More
importantly, though, the technical capabilities of COCOTs were limited. The
Robotguard design could charge only a fixed fee per call, which made it a
practical necessity to limit the payphone to local calls. Telephone company
payphones, which allowed long-distance calls at a higher rate, had an
advantage. Long-distance calls were also typically billed by minute, which made
it important for a payphone to impose a time limit before charging more. These
capabilities were difficult to implement in a reasonably compact, robust device
in the 1970s.</p>
<p>A number of articles will tell you that COCOTs became far more common as a
result of payphone deregulation stemming from the 1984 breakup of AT&amp;T. I would
love to hear evidence to the contrary, but from my research I believe this is a
misconception, or at least not the entire story. In fact, payphones were
deregulated by the Telecommunications Act of 1996, but that was done in large
part because COCOTs were already common and telephone companies were unhappy
that conventional payphones were subject to rate regulation while COCOTs were
not [1].</p>
<p>Divestiture did definitely open the floodgates of COCOTs, although I think that
the advances in electronics around that time were also a significant factor in
their proliferation. In any case, several manufacturers introduced COCOTs in
1984 and 1985.</p>
<p>These later-generation COCOTs were significantly more sophisticated than the
mechanical system used by Robotguard. To the user, they were pretty much
indistinguishable from carrier-operated payphones, charging varying rates based
on call duration and local or long distance. This local simulation of the
telephone exchange's charging decisions required that each COCOT have, in
internal memory, a prefix and rate table to determine charges. Early examples
used ROM chips shipped by their manufacturer, but over time the industry
shifted to remote programming via modems. These sophisticated,
electronically-controlled coin operated phones that did not rely on an
exchange-provided coin line came to be known as "smart payphones" and even,
occasionally, as "smartphones."</p>
<p>Smart payphones greatly simplified payphone operations and were even adopted by
the established telephone companies, where they could save money compared to
the more complex exchange-controlled system. But they also made COCOTs
completely practical, as good to the consumer as any other payphone. As COCOTs
became remotely programmable, the payphone business started to feel like a way
to generate---dare I say it---passive income. All you had to do was collect the
coins. Well, that and keep the phone in working order, which would become a
struggle for the thinly staffed and overleveraged Payphone Service Providers
(PSPs) that would come to dominate the industry.</p>
<p>One of the new entrants into the payphone business was a company that
specialized in exactly the kind of remote management these new smart payphones
required: Jaroth Inc., which would do business as Pacific Telemanagement
Solutions or PTS. Today, PTS is the largest PSP in the United States, but that
isn't saying a whole lot. They enjoyed great success in the 1990s, though, and
were so well-positioned as a PSP in the '00s that they often purchased the
existing payphone fleet from former Bell Operating Companies that decided to
abandon the payphone business.</p>
<p>The 1990s were a good time for payphones, and they were also a good time for
investment scams. Loose enforcement of regulations around investment offerings,
the Dot Com Boom, and a generally strong economy created a lot of opportunities
for "telecom entrepreneurs" that were more interested in moving money than
information.</p>
<hr />
<p>The problem of 1990s telecommunications companies funded in unscrupulous ways
is not at all unique to payphones, although it did reach a sort of apex there.
I will take this opportunity to go on a tangent, one of those things that I
have always wanted to write an article about but have never quite had enough
material for: MMDS, the Multichannel Multipoint Distribution Service.</p>
<p>MMDS was, essentially, cable television upconverted to a microwave band and
then put through directional antennas. It was often marketed as "Wireless
Cable," sort of an odd term, but it was intended as a direct competitor to
conventional cable television. I think it's fair to call it an ancestor of what
we now call WISPs, using small roof-mounted parabolic antennas as an
alternative to costly CATV outside plant. Some MMDS installations literally
were early WISPs: MMDS could carry a modified version of DOCSIS.</p>
<p>Wireless cable got a pretty bad rap, though. If you pay attention to WISPs, you
will no doubt have noticed that while the low capital investment required can
enable beneficial competition, it also enables a lot of companies that you
might call "fly by night." Some start out with good intentions and just aren't
up to the task, while some come from "entrepreneurs" with a history of fraud,
but either way they end up collecting money and then disappearing with it.</p>
<p>MMDS had a <em>huge</em> problem with shady operators, and more often of the "history
of fraud" type. Supposed MMDS startups would take out television and newspaper
ads nationwide offering an incredible opportunity to invest in this exciting
new industry. The scam took different forms in the details, but the most common
model was to sell "shares" of a new MMDS company in the four-to-five-digit
range. Investors were told that the company was using the capital to build
out their network and would shortly have hundreds of customers.</p>
<p>In practice, most of these "MMDS startups" were in cities with powerful
incumbent cable companies and, even worse, preexisting MMDS operators using the
limited spectrum available for such a wideband service. They never had any
chance of getting a license, and didn't have anyone with the expertise to
actually build an MMDS system even if they got one. They just pocketed the
money and were next seen on a beach in Mexico or in prison, depending on the
whims of fortune.</p>
<p>These wireless cable schemes became so common, and so notorious, that if you
asked a lot of people what wireless cable was the two answers you'd get are
probably "no idea" and "an old scam."</p>
<hr />
<p>It only takes a brief look at newspaper archives to find that the payphone
industry was a little sketchy. There are constant, nationwide, near-identical
classified ads with text like "buy and retire now" and "$150k yearly potential"
and "CALL NOW!". Sometimes more than one appear back to back, and they're still
nearly identical. None of these ads give a company name or really anything but
a phone number, and the phone numbers repeat so infrequently that I suspect the
advertisers were intentionally rotating them. This was pretty much the
Craigslist "work from home" post of the era.</p>
<p>To understand payphone economics better, let's talk a little about how the
payphone business operated. Telephone companies had long run payphones on the
same payment model, by finding a location for the payphone (or being contacted
by the proprietor of a location) and then offering the location a portion of
revenue. In the case of incumbent telcos, this was often a fixed rate per
call. So someone owned the location and the payphone operator paid them in
the form of a royalty.</p>
<p>COCOTs enabled a somewhat more complex model. A COCOT might be located in a
business, connected to a telephone company line, and remotely programmed by a
service provider, all of which were different companies from the person that
actually collected the money. The revenue had to get split between all of these
parties somehow, but COCOTS weren't regulated and that was all a matter of
negotiation.</p>
<p>Much like the vending machine industry today, one of the most difficult parts
of making money with a payphone was actually finding a good location---one that
wasn't already taken by another operator. As more and more PSPs spread across
the country, this became more and more of a challenge. So you can imagine the
appeal of getting into the payphone hustle without having to do all that
location scouting and negotiation. Thus all the ads for payphone routes for
sale... ostensibly a turnkey business, ready to go.</p>
<p>Ah, but people with turnkey, profitable businesses don't tend to sell them.
Something is up.</p>
<p>Not all of these were outright scams, or at least I assume some of them
weren't. There probably were some PSPs that financed expansion by selling
or leasing rights to some of their devices. But there were also a lot of...
well, let's talk about the second largest PSP of the late '90s.</p>
<p>Somewhere around 1994, Charles Edwards of Atlanta, Georgia had an idea. His
history is obscure, but he seems to have been an experienced salesman, perhaps
in the insurance industry. He put his talent for sales to work raising capital
for ETS Payphones, Inc., which would place and operate payphones on the behalf
of investors.</p>
<p>The deal was something like this: ETS identified locations for payphones and
negotiated an agreement to place them. Then, they sold the payphone itself,
along with rights to the location, to an investor for five to seven thousand
dollars a pop. ETS would then operate and maintain the payphone while paying
a fixed monthly lease to the investor who had purchased it---something like
$83 a month.</p>
<p>It was a great deal for the investors---they didn't need any expertise or
really to do any work, since ETS arranged the location, installed the phones,
and even collected the coins. In fact, most investors purchased phones in
cities far from where they lived, such was the convenience of the ETS model.
There was virtually no risk for investors, either. ETS promised a monthly
payment up front, and the contract said that they would refund the investor
if the payphone didn't work out.</p>
<p>The ETS network was far larger than just Edwards could manage. Most of the
investment deals were sold by independent representatives, the majority of them
insurance agents, who could pick it up as a side business to earn some
commission. Edwards sold nearly 50,000 payphones on this basis, many of them in
deals of over $100,000. Small-time investors convinced of the value by their
insurance agents, many of them retirees, put over $300 million into ETS from
1996 to 2000.</p>
<p>There was, as you might have guessed, a catch. One wonders if the payphones
were even real. I think that at least many of them were; ETS ran job listings
for payphone technicians in multiple cities and occasionally responded to press
inquiries and complaints about malfunctioning payphones bearing their logo.
Besides, the telecom industry recognized ETS as a huge PSP in terms of both
installed base and call volume.</p>
<p>What definitely wasn't real was the revenue. ETS was a ponzi scheme. In 2000,
the SEC went for Charles Edwards, showing that ETS had never been profitable.
Edwards sponsored a NASCAR team and directed millions of dollars in salary and
consulting fees to himself, but in the first half of 2000 ETS lost $33 million.
The monthly lease payments to investors were being made from the capital put in
by newer investors, and even that was drying up.</p>
<p>SEC v. ETS went on for six years, in good part due to an appeal to the Supreme
Court based on ETS' theory that a contract that paid a fixed, rather than
variable, monthly rate could not be considered a security. In 2006, Charles
Edwards was convicted of 83 counts of wire fraud and sentenced to thirteen
years in prison.</p>
<p>Edwards was far from the only coin-op fraudster. ETS was not unusual except in
that it managed to be the largest. When a class-action firm and several state
attorneys general went after ETS, their press releases almost always mentioned
a few other similar payphone schemes facing similar legal challenges. Remember
all of those classified ads? I suspect some of them <em>were</em> ETS, but ETS also
had a more sophisticated sales operation than two-line classifieds. Most of
them were probably from competitors.</p>
<p>The payphone industry crashed alongside ETS; ETS almost certainly would have
collapsed (albeit likely more slowly) even if it had been above board.
Increasing cellphone ownership from the '90s to '00s made payphones largely
obsolete, and more and more established telcos and PSPs decided to drop them.
One of the reasons for PTS's ascent was its willingness to buy out operators
who wanted out: in 2008, PTS bought most of AT&amp;T's fleet. In 2011, they bought
most of Verizon's fleet. Almost every incumbent telephone company got out of
the payphone business and most of them sold to PTS.</p>
<p>Given all that, you might think that payphone scams were only a thing of the
'90s. And they mostly were, but you can imagine that there was an opportunity
for anyone who could adapt the ETS model to the internet age.</p>
<p>Pantheon Holdings did just that. It's even more difficult to untangle the early
days of Pantheon than it is ETS. Pantheon operated through a variety of shell
companies and brands, but "the Internet Machine Company" was perhaps the most
to the point. Around 2005, Pantheon built "internet kiosks" where customers
could check their email, print documents, and even make phone calls for a
nominal cash or credit card payment. Sometimes called "global business
centers," these kiosks were presented as an exciting business opportunity to
mostly elderly investors who were given the opportunity to buy one for just
$18,000.</p>
<p>Once again, the kiosks were real, but the revenue was not. Pantheon placed the
machines in low-traffic locations and did nothing to market them. By 2009, more
than a dozen people had been convicted of fraud in relation to the Internet
Machines.</p>
<p>Pantheon kiosks still turn up on the junk market.</p>
<p>[1] I spent quite a bit of time researching the history of payphone regulation
to try to understand exactly what did change in 1984, how many COCOTs operated
and on what legal basis from 1970-1984, etc. I did not have much success. What
I can tell is that COCOTs were very rare prior to 1984 (so rare that the FCC
apparently didn't know of any, according to a 1984 memo, despite the 1970
example), and by the late '80s were very common. The FCC seems to have taken
the view, in 1984, that COCOTs had <em>always</em> been legal, and just weren't being
made or used on any significant scale. That's somewhat inconsistent, though,
with the fact that suddenly after 1984 divestiture a bunch of companies started
making COCOTs for the first time. My best guess right now is that from 1970-1984
COCOTs were probably legal but were something of a gray area because of the lack
of any regulations specifically applying to them. Some combination of divestiture
broadly "shaking up" the phone industry, electronics making COCOTs much more
feasible, and who knows what else lead multiple companies to get into the COCOT
business in the mid-'80s. That lead the FCC to issue a series of regulatory
opinions on COCOTs that consistently upheld them as legal, culminating in the
1996 act dropping payphone regulation entirely.</p> ]]></description>
	</item>

 	<item>
		<title>2024-10-19 land art and isolation</title>
		<pubDate>Sat, 19 Oct 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-10-19-land-art-and-isolation.html</link>
		<guid>https://computer.rip/2024-10-19-land-art-and-isolation.html</guid>
		<description><![CDATA[ <p>Prescript: I originally started writing this with the intent to send it out to
my supporter's newsletter, <a href="https://ko-fi.com/jbcrawford">EYES ONLY</a>, but it
got to be long and took basically all day so I feel like it deserves wider
circulation. You will have to tolerate that it begins in the more
conversational tone I use for the supporters newsletter. I am going to write a
bit about some related local works of art and send that to EYES ONLY instead.</p>
<p>Over on <a href="https://pixelfed.social/i/web/profile/396485435616948986">pixelfed</a> I posted these two photos.</p>
<p><img alt="Turrell installation at The Crystals" src="https://computer.rip/f/landart/1.jpg" /></p>
<p><img alt="Turrell installation at The Crystals" src="https://computer.rip/f/landart/2.jpg" /></p>
<p>It's Saturday morning, I have coffee and the cat is here and the work thing I
was planning to do has mercifully turned out to not need to be done today, so I
have time to kill. Let's talk a little bit about Art before I take on a real
project for the day.</p>
<p>So let's talk about the photos first, and then we'll sort of widen our view to
the big picture. Neither photo is that good, tbh, I really enjoy architectural
photography but I rarely have more than my phone. It's actually rather
difficult to find any good angles to photograph this particular corner from
anyway, which is one of the common criticisms of it as a work of art. But I
have to actually say what it is! It's a corner of the upper floor of The Shops
at Crystals, an upscale shopping center attached to the Aria on the Las Vegas
strip. It opened in '09, and the building was designed by prominent architect
Daniel Libeskind.  It is, in my opinion, not really that interesting of a
building. It has a combination of dead mall vibes and Las Vegas energy that is
not all that compelling, and it doesn't have the surrealism of The Forum Shops
at Caesars, the Las Vegas destination I recommend if you want to see a real
shopping mall situation. But, it is connected to an APM (the Aria Express),
and I will take the slimmest of excuses to ride an APM.</p>
<p>There aren't a lot of good things I can say about Las Vegas architecture, but
one compliment I can extend is that the casino developers have mostly retained
a tradition of commissioning fine art for their buildings. Nothing can really
unseat Dale Chihuly's "Fiori di Como," the 2,000 square foot, 40,000 pound
glass sculpture that occupies the ceiling of the Bellagio's lobby. But the
Shops at Crystals took a pleasingly modernist direction by commissioning James
Turrell. Turrell is probably the most prominent member of the "Light and Space
movement," which can be simply described as an installation art view of
architecture with a particular emphasis on architectural lighting. Turrell's
portfolio includes an array of works he calls "Skyspaces," often vaguely
gazebo-like structures with apertures in their ceilings intended to frame the
sky as if it were a canvas. Most examples are in private ownership, the only
one I have seen is "Dividing the Light" at Pomona College (Turrel's alma
mater) in Claremont, California. I do recommend a visit.</p>
<p>There's something important to understand about the Skyspaces: the idea is more
or less that the art is the sky, and the space is only there to structure your
perception of it. Turrell has described them as naked-eye observatories. The
naked-eye observatory has a long tradition, perhaps unsurprisingly, since it
was the only kind of observatory until the development of the telescope.
Stonehenge is a rather famous one (I am referring, of course, to the Stonehenge
of Odessa, Texas. What else?). A lot of Turrell's work is like this, creating a
space to structure your view of the world beyond it, and he is particularly
interested in the sky as a subject. Stick a pin in that.</p>
<p>So back to the shopping mall. Turrell did a scattering of different projects
for the Crystals (I am hereforward dropping "The Shops At" for sanity),
including the monorail station. Now, I know you are thinking, what could
possibly appeal to me more than a James Turrell monorail station? Why have I
not up and moved to Las Vegas to devote my life to the preservation and
interpretation of this remarkable artifact?</p>
<p>Well, because it kind of sucks, is why. Almost all of the work Turrell did at
the Crystals feels badly hampered by the design of the larger building and the
practical necessities of an upscale shopping mall. It's hard to produce that
remarkable of a perceptual experience in a wide, crowded hallway. The monorail
station doesn't even always have Turrell's lighting turned on, is my
experience.  What stands out more is the space I photographed, basically an
awkward mezzanine floor that exists mainly to be a hallway to the monorail
station. You can't help but feel that it was handed over to Turrell because
they realized they'd made a mistake and there wasn't really anything else they
could do with the square footage. If you're in Las Vegas I would go to the
Crystals and take a look, because it is <em>something,</em> but it's a very long ways
from a masterpiece. Pretty much every view of it is intermediated by escalators
or food court signage, it's jarringly out of context, and exists in a broadly
uncomfortable part of the building that is too far from the ground floor to
appeal to shops. Instead, it hosts a traveling exhibition on Princess Diana.
And if that's not dead mall energy, I don't know what is.</p>
<p>So why are we talking about this, besides that I get to gush a little about
Turrell? Well, I really wanted to talk a little bit about land art, and I think
these smaller Turrell works are a good inroads. Land art can be succinctly
described as art that makes use of, or consists of, the landscape. One of the
prototypical works of land art is the Spiral Jetty, built by Robert Smithson on
the shores of the Great Salt Lake. It is what it says on the tin, a rock jetty
that reaches out into the lake before turning in on itself in a tightening
spiral. It was installed in 1970 and wore away with time, so for many years it
was completely submerged below the water. But the water is now receding, and it
is revealed once again, as a faint rock spiral in a dry plain. There is some
active debate over whether or not it would be appropriate to make repairs. It
is often an accepted part of land art that it will change over time due to
natural processes. At the same time, the Spiral Jetty's exact fate (to be
submerged and then later beached, as it were) was not foreseen and is largely a
result of human disruption of the ecosystem. So you can make arguments either
way.</p>
<p>In any case, it is an interesting aspect of the work (which merits a discussion
that makes up more than half of the Wikipedia page) that it exists, today,
mostly in the form of photographs. For much of its history it was entirely
invisible, and more recently it has reemerged, but in a severely eroded state. If
you are killing time in Utah on the wrong side of the lake, say because you
have been at the nearby Golden Spike National Historic Park, you should go see
it. Or perhaps not see it, as it no longer resembles Smithson's original
creation. This is, ultimately, the fate of all land art, as it is the fate of
the land. Think about that while you look at it.</p>
<p>One of the most famous works of landscape art is Walter De Maria's "The
Lightning Field," in Catron County, New Mexico. The Lightning Field has
something in common with the Spiral Jetty and several other prominent works of
land art: the Dia Art Foundation. Dia was founded in New York City in the '70s
with Schlumberger money, and had a rather explicit mission of funding art works
that are particularly expensive. This focus on large projects and the timing of
their heyday lead them naturally to land art. I would wager, just off the top
of my head, that roughly half of the prominent works of land art in the United
States are owned by Dia.</p>
<p>Land art really is big, and that can be its undoing. Turrell's Skyspace at
Pomona ran over $2 million, and it's not even <em>really</em> land art, I'm just
making a connection there in service of what I originally set out to talk about
but still haven't gotten to. I'm not sure if the total original cost of The
Lightning Field has been publicized, but it relied on grant funding from
multiple art foundations and the state, and a refurbishment about a decade ago
ran nearly half a million.</p>
<p>But, well, let's be real, it's not that unusual for original works of fine art
to run into the millions, and that's especially true of sculpture which often
requires fairly sophisticated fabrication and installation techniques. Land art
at its best tends to rely on sophisticated construction techniques, as well.
The Lightning Field, for example, required five months of extensive surveying
by land and air. Part of this was to produce what we would now call a digital
elevation map, in order to create the field's flat top despite the varying
terrain.</p>
<p>I am sort of purposely not describing The Lightning Field here. It's not that
it defies explanation, it's actually very easy to describe. In an article that
De Maria wrote himself for "Art Forum" to describe the project, he says that
"the sum of the facts does not constitute the work or determine its esthetics."
This is sort of a pretentious thing for a sculptor to say before rattling off a
bunch of large numbers, but he has a point: The Lightning Field is literally a
bunch of poles stuck in the ground, which is easy to tell you, but gives you
very little idea of what it is actually like.</p>
<p>This is a common and important aspect of land art. When I worked for Meow Wolf,
we talked a lot about "immersive art," which is pretty much the term that has
come to describe "whatever Meow Wolf is." If we allow ourselves some
rose-colored glasses, most land art projects were a form of immersive art, an
earlier form that seems to predate the forthright commercialization of the
projects I got to work on, like Meow Wolf's Omega Mart. To be fair, the
commercialization is part of the work, but it sure is on display. Immersive art
means that "you have to be there," and "you have to be there" is a great
opportunity for the real estate developer.</p>
<p>Ah, but this is indeed a rosy view of the past. There is another very
interesting trait of prominent works of land art. Since I have done such a
paltry job of describing The Lightning Field, go ahead and look up some photos.
They're worth a thousand words, and so you'll get... several thousand
words of information. You will quickly notice that there are <em>very few</em> photos
of The Lightning Field, and some of them turn out to be crops or recolors of
the others. The Dia Art Foundation, apparently according to Walter De Maria's
wishes, prohibits photography, or even the presence of cameras.</p>
<p>I have never been to The Lightning Field. This is also a surprise, I'm in
Catron County reasonably often. But it costs either $250 or $150 to visit
depending on the season, and the larger issue I have encountered is that the
visiting nights sell out immediately.</p>
<p>There is no public access to The Lightning Field. You have to make arrangements
with Dia to stay the night at a cabin on the property. Lots of people do this
and end up writing travelogues about how the experience changed them, and God
knows that I probably will too, some day. But after reading enough of these
travelogues you start to go a little mad. You are reading someone else's
accounting of an experience---something that is fundamentally an experience,
not a sight or a sound, not even space or light---that you have not had and
probably never will. It's like when you meet someone and the main and only
thing they have to talk about is their nomadic travels of Europe, but instead
of the routine lifestyle of people with Silicon Valley salaries and few
attachments, it is supposedly a great work of art. Like photographs of the
spiral jetty, personal essays of The Lightning Fields describe something that
hardly exists.</p>
<p>The magazine articles replace the art.</p>
<p>The charmingly-named journal "Art World Follies" featured an essay about The
Lightning Fields by art historian John Beardsley. It is titled "Art and
Authoritarianism." It's an exercise in self-control to not quote nearly the
entire thing here, but it is <a href="https://www.jstor.org/stable/778373">available on
JSTOR</a> if you have just three pages worth
of time to kill. Let me take just this, from the introduction:</p>
<blockquote>
<p>...The directive posture assumed toward the viewer by De Maria and Dia
suggests that both artist and patron lack confidence in either the quality of
the work or the discernment of the viewer.</p>
</blockquote>
<p>I can respect that Dia has certain practical concerns that encourage them to
limit access to the site, such as preservation of both the artwork and the
delicate desert ecosystem that it incorporates. But Beardsley points out that,
like most landscape art, The Lightning Field is in a remote location. Distance
and unimproved roads provide a natural limiting effect on visitation to these
sites; I've been to the Spiral Jetty several times and seldom seen more than
one other visitor around---fewer than are permitted to stay at The Lightning
Field each night. And that's another Dia-owned site, although not one
originally commissioned by them. They clearly do not apply such restrictive
measures to everything.</p>
<p>It might be tempting to attribute it to finances, especially after seeing
Beardsley complain about the required $30 donation, which has gone up by 500%
at the least. I do think that Dia has some financial struggles, but no doubt
they could raise more revenue by accepting more visitors to the site.</p>
<p>So, while it is tempting to blame Dia for their restrictive attitude, it's
clear that Walter De Maria shares in the fault. Many of the restrictions are in
place at his request; the whole notion that you can only see the artwork
through a 24-hour stay in a remote cabin to which you were transported by Dia
staff was apparently part of his vision.</p>
<p>Artist's vision or not, it is an affront to the public.</p>
<p>There are no doubt some regional politics at play. I cannot help but view Dia
with skepticism. Dia is a high-society NYC institution with galleries in New
York and, incongruously, several of its most prominent holdings in remote parts
of Utah and New Mexico. That land art requires land is self-evident, but it
also tends to require a degree of isolation. The most prominent and ambitious
land art projects, even when conceptualized in the more populous East, tend to
be actualized in the West. Here, land is our most important asset, and in
places like Catron County, it often seems to be our <em>only</em> asset.</p>
<p>So you can see the appeal to land artists. But you can also see the indignation
when those land artists claim the very land as their art, and keep it for
themselves.</p>
<p>To say that The Lightning Field is a betrayal of Western values is probably a
little over the top. Besides, it takes only a casual reference to the Bundys to
show that those values are not universal. But it is fair to say that the kind
of person who travels the west and is interested in land art is the type of
person who is interested in the land itself, and holds it dearly. There is
something unbearable about Walter De Maria, an artist from and in New York
City, making his most famous work out of a slice of our desert and then
narrowly dictating the terms on which we can see it.</p>
<p>The earth did not set forth one hundred thousand acres of lava across El
Malpais and then establish an elaborate booking policy, except that you must
put in the effort to cross such difficult terrain. The beautiful and unique
Quebradas are remote but open to anyone willing to make the trip. De Maria
seems to view his work as part of this tradition: "the land is not the setting
for the work but a part of the work." And yet he apparently thinks of himself
as being far above it.</p>
<p>Much of the beauty of the land is in the discovery. De Maria writes that "the
sky-ground relationship is central to the work." Anyone who loves the desert
can tell you that the sky-ground relationship is different everywhere you look,
that it does not photograph well but must be experienced, that the most
important and striking examples of it are found by chance, or by the dedication
of long hours spent looking. Yet The Lightning Field, ironically, offers no
such experience. It is intensely curated, guarded by Dia's many restrictions,
relentlessly interpreted for its scant audience by the demands its now-dead
creator makes of them. "Isolation is the essence of Land Art," he said.</p>
<p>Smithson built the Spiral Jetty and then walked away. It is technically owned
by Dia but you would be hard-pressed to tell it apart from the public land that
surrounds it. It is, nonetheless, truly isolated. At the lightning field, they
make a show of leaving visitors on their own, but in a way that provides the
exact opposite message: you are being allowed to glimpse something special, but
only on its creator's terms, a creator who has made very certain that his
presence will not be forgotten.</p>
<p>I shouldn't be too hard on the Dia foundation. Not only the Spiral Jetty, but
also Nancy Holt's "Sun Tunnels" are owned by Dia and open to the public in the
way typical of things found in the desert. Like the land itself, they leave it
to the visitor to have an experience of their own.</p>
<p>Unfortunately, this demand for isolation that turns to isolationism has become
too typical of land art.</p>
<p>We started, you might remember, with James Turrell. His work at the Crystals is
open to the public in the most commercial sense possible, perhaps to its
detriment. Even there, he has made a concession to isolationism. By far the
best part of his multi-installation work there, titled "Akhob," was upstairs in
the Luis Vuitton. Access required not only a reservation but passing muster
with the high-end store's doorman. Unfortunately, it doesn't seem to have
survived COVID: Luis Vuitton stopped advertising Akhob in 2021 and, today, its
fate is unknown besides that there is no access. Like the submerged Spiral
Jetty, we can experience it only by photos, unless perhaps some shift in the
economy causes the waters to recede.</p>
<p>Turrell, as you might suspect, can be viewed as a land artist. The Skyspaces
have a natural connection to the land art movement, and as Turrell became more
ambitious, his projects became larger, taking on the scale of landforms. Since
1979, Turrell has been working on "Roden Crater." It is a cindercone near
Flagstaff that may one day become the greatest of the Skyspaces, excavated and
reconstructed as a naked-eye observatory.</p>
<p>It is a huge vision that has faced a great deal of struggle. Despite many
announced opening dates, it remained "under construction," closed to the public
for 45 years. More recently, a $10 million donation from Kanye West and a
partnership program with Arizona State University have brought in renewed
funding, but a "tentative opening date" of 2024 looks set to pass just like the
last five. In the mean time, it is mainly ASU students (a few of which have
been entitled to visit the site by ASU's partnership agreement) and celebrities
that have been found worthy. Besides Kanye West, with Kim Kardashian as a +1,
Drake recently used Roden Crater as an Instagram backdrop.  Clearly, the
mandatory donation to experience the isolation of Roden Crater is a great deal
more than $250. Perhaps I shouldn't be so cynical, but the main benefactor of
Roden Crater is the Dia Art Foundation, and plans call for a set of cabins by
which visitors will experience it.</p>
<p>Massive land art projects have a tendency towards vaporware, but that's not to
say that they never escape. Michael Heizer's "City" is in a similar vein to
Roden Crater, also initiated in the 1970s, also blowing through its proposed
opening dates, also admitting no visitors due to its incompleteness. But City
made it out: in 2022, it was finally declared open. It is managed by the Triple
Aught Foundation, which has apparently learned a thing or two from Dia.</p>
<blockquote>
<p>Triple Aught Foundation, the 501(c)(3) that oversees and operates Michael
Heizer's City, has complete discretion as to the acceptance of any visitor
request. City is located on private property and only invited guests are
permitted on the property. All other visitors will be denied access to the
property. Invited guests must advise Triple Aught Foundation of any medical
conditions. The sculpture City is a registered work, protected by federal
copyright law. Triple Aught Foundation has a strict copyright enforcement
policy regarding unauthorized photographing or filming of the work. No
unauthorized reproductions, public display or distribution of copies of the
work, in whole or in part are permitted.  Anyone violating this policy will
be immediately asked to leave.</p>
</blockquote>
<p>Only six visitors are allowed per day, three days a week, for a maximum of
three hours, weather permitting, for a fee of $150, from May to November.
Reservations are available on a strictly first-come-first-served basis.</p>
<p>You, I can say with a fair degree of confidence, will only ever experience City
in the form of the few photographs the Triple Aught Foundation has seen fit to
release. The Lightning Field, Roden Crater, they are all submerged, not by the
waters of the Great Salt Lake but by the inability of their creators to let
land art be like the land itself: unrestricted, unconfined, unassuming.</p>
<p>In the essay collection "LAND/ART New Mexico," curator Lucy Lippard writes that
"I've come to the reluctant conclusion that Land Art is for city people."</p>
<blockquote>
<p>I live between inhabited and mostly uninhabited areas---which makes this
essay a kind of NIMBY rant: not in my backyard, not on my back forty. Given
the fact that I have spent my life writing about art (sometimes Land Art),
and ranting about the importance of public art, this sounds like a kind of
betrayal. But it's hard to imagine what kind of art would work here, at the
edge of a tiny village in north central New Mexico, looking out across a
highway to private ranchlands and distant mountains. When I was a
citydweller, I might have welcomed the sight of some visual extravagance, or
oddity, or subtle highlights to my daily surroundings. But the fact remains
that even semi-rural New Mexico is hard to improve upon.</p>
</blockquote>
<p>Perhaps that's the problem, perhaps land art as a movement is fundamentally at
odds with appreciation of the land itself. We might view Turrell, Smithson, De
Maria, Heizer as entitled for thinking that the land needed their help. It is
already art, and it always has been.</p>
<p>But we are humans, and we have always been inclined to interpret the land in
the context of its impact on us, and our impact on it. So often that impact is
happenstance, and more often for the worse than for the better. There must be
<em>some</em> room to manipulate the land entirely by intent, in service of aesthetics
and meaning rather than commercial exploitation. Indeed, the Land Art movement
viewed itself in part as an anti-commercial backlash to the museums and
galleries that held, and confined, so many forms of fine art. And yet, some of
the greatest works of the movement are displayed in conditions more
restrictive, more removed from the nature of the land, than the upper floor of
a Las Vegas Luis Vuitton.</p>
<p>"Isolation is the essence of Land Art." I am inclined to agree. But isolation
is not made, it is found. De Maria went to Catron County to seek it out, but
somehow left thinking that he had created it. This is not New York City, you do
not find space to think and experience behind a rope stanchion and a guest
list. The land is already there, and land artists should trust their audience
to experience it.</p>
<p><img alt="Spiral Jetty, 2021" src="https://computer.rip/f/landart/3.jpg" /></p> ]]></description>
	</item>

 	<item>
		<title>2024-10-12 commercial HF radio</title>
		<pubDate>Sat, 12 Oct 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-10-12-commercial-HF-radio.html</link>
		<guid>https://computer.rip/2024-10-12-commercial-HF-radio.html</guid>
		<description><![CDATA[ <p>According to a traditional system of classification, "high frequency" or HF
refers to the radio spectrum between 3 and 30 MHz. The label now seems
anachronistic, as HF is among the lowest ranges of radio frequencies that see
regular use. This setting of the goalposts in the early days of radio
technology means that modern communications standards like 5G are pushing major
applications into the EHF or "extremely high frequency" band. The frontiers of
basic radio technology now lie in the terahertz range, where the demarcation
between radio waves and light is blurred and the known techniques for both only
partially apply. HF, by contrast, is ancient technology. HF emissions can be
generated by simple, brute-force means. Ironically, this makes HF a bit
difficult: the incredible miniaturization and energy efficiency of modern
electronics makes HF radio hard to receive and transmit in a reasonable
footprint, one of several reasons that HF radio sees little consumer use.</p>
<p>Let's briefly consider the propagation characteristics of the HF band, which
are its most remarkable property. HF frequencies have long enough wavelengths
that they can reflect and refract in the earth's atmosphere. Somewhat like the
skin effect observed with AC electricity or surface tension in liquids, HF
emissions have a tendency to bend their path to follow dielectric boundaries.
All of these effects are mercurial and difficult to predict; the reliability of
sky or ground wave HF propagation can depend on the time of day, the weather,
the number of sunspots. All of this makes HF radio a bit of a pain in the ass,
but it can be worth it to achieve a feat that higher radio bands cannot:
propagation beyond the line of sight.</p>
<p>As a rule of thumb, radio emissions in the VHF band and above behave much like
light. Many materials are more transparent to RF than they are to light, but
still, most modern radio communications will not propagate beyond the horizon,
over a hill, or even past a sturdy building. An HF transmission, by contrast,
can be received around the globe in good conditions.</p>
<p>HF radio thus appeals mostly to users that desire long-range communications
with minimal infrastructure, and that have the sophistication (of operating
practice or technology) to handle the vagaries of HF. The usual suspects are
militaries, who fall more on the side of technical sophistication by using
computer-driven link establishment systems, and amateurs, who enjoy the
complexity of the operating practice. Other major HF radio applications include
international broadcasting (often of either national or religious propaganda),
intelligence and law enforcement agencies, and communication with ships and
aircraft at sea. Nearly all of these applications are giving way to satellite
communications, but the relative simplicity and low cost of maintaining HF
equipment, and its independence from vulnerable satellites, give it enduring
appeal to government users.</p>
<p>Let's consider a few interesting examples of these government applications,
although they are not the focus of this article. Military radio is a curious
combination of secretive and well-documented. The US military possesses
multiple major radio systems, but is theoretically consolidating onto the
Joint Tactical Radio System (JTRS). JTRS is an infamous military acquisition
debacle and has run vastly overbudget and behind schedule while failing to
deliver on many of its promises, but is now in daily use and consists of
a diverse lineup of software-defined radios that can operate a variety of
modes across many bands. In other words, it is a conceptually fairly simple
system that hides enormous, combinatorial complexity in its software. This
is modern military tradition.</p>
<p>Non-military systems are more recognizable to those without a background in
1990s military technology programs. One of the largest civilian government HF
radio systems is COTHEN, the Customs Over-The-Horizon Enforcement Network.
COTHEN was constructed, as the name suggests, by Customs and Border Patrol.  It
is now widely used by other federal law enforcement agencies with remote field
operations, as well as by the military when cooperating with law enforcement
agencies. The principle day-to-day business on COTHEN is drug interdiction by
CBP and the Coast Guard. COTHEN employs second-generation Automatic Link
Establishment (ALE), a popular system developed by the military to allow HF
radios to "discover" working frequencies between two locations. Besides various
mobile radios, COTHEN has fixed radio sites throughout the country, including
one <a href="https://www.google.com/maps/@35.14125,-105.9082222,981m/">in the high plains east of
Albuquerque</a>.</p>
<p>There are many smaller HF radio systems operated by executive agencies, mostly
for continuity of operations. Many are integrated into SHARES, a joint system
sponsored by the Department of Homeland Security. For a more specific example,
the Department of Energy has installed HF radio equipment at many of its
facilities including national laboratories, infrastructure sites, and the
historic AEC campus at Germantown. The DoE operates its own ALE network, but
most (and very probably all) of its sites are also seconded to SHARES. DoE also
makes use of HF radio for communications with Office of Secure Transportation
vehicles.</p>
<p>We understand that there are government applications of HF radio, that's
probably no surprise to anyone. But what about commercial applications? The
complexity of HF operations, the size of the antennas, and ready availability
of other communications options (like the internet) limit the appeal of HF
to business users. Still, there must be some? Obtaining a license to use HF
radio is a reasonably simple procedure, the FCC has allocated a number of
HF ranges to the IG (industrial/business pool) service. As is usual in the IG
service, frequency allocations are not exclusive and there may be other users.
Interestingly, FCC regulations place a significant break point at 25MHz: 25MHz
to 30MHz is technically HF, but FCC rules don't really differentiate between
these frequencies and the more common business pool uses in VHF. Below 25MHz,
though, special rules apply.</p>
<p>47 CFR 90.266(b):</p>
<blockquote>
<p>Only in the following circumstances will authority be extended to stations to operate on the frequencies below 25 MHz:</p>
<p>(1) To provide communications circuits to support operations which are highly important to the national interest and where other means of telecommunication are unavailable;</p>
<p>(2) To provide standby and/or backup communications circuits to regular domestic communications circuits which have been disrupted by disasters and/or emergencies. </p>
</blockquote>
<p>As is often the case in federal regulation, there are some additional terms
that require a little closer reading. 47 CFR 90.35, which governs the
Industrial/Business Pool radio service, has a table of possible frequency
allocations. For the range below 25MHz, the table specifies the following
restriction, in part:</p>
<blockquote>
<p>(c)(i) Only entities engaged in the following activities are eligible to use this spectrum, and then only in accordance with s 90.266:</p>
<p>(A) Prospecting for petroleum, natural gas or petroleum products;</p>
<p>(B) Distribution of electric power or the distribution by pipeline of fuels or water;</p>
<p>(C) Exploration, its support services, and the repair of pipelines; or</p>
<p>(D) The repair of telecommunications circuits. </p>
</blockquote>
<p>Available bandwidth at these low frequencies is already rather constrained by
the allocations, but the situation is worse than it appears. HF propagation
behavior mean that radio operators seldom have their choice of anywhere in the
HF spectrum; usually there are only limited "windows" in which propagation is
good. It would not take very many users to create congestion in this valuable
long-range spectrum, so licenses are limited to users without other options.</p>
<p>Unsurprisingly, special restrictions below 25MHz mean that frequencies just
above 25MHz are quite popular. For example, from 25MHz-25.5MHz you can find a
veritable who's-who of the petroleum industry, who use HF radio for
communications with off-shore oil platforms. You also find some other peculiar
licenses in this range. For example, Ritron is a manufacturer of radio
equipment and holds a license for the use of 25-50MHz for the purpose of
demonstrating and testing that equipment. This 25-50MHz range is more or less
"low band VHF," which was formerly in reasonably common business use, but is
now becoming rare. Online discussion leads me to believe that it may not be
possible (or is at least very difficult) to obtain IG licenses for the low
band today, but some people that had them continue to renew them.</p>
<p>When I say "people" here, I mean it. One of the more common types of licensee
for this range is... people, filling out their application with a title of
"self" or "person." Although it is public record I am hesitant to give names or
addresses of these people, but it explains a lot about them that you can almost
always (invariably, from my spot-checking) find an amateur radio license for
the same person. I have known some amateur radio operators that obtained their
General Radiotelephone Operator license, a broad license for maintaining
certain types of commercial radio equipment, more or less for the hell of it. I
suppose obtaining a low-band IG license is similar, and adds some more bands
to your potential operations.</p>
<p>Let's limit our consideration further, then, to the rarefied frequencies below
25MHz. Besides special justification, applicants for these frequencies are
limited to certain emission types (generally narrow 2.8kHz emissions), must
use equipment capable of tuning across the entire range, must submit their
written communications plan, and are prohibited from testing or exercises
that exceed seven hours per week. There are relatively few such licenses, the
ULS returns just 61.</p>
<p>I totaled up the licenses by user type. For companies that provide radio
services, I categorized them with the industry they normally serve, except when
that industry was emergency communications itself. This sometimes required a
bit of a subjective call, as we'll see when looking at some specific licenses.
But here are license counts by type:</p>
<ul>
<li>Telecom providers: 23</li>
<li>Electrical utilities: 15</li>
<li>Emergency communications/disaster relief contractors: 11</li>
<li>Petroleum: 7</li>
<li>Railroads: 2</li>
<li>Ranches: 1</li>
<li>Weird: 1</li>
</ul>
<p>Let's discuss these a bit by category. Telecom providers are a fairly obvious
user group, as are electrical utilities. Both types of organizations operate
infrastructure over large areas and will be expected to begin recovery quickly
after a natural disaster or another event that disrupts conventional
communications infrastructure. These licensees include AT&amp;T, with the single
largest license count (9), Pacific Bell dba AT&amp;T California at the second
largest (4), down to smaller entities like the Grand River Dam Authority.
A notable selection is National Grid USA Service Company, which despite its
generic and almost sketchy name holds a fixed location license for the Nine
Mile Point Atomic Power Station among other power plants.</p>
<p>Verizon New York Inc. holds a license for a number of fixed locations including
an inconspicuous brick building (reminiscent in its design of early telephone
infrastructure but likely today a remote exchange) in Philmont, NY and a
mountaintop site near Schenectady. The license seems to cover multiple
telephone exchanges, only some of which have apparent HF antennas... some of
the locations listed may be historic, nearly all commercial HF licenses are
old, with renewal histories stretching back to the beginning of online records
in 2001. Besides, virtually all of these licenses include either a nationwide
mobile or nationwide temporary fixed location, making the listed locations
less important than they might otherwise be.</p>
<p>Most licenses are also nonspecific as to frequency. It is the nature of HF
radio that any given frequency will not reliably provide good propagation. The
frequency lists on commercial HF licenses routinely stretch on for multiple
pages of 20 ranges each, giving operators ample choice. Besides, the CFR
requires these licensees to be frequency-agile across the band, in part because
the FCC may require them to stop using a given frequency at any time.</p>
<p>The next largest category of licensees are companies that provide
communications services to relatively nonspecific customers. Some, such as
L3Harris, are principally in the defense industry but likely also provide
services to infrastructure providers and regional governments. A few are tiny,
like Hazard Zone Technology LLC, which does business from a residential
address. Judging by forum discussions of the FCC's approach to HF licensing, it
is possible that some of these smaller licensees are largely fictitious
entities created to entice the FCC to issue a sub-25MHz license despite the
restriction to emergency communications. While not conclusive, the fact that
the same individual holds both a general radiotelephone license and an amateur
extra license with a vanity call sign is certainly suggestive of a certain
personality. There are several such examples, not an inconsiderable portion of
the 61 total.</p>
<p>Others are familiar names in an unfamiliar context. Cisco Systems holds a
license that lists their San Jose and Research Triangle Park campuses, besides
a nationwide mobile location. This license is likely intended to cover the use
of HF equipment as a backup point-to-point link for customers of fully managed
industrial communications services, but it's hard to say exactly. They have
apparently demonstrated a mobile communications trailer for emergency response
coordination at conferences.</p>
<p>Just those three categories get us into a long tail. A railroad, CSX, holds two
licenses. A ranch holds one; it's unclear how the ranch would qualify under the
requirements but they may have held the license long enough to be
grandfathered.  The justification listed on the license is simply farm
operations, not a 90.266 justification statement as found on most of these. The
sub-25MHz restrictions have applied since 1983, making it difficult but
certainly not impossible to hold a grandfathered license. One license is held
by a missionary transportation group, they may have also held the license since
before 1983 and list a location in Guam. They likely use HF to communicate with
facilities on outlying islands.</p>
<p>There is exactly one license that I have described as "Weird." It was issued to
the Bran Ferren Corporation in 2000. Bran Ferren is a bit of a character, a
former executive at Walt Disney Imagineering who apparently has a side business
of building off-road vehicles. This helps explain the justification statement,
"MANUFACTURER AND DEVELOPER OF ALL TERRAIN COMMUNICATIONS VEHICLE." I would
like to know exactly which vehicle this relates to, but the Bran Ferren
Corporation keeps a low profile as compared to Ferren's main business venture,
Applied Minds. Bran Ferren Corporation holds an active USDOT motor carrier
number, but has had zero vehicles inspected in the last two years. That may not
mean anything, they are listed as a private (not for hire) carrier and inspections
may not be required. It's all a bit of a head-scratcher.</p>
<p>These 61 licenses for sub-25MHz commercial radio represent only a tiny fraction
of the activity in the HF band. Besides amateur radio, a long list of
government users are authorized to use HF radio by the NTIA, rather than FCC.
Indeed, the NTIA master file includes at least hundreds of entries under 25MHz,
more detail will have to wait on my finally finishing the parser for the more
recent format they have used for FOIA disclosures (universal rule: if you spend
many hours writing tools to parse a PDF export of a relational database back
into a relational database, they will change the format of the PDF).</p>
<p>High-frequency traders have created renewed interest in HF radio, because of
its low latency for global communications and the increasing ease of
implementing automatic link negotiation with SDRs. <a href="https://sniperinmahwah.wordpress.com/2018/05/07/shortwave-trading-part-i-the-west-chicago-tower-mystery/">Sniper in
Mahwah</a>
has some well-known writing on this topic. To date, the FCC has authorized this
activity only on an experimental basis. In 2023, a group called the Shortwave
Modernization Coalition (SMC) and consisting of a group of HFT firms submitted
a petition for rulemaking to allow regular use of the 2-25MHz range. The
petition opened docket RM-11953, which remains open. Most recently, the FCC
conducted a series of meetings between the SMC and federal spectrum users to
discuss the impacts of such a new radio service.</p>
<p>Various documents filed by SMC do contain interesting details. SMC members are
operating at least fourteen experimental HF sites, with one operating since
2016 and most since 2020. The median transmit power (EIRP) is 21.5kW, and the
experimental licenses authorized 5.06-30MHz while SMC members conducted most
activity between 6.675 and 21MHz. The ARRL and various amateur radio operators
have filed comments opposing the change, a competing HFT radio operator has
discussed a counter-proposal that would impose performance obligations on
commercial HF radio operators ("use it or lose it" rules). The FCC has not yet
produced a Notice of Proposed Rulemaking, the next step in the process. They
are not obligated to, and it is possible the proposal will not reach that
stage. For its part, the federal government itself has weighed in (in the form
of comment from the NTIA), and has requested that the FCC either extensively
study how such commercial HF use would mitigate interference with federal
users, or exclude the use of any frequencies allocated to federal use. This
includes certain bands like the Radio Astronomy service allocation at 13MHz
that exist as a matter of federal policy due to foreign agreements. For my
own part, I am skeptical that the FCC will act on the SMC petition unless the
scope of the SMC's proposed use is reduced.</p>
<p>Finally, it is worth noting that these commercial HF licenses do not represent
the full extent of private industry use of HF radio for continuity of
operations. Some critical infrastructure operators have been sponsored by
federal agencies as operators of SHARES stations. However, SHARES documents
suggest that there are relatively few of these stations (around 100),
suggesting that they almost completely overlap with commercial HF licenses.
State and municipal governments also operate HF radio stations, which are
generally licensed under public safety radio services. A curious exception is
the City of Lafayette, Louisiana, which holds an IG commercial license for HF
frequencies. I suspect that license is actually for the use of the
publicly-owned Lafayette Utilities System (it lists electrical distribution as
justification), and was issued in the name of the City of Lafayette for
bureaucratic reasons.</p> ]]></description>
	</item>

 	<item>
		<title>2024-09-26 the GE switched services network</title>
		<pubDate>Thu, 26 Sep 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-09-26-the-GE-switched-services-network.html</link>
		<guid>https://computer.rip/2024-09-26-the-GE-switched-services-network.html</guid>
		<description><![CDATA[ <p>We currently find ourselves in something of a series, working our way from
<a href="https://computer.rip/2024-09-08-private-lines.html">private lines</a> to large
private line systems like the four-wire private-line <a href="https://computer.rip/2024-09-14-the-national-warning-system.html">national warning
system</a>.
Let's continue to build on the concept of the private line into large
corporate systems.</p>
<p>In principle, a large organization in want of a private telephone system could
build one out of a set of private lines and switches, such as under a Centrex CU
(Customer Unit) arrangement. And this did happen: one common type of private
line was the tie line, a private line used to link two switches together (which
could be PABXs or Centrex) so that users could call from one to the other
without using a conventional dial telephone line. This could save money, if
usage of the tie line was heavy enough and especially if the two switches were
far enough part that a standard call would be long-distance.</p>
<p>Consider a corporation with two large offices, each with a PABX. If they are in
different local calling areas, calls between them placed by dial line would be
long distance. If employees at the two offices call each other often, the long
distance bills would add up to more than the fixed monthly cost of a tie line
from one office to the other. There are a few different ways to solve this
problem, such as getting WATS (wide area telephone service) at one or both
offices, but it illustrates the general idea that getting a fixed private line
can sometimes be a cost-saving measure compared to placing a lot of calls over
standard dial service.</p>
<p>But what about a bigger organization, with many offices? You can imagine that
getting a huge number of tie lines between different offices, planning where
those tie lines should be located and how many were needed on each link, could
become a feat of traffic engineering on par with the telephone company's own
work. It might be easier to just pay the telephone company to work it out, and
indeed, that's what large organizations often did.</p>
<p>So let's say the telephone company meets this request by designing a scheme of
tie lines and Centrex exchanges. It's not so far off the mark to say that this
describes AUTOVON. AUTOVON was a complete system of tandem exchanges and at
least semi-private telephone lines provisioned by AT&amp;T [1] for use by the
military.  The problem with this arrangement is that it is very expensive: the
customer is paying the telephone companies to purchase, install, and maintain a
huge amount of hardware, just for the customer's private use.</p>
<p>Now, comparing AUTOVON on a price basis is both difficult and unfair. Difficult
because AUTOVON was paid for in a somewhat complex way, by the military paying
a central, cumulative rate for the entire system and then performing cost
recovery from individual user agencies and installations using a non-trivial
cost assignment calculation. It is also often said that the Bell System
themselves did not recover the full cost of AUTOVON from the military and that
it was, to some extent, subsidized by other telephone services.</p>
<p>And I say unfair, because AUTOVON was more than just a private telephone
network. It was a <em>hardened</em> private telephone network, with four-wire service
and a precedence capability that required the development of novel equipment.
It wasn't really expected to save money compared to the public telephone
system, because it was acquired in order to provide capabilities that the
public telephone system did not.</p>
<p>Still, we can safely say that AUTOVON was expensive. A 1979 study by the
Defense Communications Agency, responsible for AUTOVON cost recovery at that
time, comes out to an impressive total of $255,492,000 in AUTOVON operating
costs for FY 1978. By way of example, the report puts the monthly service cost
of a two-way capability from CONUS to Europe with priority precedence at
$1,182. Obviously this example case is one of the most expensive, but I still
shudder to imagine a monthly phone bill of over eleven hundred dollars in 1979
money. The military was willing to swing the stiff cost of AUTOVON because,
first, it was the military and they were willing to swing the stiff cost of
many things, and second because AUTOVON's military capabilities would be very
expensive to build by any means. It was the Cold War, after all, and it could
be said that outspending the Soviet Union was a military objective.</p>
<p>The situation was rather different when it came to non-military communications.
The civilian federal government ran up some enormous telephone bills between
its many offices, and initially considered purchasing an AUTOVON-like system to
serve as a private network between federal offices. The concept simply wasn't
cost effective, it likely would have increased the cost of federal telephone
calls overall. The Federal Telecommunications System or FTS would eventually
come to be, but not in the form of a private switched system. It is, after all,
intuitive that cost savings would not come from installing a great deal of
dedicated hardware. Rather, the Bell System would have to find a way to serve
these large institutional customers with <em>less</em> investment. And that was the
Common Control Switching Arrangement, or CCSA.</p>
<p>It is very tempting to draw an analogy between the CCSA and virtualization in
contemporary computing, but it is probably more accurate to draw an analogy
between the CCSA and LPARs in IBM hardware, or even more aptly, to
virtualization's early precedent in the limited subdivision capability of
Babbage's difference engine. Let's stop indulging the temptation and explain it
more directly: a CCSA is created by configuring existing telephone switches to
treat a subset of their lines as part of a separate network.</p>
<p>The technical details by which this was achieved varied significantly by the
switch. CCSAs were introduced in the early 1960s and could be configured on the #5
crossbar exchange, where "configuration" consisted of strapping or jumpering
certain components of the switch to operate independently of the others. CCSAs
continued just about to the modern era, where configuration became a matter of
selecting the appropriate lines in the business office system that generates
configurations for computer-controlled exchanges.</p>
<p>I think that it's most interesting to examine the CCSA by way of example---by
looking at a specific, real CCSA. BSP 310-200-007 I2 (1966) conveniently
provides a directory of the code numbers that were used to identify CCSAs
within the telephone system. Number 02 is FTS, the Federal Telecommunications
System. I didn't bring it up without reason, the concept of the CCSA was
developed in large part in order to bring the cost of FTS under control.
We can ponder what happened to number 01, but I'm guessing that AT&amp;T used
that code for testing and thus reserved it, or maybe even to identify the
public telephone system.</p>
<p>One could think of the normal, public telephone system as just another CCSA,
although as I understand it this was not the nature of the actual
implementation. Another appealing analogy for the CCSA is the VLAN, we could
think of these CCSA network numbers as VLAN tags. In this analogy, the public
telephone system is the PVLAN, sometimes called 0 or sometimes called 1 at the
whim of vendors. If you are familiar with VLANs, that somewhat illuminates why
I say that the public telephone network is not just another CCSA: it is the
"untagged" network into which equipment not capable of CCSAs and lines not
attached to a CCSA are presumed to exist. Anyway, that's all besides the point,
what other CCSAs existed?</p>
<p>04, General Electric. 05, New York Central (railroad), 06 Lockheed, 07 State
of California, 08 AUTOVON (used to facilitate expansions of AUTOVON over
non-AUTOVON telephone infrastructure, as a more cost effective way to provide
AUTOVON lines at smaller installations), 09 American Airlines, 10 Boeing,
11 Westinghouse, 12 Western Electric, 13 IBM, 14 North American Aviation.
That's the complete list as of 1966, and while short, it is a who's-who of
the industrial giants of the post-war United States. Plus the State of
California. Most state governments used large Centrex-and-WATS arrangements,
but some combination of the large size of California and GTE's different
approach to the network steered them in the CCSA direction.</p>
<p>Of these CCSAs, I will focus on General Electric. There are two reasons: first,
GE had an early and large CCSA---the largest CCSA outside of the federal
government, at the time. Second, I was an intern at a failing GE business in a
large, half-abandoned corporate campus [2] during the summer that the last
vestige of the GE Switched Services Network, as AT&amp;T called it, was retired.
Among my scattershot duties was working on the decommissioning of the campus's
Nortel PABX in favor of Cisco UCM. GE SSN would go with it, replaced by IP
trunking between UCM sites.</p>
<p>In 1963, the GE SSN spanned fifteen central offices ranging from New York to
Los Angeles, all #5 crossbars. It was intended to provide voice as well as data
at 1200bps.  Unlike some (mostly federal) CCSAs, it was designed to provide
standard two-wire dial service only, without station-to-station four-wire
connections or call precedence. In other words, it was a standard telephone
network, but intended to make calls between GE offices more reliable and less
costly than calls over the long-distance telephone network.</p>
<p>One of the complications of the GE SSN, and of CCSAs in general, is the
diversity of telephone equipment in use across the different corporate offices.
The GE of 1963 had Centrex service, step-by-step PABXs, crossbar PABXs, key
systems, and manually operated PBXs. All of these were integrated into a
7-digit dialing scheme for GE SSN, with the NNX prefix (different from NXX used
in the public telephone network by prohibiting a 0 or 1 in the second digit
position) identifying a location on the network such as a PBX, and the
four-digit subscriber number generally being the telephone's local extension,
padded with arbitrary digits as needed to be four digits long. Of course, the
details were less tidy, with smaller locations sharing prefixes and some
locations acting almost like toll stations with single telephones on the
GE SSN and selecting it by key.</p>
<p>In general, though, at PABX-served locations, extension users had a choice as
to how to place their call. The configuration wasn't the same at every office,
but the recommended practice was to use a 9 prefix (or "exit code") to dial on
the public telephone network, and an 8 prefix to dial on the GE SSN. Most PABXs
have some version of this capability: specific trunks can be selected for
outgoing calls based on the dialing prefix.</p>
<p>At locations with manual exchanges and locations without compatible PABXs, GE
SSN calls had to be placed with the assistance of the local PBX operator. Still
other locations used a small PABX connected via tie line to a larger PABX at a
larger office, in this case the dialing prefix "18" was recommended to first
dial a trunk from the satellite PABX to the main PABX, and second from the main
PABX to the telephone exchange providing GE SSN service.</p>
<p>Indeed, let's reflect a bit on the wiring scheme involved. </p>
<p>CCSAs were served by Offices, like the fifteen I mentioned for GE. FTS and
AUTOVON had more offices (AUTOVON's CCSA office list tellingly includes the
proper AUTOVON exchanges), but most CCSAs had fewer, sometimes only a handful.
Between these offices, trunk capacity could be shared with normal  telephone
traffic, giving CCSAs a significant cost advantage. Individual phones (or PABXs
or etc) on CCSAs needed to be connected to an actual CCSA office, though, in
order to have access to the CCSA at all. This made practical CCSAs sort of a
hybrid situation.</p>
<p>A given GE office might have lines running directly to the serving office, for
example in New York City where there were two offices on the GE SSN. Offices
that weren't near a 5XB included in the scheme, though, would need to somehow
be connected up to one. The BSPs are not replete with details, but presumably
this was done using the fairly conventional foreign exchange service.</p>
<p>I think I have mentioned this before but I will provide a very short summary.
In the VoIP industry, it is extremely common to identify the "ends" of a
telephone subscriber loop using the terms FXO and FXS, for Foreign Exchange
Office and Foreign Exchange Station. Confusingly, the terms refer to where a
given connection "goes" rather than where it "is," with the result that a
Foreign Exchange Office connection is what you would plug into a phone, and a
Foreign Exchange Station connection is what you would plug into a device like
an ATA---something that provides talk battery, ringing, etc., the traditional
role of the telephone office.</p>
<p>So the "Office" and "Station" part of those terms makes sense, besides the fact
that they are arguably the opposite way around from what you would first think.
But what about the Foreign Exchange? Well, these terms predate VoIP by decades,
and were originally used to identify the ends of a foreign exchange service.</p>
<p>Foreign exchange was a specific type of private line that allowed a phone to be
connected to a different central office from the one that physically served it.
There are different reasons that this was useful, but a common one had to do
with long-distance rates and suburban areas: it was historically common in
large metro areas that suburbs could call into the city at local rates, but
suburbs could not call into other suburbs at local rate... that would be a long
distance call. You can see that this provides a bit of an economic advantage to
city phones. So if you are, say, a plumber with your shop located in a suburb,
you might pine after a big-city telephone line that would allow more of your
prospective customers to call you for free. Foreign exchange could solve that
problem.</p>
<p>When you ordered foreign exchange service, the telco connected your phone line,
at the distribution frame of your local central office, directly to a private
line. The private line went to a central office in the Big City, where it was
connected at the distribution frame to a local line served by the switch. In
practice there were complications and details to how this was set up, but this
description gives you the idea: your telephone was now connected to the switch
in a different central office from the one your local loop was actually
connected to.</p>
<p>Foreign exchange service was expensive, because it took up private line
capacity, so various combinations of WATS, InWATS, zenith numbers, toll-free
numbers, etc. have pretty much replaced it. Some of the terminology got stuck
in our modern telephone parlance, though. FXO and FXS were designations used by
the telco to keep track of which ends of the private line needed to connect to
what equipment. Why was I talking about this, though? Oh, right, because large
CCSAs in practice also relied on what was basically foreign exchange service in
order to connect outlying locations to CCSA offices.</p>
<p>It's a good thing, here, that most of these GE offices had PABXs. This limited
the number of outside lines that needed to actually go to a CCSA office. What I
am calling foreign exchange lines could also be viewed as tie lines, not really
serving phones but providing trunks from PABXs to CCSA offices.</p>
<p>The nature of the CCSA is pretty much in its name: it is a switched private
line service, but it makes use of common control equipment to minimize costs.
I have tried to make terminology a little simpler here, but I have kept saying
"GE SSN." Switched Services Network, or SSN, seems to have been a term used by
the Bell System to refer to any private switched network. A CCSA was one of the
ways of implementing an SSN, and seems to have been the most common throughout
telephone history. There were not many truly private switched systems. AUTOVON
could be considered an example, although it had requirements above and beyond
typical telephone service. That leaves the FAA as a purer example, as the FAA
used a significant number of private line services for both switched and
unswitched communications between air traffic control sites and equipment.</p>
<p>Incidentally, a precursor to AUTOVON was called SCAN, the Switched Circuit
Automatic Network. SCAN was a US Army four-wire system, because four-wire
service was required for the cryptographic equipment of the era to function.
AUTOVON seems to have inherited its four-wire nature directly from SCAN.
Skimming through a telephone tariff almost always turns up some interesting
details, one of them being that a few state telephone tariffs describe
Switched Service Networks as being private line service based on either CCSA or
SCAN. Given that these same 2024 telephone tariffs define SCAN as a federal
government service for secure communications, this definition (and the presence
of an entry for SCAN at all) seems to be purely a holdover from some
fifty-year-old tariff documents. It does go to show that non-common control
switched services networks were uncommon enough that telcos viewed AUTOVON as
the odd exception to the CCSA rule.</p>
<p>I am trying not to get too tied up in the history of AUTOVON, because it is
easily its whole own article. I do think it is fair to say that the CCSA
emerged largely as a response to the high price of AUTOVON, as building FTS
based on the pattern of AUTOVON was deemed completely unrealistic on a cost
basis. FTS launched in 1963, not far behind AUTOVON at all, but consisted of
CCSA service for long-distance calling with PABXs in government offices
furnished under lease agreements. Once the heavy lifting had been done for
FTS, it was natural to extend CCSA as an offering to large private companies.</p>
<p>FTS also holds some of the seeds of the Bell System's undoing. High costs and
lackluster service plagued FTS in its early years. During the 1970s the General
Service Administration, which was responsible for FTS, decided to introduce a
competitive bidding process for long-distance capacity. That lead to companies
like Western Union and MCI joining the network, and the introduction of
least-cost routing to select the carrier. These ideas would also spread from
government to private industry, helping to set up the industry-wide tensions
that culminated in the 1982 breakup of AT&amp;T.</p>
<p>A common term in the telephone industry is "Universal Service." In the modern
world, universal service is understood to be the goal of providing telephone
service to all customers. The Universal Service Fund, for example, levies a fee
on telephone lines to subsidize the provision of telephone service to those who
would otherwise be unable to afford it. This is a recent invention. 1960s
documentation on CCSAs makes repeated reference to Universal Service, a very
different form of the concept championed by AT&amp;T more or less until its
breakup: that everyone would be served by one, unified telephone system, the
Bell System. During the early days of the telephone, before AT&amp;T's monopoly was
cemented, Universal Service was a rallying cry against competing telephone
companies, whose independent networks interfered with the ability of any
telephone to call any other. In the mid-century, it became a term somewhat like
public switched telephone network (PSTN). CCSAs were capable of Universal
Service, where desired, on a somewhat limited basis, in that CCSA exchanges
could be configured to allow calling outside of the CCSA, into the public
system.</p>
<p>It is amusing, then, that AT&amp;T was so willing to abandon the ideal of universal
service when their customers offered to pay for it. But that's business, and in
the mid-century, AT&amp;T was one of the biggest businesses in the world. CCSAs
show us some of the ups and downs of the age of the telephone monopoly: CCSAs
were an innovative concept that was rapidly developed and delivered, first to
the government and then to private customers, over a span of just a few years.
They were also frightfully expensive, and offered a new level of lock-in that
kept customers from using competitive carriers.</p>
<p>It's hard to find any contemporary information about GE's private telephone
system. It has almost entirely vanished from history, except in the form
documented by the early BSPs. I don't even quite remember what it was called
when I was at GE, I think it might have had "star" in the name. There was a
printed directory to help you figure out the correct office code and means of
transforming an extension in order to dial over the system. I don't think the
copy I saw was recent, I'm not sure if a recent copy even existed. The system
was barely used at all. It was replaced by a common modern arrangement, least
cost routing with IP trunks.</p>
<p>When calling another GE office, the Cisco Call Manager installation would
connect the call over the data network to the other office's Call Manager.
Very practical, very easy to use, kind of boring.</p>
<p>[1] Even prior to divestiture, the practical construction and operation of
these telephone systems was split between AT&amp;T Long Lines and the telephone
operating companies. Many, but not all, of these were subsidiaries of AT&amp;T.  In
the case of AUTOVON, for example, we must consider that non-AT&amp;T subsidiary GTE
built and operated part of the system. I'm trying not to get bogged down in
this complexity, but I'm also trying not to keep writing "AT&amp;T" when referring
to work done by multiple companies, some of them independent. Please do me the
kindness of understanding that when I use terms like "the telephone company" or
even "the Bell system" I am trying to encompass all of the parties, AT&amp;T, Bell
Operating Companies, independents, etc. that were involved in this work.  The
term I use may not be exactly correct.</p>
<p>[2] It was GE Intelligent Platforms, and the beautiful but poorly maintained
corporate estate in Charlottesville, itself the remains of a failed joint
venture,  might have been a more suitable exterior for <em>Severance</em> than Bell
Labs Holmdel.</p> ]]></description>
	</item>

 	<item>
		<title>2024-09-14 the national warning system</title>
		<pubDate>Sat, 14 Sep 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-09-14-the-national-warning-system.html</link>
		<guid>https://computer.rip/2024-09-14-the-national-warning-system.html</guid>
		<description><![CDATA[ <p>Previously on Deep Space Nine, we discussed the extensive and variable products
that AT&amp;T and telephone operating companies sold as <a href="https://computer.rip/2024-09-08-private-lines.html">private
lines</a>. One of the
interesting properties of private line systems is that they can be ordered as
four-wire. Internally, the telephone network handles calls as four-wire with
separate talk and listen pairs (or at least, it did before digitization). For
cost reasons, though, service to individual customers is virtually always
two-wire, with talk and listen combined onto a single pair via hybrid
transformers. Four-wire private lines are just about the only exception.</p>
<p>Why? Well, one of the major advantages of four-wire service to the telephone
instrument is that it avoids the echo and sidetone that normally occur within
the hybrid transformers. On a call between two telephones, this effect is
acceptable and even desirable. In conference systems, though, with many phones
attached, echo accumulates until the line is almost unusable. Prior to the
introduction of DSP technology to "clean up" the audio, multiparty conferences
were a lot more limited than we take for granted today... except for the
four-wire private line systems specifically built for large conference calls.
The most notable of these is the National Warning System, or NAWAS, operated by
AT&amp;T for the Federal Emergency Management Agency (FEMA).</p>
<p>FEMA has an interesting history. It is most directly a product of the
Department of Housing and Urban Development, where was originally established
in 1973 with the responsibility for coordinating reconstruction after natural
disasters. Over time, a series of federal reorganizations expanded FEMA and
added additional roles. Most notably, in 1979 President Jimmy Carter instituted
a major reorganization of federal emergency agencies that dissolved the Office
of Civil Defense, made FEMA an independent agency, and placed all civil defense
responsibilities within FEMA. As a result, part of FEMA is a direct descendent
of the civil defense efforts at the peak of the Cold War. FEMA operates the
government relocation bunker at Mt. Weather, for example, and by the same coin
is responsible for the dissemination of attack warnings to the contiguous
United States.</p>
<p>The origins of NAWAS can be traced back to the Civil Defense Warning System
(CDWS), often known as the "Bell and Lights System," which was introduced in
the 1960s and is itself an extension of some earlier precedents. The various
iterations and renaming of NAWAS make it a little bit difficult to trace its
history exactly. Wikipedia says that NAWAS was formed in 1978, a reasonable
claim given that FEMA organized around that same time. But NAWAS cannot have
been new in '78: AT&amp;T published a BSP covering the "OCD NAWAS," prior to FEMA's
existence, a full decade earlier in 1968. Indeed, NAWAS and the CDWS or "Bell
and Lights" must have operated in parallel, as both had BSPs issued that same
year.</p>
<p>That's not actually that surprising: one of the reasons that the nation's
emergency communications networks kept being replaced is because the
requirements kept changing. CDWS was designed primarily as an automated or
machine-to-machine system, capable of activating air raid sirens and sounding
local alarms automatically. During the height of the Cold War this was a good
fit for the intent. The emergency scenario was nuclear attack, and a warning
would need to be disseminated as rapidly as possible for optimum lifesaving
effect. The Office of Civil Defense once targeted a 30-second timeline from
declaration of an alert to the American public becoming aware.</p>
<p>Even as CDWS was put into service, though, OCD was aware that a more extensive
communications capability would be required to distribute information on attack
outcomes, evacuation and recovery efforts, and to enable continuity of
government even in a possible scenario of devolution of control to local
emergency response authorities. That need would be served by NAWAS: not an
alarm system, but a voice communications system, ready for two-way use on
state, regional, and national scales.</p>
<p>Over time, FEMA has shifted its emergency alert programs away from nuclear
conflict and towards the "All-Hazards" model, in which the scope of the systems
is interpreted broadly and the primary use tends to be natural disaster and
weather alerts. The "All-Hazards" concept came about mostly because of a sense
that FEMA and the National Weather Service were pointlessly duplicating
capabilities; so because of All-Hazards thinking, FEMA NAWAS distributes
critical weather alerts and NWS's weather radio network distributes FEMA
alerts. NAWAS has thus changed its identity from a wartime system to a more
general emergency management system.</p>
<p>Let's see how NAWAS actually works. NAWAS is, at its core, a network of
interconnected four-wire conference circuits. A national line connects the
Warning Centers to the Regional Warning Centers, eight regional loops connect
the Regional Warning Centers to states and other federal warning points, and 48
state circuits connect warning points within each state. Each of these circuits
or loops is essentially a party-line or conference line. If you pick up one of
the phones, you'll hear if someone is talking on any of the others.</p>
<p>Nationwide messages generally originate in the National Warning Center or its
alternate. Historically, the National Warning Center was at Cheyenne Mountain,
Colorado (with NORAD where alerts would most likely originate), and the
Alternate National Warning Center was at Denton, Texas. There was a second
alternate facility in 1968 at Olney, Maryland. At some point, FEMA seems to
have "promoted" the sites to remove Cheyenne Mountain,  making Denton the
primary and Olney the alternate.</p>
<p>Denton and Olney were both the locations of FEMA regional headquarters, which
have gone by various names over time like Special Facilities or Federal Support
Centers. They were originally built by Civil Defense or FEMA (depending on the
year) to coordinate the recovery from nuclear attack, and as such they were
hardened. Perhaps the most famous is the Region 8 "FEMA Bunker" at the Denver
Federal Center. Other FEMA regional headquarters tended to be either in more
remote areas or were built by repurposing existing military facilities (such as
that at Olney, a reused Nike missile site), and as such kept a lower profile.</p>
<p>It's hard to know what exactly is going on today. In the 2016 NAWAS Operating
Manual, the most recent version that seems to have been made public, the
Alternate Warning Center is directly named as Olney, MD but the location of the
primary is left unsaid. The Olney facility has been transferred to the Naval
Surface Warfare Center and is no longer in use by FEMA. A document suggests
that the Alternate Warning Center may have moved to Thomasville Federal Center,
near Atlanta. Denton, TX remains a major FEMA site and may still be the
primary.</p>
<p>The primary control circuit connected the (originally) three warning centers
with the eight regional headquarters, but considering that some of the regional
headquarters were themselves warning centers, it had fewer points that it might
sound like. That circuit served as one of several ways (including the HF radio
system FNARS) that these major FEMA sites could communicate with each other in
a major disaster, but it was less important for alert dissemination.</p>
<p>The more important parts are the regional circuits, which are configured as
loop to provide redundancy against a line break. The regional circuits connect
a FEMA Regional Headquarters, at least one Warning Center for redundancy,
federal alerting points (an AT&amp;T document lists Coast Guard stations as an
example), and a primary and alternate warning point within each state. FEMA
would use these circuits as the main way to distribute a nationwide alert,
reading it directly to the state Warning Points, which are generally the offices
of the state's emergency management agency. For example, in New Mexico, the
Primary Warning Point is the Department of Public Safety office in Santa Fe;
the Alternate Warning Point is the state Emergency Operations Complex at the
National Guard complex by Santa Fe.</p>
<p>There are likely some additional levels of redundancy in most cases. For
example, state emergency planning documents imply that the Department of Energy
has a NAWAS site at Kirtland Air Force Base that is connected to at least the
regional and state networks, and possibly the national control circuit as well.
It thus serves as an additional contingency for distribution of alerts on the
state network, were the Primary and Alternate Warning Points to be lost.</p>
<p>Each state circuit is left in part to the discretion of the state, with criteria
stated by FEMA. Generally, it includes the state warning points along with county
emergency operations centers and major infrastructure facilities like hospitals
and power plants.</p>
<p>Along with the specialized purpose of NAWAS come specialized equipment. NAWAS
relies on a system that AT&amp;T calls SS1, presumably Signaling System 1, but is
more similar to a selective calling scheme than to a more general signaling
system. SS1 appears fairly similar to the control pulses used by CDWS, but
repurposed to ring phones on NAWAS to get a user's attention. On the national
control circuit, for example, the National Warning Centers have SS1
transmitters that can be used to signal SS1 receivers at the Regional
Headquarters to ring. Keep in mind that this is conference system, so there is
no real sense of "placing a call" or "hanging up." The telephones are always
connected. The provision of selective calling is just to get another user's
attention so that they pick up the phone. In practice, they likely won't even
pick up the phone, as most NAWAS sets are equipped with an always-on speaker to
monitor activity on the circuit.</p>
<p>The SS1 selective ringing system also allows the National Warning Centers to
selectively call state warning points on the regional circuits. State warning
points, at least those operated by the state itself, are able to selectively
call other sites on the state network. And that's actually nearly the limit of
the SS1 capabilities. All other selective calling is done by "voice paging,"
basically yelling into the phone in the hopes that the party you want is
listening to their speaker.</p>
<p>There is one other interesting capability of SS1, which requires understanding
the structure of the network, beyond just the circuits. At each state primary
warning point, the regional NAWAS circuit is actually bridged to the state
warning circuit, so that any traffic on the regional circuit will also be heard
on the state circuit. Essentially, in its "normal" state, the whole state
circuit is just a leg of the regional circuit, and all sites on it hear
regional traffic. This ensures that a warning read by one of the National
Warning Points will be heard as quickly as possible through the over 2,200
phones in the total NAWAS system. This connection is only present on one pair,
though, so it's one way: the state circuit hears traffic on the regional
circuit, but the regional circuit does not normally hear traffic on the state
circuit.</p>
<p>As originally designed, a foot switch in each state Primary Warning Point
disconnects the two networks when depressed, allowing the Primary Warning Point
to "speak" on the state circuit only. The foot switch basically selects which
of the two networks the Primary Warning Point phone will transmit onto, but
also disconnects the bridge so that the Primary Warning Point can speak on the
state network even if the regional network is busy. You can imagine, though,
that this would pose a problem when a critical alert needs to be distributed.
To guard against a stuck footswitch or just a particularly chatty state
emergency manager, the National Warning Centers can send an SS1 code that will
throw a relay to bypass the footswitch and reconnect the regional and state
networks. This code would be sent just before any critical nationwide warning.</p>
<p>The use of four-wire conference systems usually requires a slightly different
type of telephone set, anyway. One of the goals of four-wire systems is often
to function as a "squawk box" or "hotline" (one of the many definitions of the
word), an always-on system that can be heard on a speaker at every connected
location. As a result, every NAWAS warning point has a loudspeaker. There is
also a telephone handset, used to speak into the system. Because there are many
phones on each NAWAS circuit and you know how large Zoom calls tend to get,
each NAWAS handset has a push-to-talk button. You don't see these very often
today, but Western Electric offered telephone handsets with a PTT button
mounted on the inside of the handle as a standard product. Apparently depending
on the preference of the installation site, the loudspeaker is automatically
disabled either when the handset is picked up or only when the PTT button is
depressed. This prevents feedback or echo. It might seem a little odd to hear a
conversation on the speaker but speak into a handset, but this was very common
in mid-century telephony, as the echo and feedback problems of a true
"speakerphone" proved hard to solve.</p>
<p>At State Warning Points, an indicator light attached to the telephone set helps
the operator understand what circuit they are connected to. Green means that
the footswitch has been depressed to disconnect the state network from the
regional network. White means that another location, presumably the State
Alternate Warning Point, has depressed their footswitch to disconnect the two
systems (the bridge was duplicated at the two sites, but one site pressing
their footswitch would disconnect the bridge at the other site as well). A red
light indicated that the National Warning Center had bypassed the footswitches
to distribute an alert.</p>
<p>BSP 310-530-901 LL Issue A (1968) provides a general description and operating
information on NAWAS as originally built. Much of the BSP relates to the
bureaucracy of operating a special service on a nationwide telephone system: it
lists, for example, the specific plant control offices responsible for the
testing and maintenance of each circuit. The Cheyenne Mountain Central Office
must have been an interesting place, with primary responsibility to handle
trouble reports and network management on the national and regional circuits.
Each regional circuit was assigned to a CO within the region as well, typically
one in a major city near the corresponding FEMA regional headquarters.</p>
<p>These plant control offices are informed that they must report all outages and
work done on the circuits to OCD, and that they must take special precautions
to ensure uptime on the NAWAS circuits. 8.05 states, in characteristically dry
AT&amp;T tone, that "the circuit may be required by the customer at a moment's
notice due to the nature of the business for which it is used." I have heard
anecdotes that switchmen working in central offices were sometimes used to
hearing routine chatter on the NAWAS, and I see that the BSP suggests (but does
not require) that control offices responsible for the system have a dedicated
monitoring speaker. Proving that customer service is always the hard part, the
BSP requires that these control offices make regular visits to customer sites
and emphasize the importance of keeping the loudspeaker volume turned up,
using the PTT button on the handset, and promptly reporting any trouble.</p>
<p>The BSP also lays out a process for routine maintenance on this critical,
high-uptime system, which AT&amp;T refers to as "circuit line-up." Such line-ups
are performed only on Saturday, and must be authorized by the National Warning
Center and announced to all Warning Points. Once started, the line-up time
allows the test rooms at each control office to perform routine quality
measurements on the circuits and detect any problems requiring maintenance.
The test room must keep the circuit audible on a speaker for the duration, so
that they can immediately stop their testing should the National Warning Center
transmit that they need the circuit returned to carry emergency traffic.
Following each line-up, the National Warning Center would place test calls to
each State Warning Point to verify correct functioning, and the test room was
expected to stay on the line to monitor this process. One can imagine that the
whole thing felt like a hassle to the test room personnel who had to stay
perpetually on edge.</p>
<p>The modern Operations Manual for NAWAS is available via FOIA, or at least a
version from 2016. It mostly describes the details of operating the station
equipment, which have not changed all that much since the 1968 BSP. The
original Western Electric sets have been replaced by NAWAS terminals
manufactured by Comlabs, a small company focused on emergency communications.
They appear to be modified AT&amp;T or Lucent phones with a new dialpad (including
the elusive DMTF ABCD "digits"), an alert light, speaker volume knob, and
presumably the electronics swapped out for four-wire operation. Phones used in
local warning points have no keypad at all, since they are not intended to
issue alerts. The pulse-based SS1 system has been replaced by DTMF; selective
calling is done using four-digit numbers while commands like linking and
unlinking regional and state networks use the DTMF "A" digit.</p>
<p>Besides some modernization of the terminals, the sites on the system have
expanded along with its scope. NWS forecast offices, NOAA facilities like the
Hurricane Center, and FEMA's "MERS" mobile response teams have been added to
the regional circuits. The NWS runs its own round of tests to ensure it can
distribute weather alerts to State Warning Points.</p>
<p>You might be somewhat familiar with NAWAS. I have mentioned it before, and in
general, it's one of the best known of the government's various emergency
communications system. No small part of this is due to a compelling bit of
drama available in the documents. The manual has scripts.</p>
<blockquote>
<p>This is the FEMA (Alternate) Operations Center. A nuclear weapon detonated in
(city, county, state) at _____ Zulu. Radioactive fallout is possible! Persons
in (city, county, state) should be advised to remain under cover and await
further instructions from state or local authorities. Residents are advised
to take protective actions in accordance with local community shelter plans
and to be alert for further instructions from state or local authorities.
Residents in all other areas are advised that protective action is not
required at this time.</p>
</blockquote>
<p>This is actually somewhat lengthy considering that the audience for the message
is mostly state emergency management agencies. Alerts of this type on NAWAS
would be proceeded by an audible alert tone to attract attention, and followed
by a roll-call of all state warning points to ensure that the message was
received. Local authorities would do... something. In the event of an attack,
they will theoretically activate local sirens with a wavering tone for 3-5
minutes. FEMA procedures dictate that this signal exclusively indicates a
confirmed impending or in-progress attack. In practice, it is either the same
as or not readily distinguishable from the signal used for tornadoes in many
tornado-prone areas of the country---generally the only areas with sirens at
all. FEMA procedures have often coped poorly with the use of warning systems
for purposes other than those that were apparent during the Cold War.</p>
<p>The manual provides scripts for other scenarios, ranging from a detection by
NORAD (probably actually by intelligence community assets) of possible fires
to to reentering space debris, errant weapon launches (nuclear or
conventional), and various natural disasters. One of the newer additions to the
procedure is use by the Pacific Tsunami Warning Center to disperse warnings of
tsunamis affecting the west coast. This is one of the newest disaster scenarios
to prompt serious investment in mass notification, and large parts of the west
coast are now equipped with sirens in case of tsunami.</p>
<p>NAWAS has a close connection to
<a href="https://computer.rip/2021-04-26-iPaws.html">IPAWS</a>, the Integrated Public
Alert and Warning System. Most NAWAS alerts would simultaneously be issued as
IPAWS alerts, distributed across <a href="https://computer.rip/2021-04-23-this-is-a-test-of-the-computer.rip-alert-system.html">radio and television
stations</a>
and via the newer Wireless Emergency Alert system. Much like IPAWS, only
federal authorities can issue national NAWAS alerts, but regional authorities
like state governors are permitted and even encouraged to use state NAWAS
circuits to disseminate local alerts. Indeed, most state governors residences
have NAWAS terminals installed for ready access.</p>
<p>NAWAS has provided reliable service for many decades, but now shows its age.
Private-line telephone systems like NAWAS are fundamentally challenging to
harden against attack and disaster due to their fixed routing. Besides, the
statewide conference line capability of NAWAS now seems rather limited compared
to the popularity of text messaging in emergency management. In 2022, FEMA
awarded AT&amp;T a $167 million contract to modernize multiple FEMA communications
systems, including NAWAS. The plans for NAWAS are vague:</p>
<blockquote>
<p>AT&amp;T will transition the NAWAS legacy technologies to newer services available
via EIS through a well-planned, phased, cost-effective, and non-disruptive
approach to the new solution with government oversight.</p>
</blockquote>
<p>Sounds great. Can't wait for the new solution.</p>
<p>So there we have it, a large, nationwide communications network made up of
four-wire private lines. There have actually been a number of these used over
time, including notably the Strategic Air Command's C2 network which involved
both radio links and private lines terminating at telco-furnished
<a href="https://computer.rip/2021-05-19-telephone-turrets.html">turrets</a> at SAC bases.
Some smaller-scale four-wire systems remain in use today for local emergency
management purposes. </p>
<p>Later, possibly as soon as next week, we will take a look at a much more
complex type of private line service: Common Carrier Switching Arrangements.
But I might get sidetracked on the road to CCSAs once again, and post first
about the federal private line services that lead to the invention of the CCSA:
AUTOVON and the Federal Telephone System (FTS).</p> ]]></description>
	</item>

 	<item>
		<title>2024-09-08 private lines</title>
		<pubDate>Sun, 08 Sep 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-09-08-private-lines.html</link>
		<guid>https://computer.rip/2024-09-08-private-lines.html</guid>
		<description><![CDATA[ <p>I have been meaning, for some time, to write about common carrier switching
arrangements (CCSAs). These could be considered an early form of products like
"virtual private ethernet:" a private telephone network that was served by the
same switching machines that handled the public telephone system. A CCSA is, in
effect, a "virtual telephone network." AT&amp;T operated a number of these for both
government agencies and large private organizations, and they might be viewed
in a way as precursors to the large CENTREX-and-WATS arrangements that became a
common fixture of state governments and school districts.</p>
<p>The problem is that I fear I am putting the cart before the horse. CCSAs, and
even the fully private telephone systems they were intended to replace, are
basically the extreme extension of the private line. Besides, private lines are
an important part of the history of computing, as well: they were the pattern
for the digital "leased line" services that formed the bulk of computer network
connections through the early days of the internet.</p>
<p>Let's ease into it by starting with an important source in telephone history:
the Bell System Practices.</p>
<p>Large organizations tend to function like religions. This is more or less
overt, depending on the organization and the fashions of the time. For example,
in the period we will mostly discuss in this article, a number of large
companies published song books. IBM is the best known for this type of
corporate spirit, ranging from their de facto hymnal to the "Think" signs
customarily installed in IBM worker's offices. The television series
"Severance" frequently referred to this aspect of post-war corporate culture,
depicting a corporation that unified its employees through songs, quotations,
and an internal museum. This type of pseudoreligious corporate culture is, of
course, taken to an extreme in "Severance," but nonetheless resembles real
practices that still echo through corporate America today.</p>
<p>Severance depicts its ominous corporation with exterior shots of the Bell
Laboratories offices at Holmdel, a prestige design by Eero Saarinen that dates
to an era in which the architecture of corporate offices was often an
idealistic representation of their intended culture [1]. This is just a little
bit ironic, as Severance is clearly patterned more after the computer industry
than AT&amp;T, which was comparatively subtle in its corporate religion. Still,
there were aspects of the sublime: Saul Bass's pitch film for AT&amp;T's 1960s
rebrand devotes much of its length to a hagiography of the Telephone Men.
Universal Service, AT&amp;T's stated goal, was both as ambitious and ever-changing
as "redemption," personified as the enormous, golden "Spirit of Communication"
depicted in sculpture and mural at former AT&amp;T offices. And AT&amp;T had signs! Of
course they had signs; the best known being the "Bell System Safety Creed" that
hung in most work areas.</p>
<p>This is all to preface my use of the word "doctrine." The kind of shared
culture that companies attempted to establish through song books is also an
important part of practical operations. We associate doctrine mostly with
religion, but more broadly, a doctrine is a set of codified, shared beliefs and
practices that define the work of an organization. The military has doctrine in
this sense, and AT&amp;T has doctrine as well. Much of that doctrine was compiled
into a lengthy, almost sacred text: the Bell System Practices, or BSPs.</p>
<p>BSPs are an invaluable source of information, and fortunately organizations
like
<a href="https://www.telephonecollectors.info/index.php/browse/bsps-bell-system">TCI</a>
have put a lot of effort into finding and preserving the remaining copies.
Unlike publications like the Bell System Technical Journal (BSTJ), BSPs were
for internal use only, and so the collection available today is scattered and
incomplete. That's not the only challenge when using BSPs for research. Fitting
the analogy to holy texts, the organization of the BSPs is arcane and has
undergone a number of changes. For the latter half of the 20th century, BSPs
were identified by nine digit numbers, separated into three groups of three
digits. The first group identified the Division, the second the Section, and
the third the Document.</p>
<p>BSP 000-000-000 provides an index to the Divisions. Each division begins with a
section index, numbered NNN-000-000. Sections are typically grouped by their
first digit, producing topics referred to by NNN-N. NNN-0 is devoted to
indexes, NNN-1 is usually a "general information" topic. There are loose
conventions for document numbering. For the weightier topics, -100 is often the
overview or general procedures. There are as many exceptions as examples of
these rules. Further complicating document lookup, BSPs were published roughly
from 1930 to 1990, numbering conventions were changed and sections reorganized,
and documents were replaced by updates identified by an issue number. The aspect
of time can make it a frustrating and time-consuming process to match up a set
of BSPs that not only cover a topic but represent a single point in time,
rather than being confused by a decade of changes from one to the next.</p>
<p>We can hope that, one day, a monastic order of telephone enthusiasts will take
up the compilation and, while they're at it, illumination of the BSPs. They
will produce a Xanadu-esque compendium of AT&amp;T doctrine navigable along
dimensions of both time and space; a Grand Enfilade of communications
technology. For my own part, I am perhaps alarmingly close but not quite ready
to take a vow of silence, move yet further into the desert, and devote my life
to the task. For the time being we'll have to settle for search engines, the
indexing efforts of a few dedicated people, and no small amount of sweat.</p>
<p>Enough of that, let's take a look at divisions 310 through 312, which cover
private lines for voice and data, as well as special switched services. A
discussion of private lines starts rather naturally with 310-300-100, Two
Point Private Line Systems: Two Point Private Line Telephone Circuits,
Voice Only---Description.</p>
<p>Of course, we will not start there, because of the joy of BSPs: Naturally, I
have searched several archives and not been able to find a copy of 310-300-100.
We can fill in some of this gap from 310-300-300 and 310-300-500, though, Test
Objectives and Test Procedures. 310-300-300 I1 (1975) 1.04:</p>
<blockquote>
<p>The circuits discussed in this section are served over nonswitched facilities
with no access to the message network. A two-point private line involves a
channel between two terminal locations. The channel may or may not be
entirely a metallic path.</p>
</blockquote>
<p>A private line is, at its essence, a pair of wires that runs from one location
to another. The locations are customer-specified, and the private line is not
connected to anything else, only to the two service points. Some version of
this service has existed for pretty much the entire history of the telephone,
although it has often, as you would imagine, been very expensive.</p>
<p>The private lines covered in this section are for voice use only. This is
important, because voice use implies a specific set of requirements. The topic
of private lines, leased lines, and related services is actually a very
difficult one to discuss succinctly. First, because there is a 100-year history
of these services, and the technology, capabilities, and use-cases have all
changed over time. Second, because there is a very diverse set of uses for
private lines, each with a different set of requirements.</p>
<p>Voice use, for example, implies a customary telephone voice passband of about
300-3400Hz, and more precisely limited by the 8kHz sampling rate of digital
telephony. This might strike you as surprisingly narrow, but it's enough for
reliable speech intelligibility. It really is quite narrow though, part of the
reason that "HD Voice" codecs now offered by VoIP technology sound startlingly
different from traditional calls.</p>
<p>The fact that the circuit may not be entirely metallic is important as well.
It might be a little unclear what a "nonmetallic" path would even mean, but
remember that we are looking at a 1975 revision. By that time digital telephony
was in full swing; a customer's "pair from one place to another" may very well
be digitized, carried by the TDM switching network, and converted back to
analog (analogized?) near the other end. The possibility of carrying private
line service over the digital switching system radically reduced the cost of
private lines, and enabled the proliferation of "leased lines" for use with
computers.</p>
<p>But, it did have limitations. A private line of the 1930s would have been able
to carry polarity reversal signaling, a common scheme at the time for alarm
monitoring and remote control. A nonmetallic path was not electrically
continuous, so it could not be used for any type of electrical signaling. It
would only work for the signaling scheme it was provisioned for, namely, voice.</p>
<p>The BSPs describe a set of tests intended to ensure suitability for that
purpose.  There are standards for loss, frequency response, and noise. We also
learn that private lines for voice use can be configured for ringing, so that
one end can signal the other to pick up the phone. Elsewhere in the BSPs the
supported types of ringing are enumerated, although they no doubt changed over
time: ringing by a ring key (today we'd call that a ring button), by a
hand-crank generator, by central office equipment, or no ringing at all.</p>
<p>I would like to cover this topic without getting too stuck in terminology, but
it's hard to avoid because there is a lot of terminological confusion. For
example, what is the difference between a "private line" and a "leased line"
anyway? Well, it's mostly a matter of who's talking. AT&amp;T seems to have always
used the term "Private Line" except when discussing data services in the
context of switched data networks, in which case they use "Leased Line" to
refer to a fixed-capacity data connectivity arrangement. We'll get to that
in a bit.</p>
<p>Another problem is how to describe the configuration of private lines. A
private line provisioned for voice use, as described in 310-300, is not
connected to any switching equipment but would be connected to battery power
(AT&amp;T called this "common battery," meaning that the phones did not require
their own batteries to function). Private lines with common battery were
sometimes referred to as "wet," while private lines without common battery were
"dry." The latter were mostly used for non-telephone signaling applications.</p>
<p>Private lines might also be connected to other types of equipment to provide
useful features. For example, you might wonder about how ringing applied by the
central office would work. A popular type of private line for voice, especially
later on, was the "Private Line Automatic Ringdown" (PLAR), also just called a
"Ringdown" circuit. These private lines are configured so that picking up the
phone at either end automatically applies ringing voltage to the other end.</p>
<hr />
<p>Shameless cross-promotion: I uploaded a <a href="https://www.youtube.com/watch?v=8QYmG060s5E">YouTube video</a>
yesterday where, among other things, I briefly mention that "hotline" is a
heavily overloaded term that means different things in different contexts.
One of the things that "hotline" often refers to is a PLAR. Mostly, though,
I try out a very cheap PABX I bought on the internet and find out that, while
not exactly great, it is certainly pretty good for the price.</p>
<hr />
<p>Another option on private lines was whether they were two-wire or four-wire. A
typical telephone uses only two wires, one pair. If you think about it, that's
a little bit magical. The trick is a device called a hybrid transformer,
installed in each telephone, that basically "subtracts" the signal it is
transmitting from the signal on the line in order to isolate the "receive"
signal. At a modern telephone exchange, another hybrid transformer splits them
out once again for handling by the exchange equipment. This is a two-wire
circuit, and has the advantage of lower-cost wiring. On private lines, you can
also get four-wire service, with dedicated pairs for transmit and receive (also
called, for obvious reasons, talk and listen). The major advantage of four-wire
circuits is that they make "conference calling" much easier. When you put a lot
of telephones onto one two-wire circuit, it becomes very difficult to manage
the cumulative echo produced by the imperfection of the hybrid transformers.
A full four-wire system avoids this problem.</p>
<p>What were these private lines <em>for</em>? Well, you can probably imagine a few
applications for voice private lines, but keep in mind that for much of their
history they were very expensive... prior to the use of TDM digital networks,
you were basically paying AT&amp;T for the installation and upkeep of however many
miles of telephone wire were required to get between the two locations... plus
carrier equipment, line conditioning, etc.</p>
<p>One of the cool things about the BSPs, and one of the reasons they were kept
internal, is that it's not at all unusual for them to go into detail on
specific customers. We can get a feel for the use-cases for these service
offerings from the specific customers that used them heavily enough to merit
BSPs. For two-point private line telephone service, that customer is the FAA.
The FAA has long had a tight relationship with AT&amp;T, rivaled only by the
military for reliance on dedicated telephone infrastructure. Air-traffic
controllers used voice private lines both to talk to each other across
facilities, and to remotely operate the radios they used to communicate with
aircraft.</p>
<p>Voice private lines were also popular in other situations where people across
multiple locations needed to communicate in real-time. For example, television
and radio networks often had private lines between studios and network control
centers to aid the staff in coordinating the start and stop of programs from
different locations. The telephone system was an obvious choice, because for
the early history of radio and television most networks used "broadcast-grade"
private lines (with larger bandwidths than voice lines, especially for
television!) to distribute their programming to the member stations. The radio
and television networks were a huge business for AT&amp;T prior to the use of
satellite transponders for the same purpose.</p>
<p>I know of some more exotic applications as well. Some critical infrastructure
or large industrial facilities, and in particular nuclear power plants with
their extensive emergency preparation requirements, had four-wire private line
systems that linked their control rooms to contingency sites, disaster response
agencies, and even the homes of their senior staff. These were configured as
conference lines for use in coordinating an emergency response. Indeed,
emergency management was another major application of private lines, and I will
eventually write about FEMA's nationwide four-wire private line system, still
used for dissemination of emergency warnings to state and regional emergency
management agencies.</p>
<p>These conference lines are, as you have likely suspected, not really
two-station private line systems. They are multistation voice systems,
described in 310-405-100. TCI has a much older issue of this BSP, from 1957,
and it describes the electrical resistor networks used to achieve the
"conference line" configuration in which the talk pairs are mixed and sent on
the listen pairs of all stations. 310-405-100 I1 (1957) 1.03:</p>
<blockquote>
<p>One of the most important requirements of a multistation private line
circuit is that the volume delivered to each receiver be substantially
the same no matter which station is talking. The above holds true even when
two or more circuits are switched together.</p>
</blockquote>
<p>Indeed, and we are still struggling with this today!</p>
<p>1.04 is interesting:</p>
<blockquote>
<p>Various signaling systems and combinations of signaling systems can be used
to "call" the various stations on a private line circuit. Some types of
signaling systems are: loudspeaker signaling, manual code ringing, ringdown
signaling, 600-1500-cycle (2-tone) selective signaling, and code selective
signaling.</p>
</blockquote>
<p>Explaining all of those would be an article of its own, but it is an
interesting note that multistation systems challenge the typical sense of
"ringing" on a telephone line, and conventional ringing doesn't seem to have
been very common on four-wire systems. A common replacement is what is
described as "loudspeaker signaling" here, where the stations have an always-on
speaker and you get someone's attention by... yelling at them, basically.</p>
<p>AT&amp;T has never limited themselves to voice. There have long been various types
of control or signaling equipment available, and 310-435 describes the SC2
Selective Control System available for use with multistation private lines.
This telegraph-like system involves a control station sending series of coded
pulses, which are detected by satellite stations that open and close relays in
response. There are still a lot of products like this today, but the SC2 as a
private line offering is a good time to make a point about the historic phone
system: it used to be that all telephone equipment was the property of the
telephone company. That included the SC2 control and satellite stations, and
they had a terminal strip on the outside of the cabinet that exposed the relay
contacts and served as the demarcation point between telco and customer
property.</p>
<p>That has an interesting implication for these private line systems: the
operating company needed a way to test and diagnose them. As a result, larger
or higher-criticality private line systems usually included some kind of
terminal equipment at the customer location that provided test functionality.
On four-wire systems, a loopback test was the norm: some sort of signal sent on
the line would cause a relay to shunt the talk pair to the listen pair at the
customer premises, allowing the test board at the exchange to send signals all
the way "around" the private line. The SC2 had a pretty complicated piece of
terminal equipment, since it had to decode the signal pulses, and as a result
there is an extensive test procedure with parts that can be performed remotely
and parts that must be done on-site. The BSP cautions that arrangements must be
made with the customer to disconnect their equipment before a telephone
technician starts opening and closing relays to test.</p>
<p>A final common application of voice private lines was for "tie lines," or any
of the other names they went by. These were lines that ran between two
switching systems, providing something like a long-distance trunk between them.
Imagine a corporation with two offices, each with a private branch exchange.
The corporation could contract for a private line between the two PBXs that
served as a tie line, and the operators at each location could then use it to
directly connect calls between the two offices. Besides avoiding the operators
having to dial those calls, tie lines could also save money over long-distance
calling, if utilization was high enough. A number of services offered by AT&amp;T
and operating companies basically amounted to different ways of using tie
lines, so Division 311 "Switched Special Services Systems" covers tie line
configurations including WATS and operating company-managed PBXs.</p>
<p>Of course, if you have read this far, you are probably wondering about data.
In the contemporary computer industry we associate this kind of private service
entirely with the leased lines of the '70s and '80s. Division 312 covers Private
Line Data Systems and Services, beginning with Electronic Telegraph Loops and
teletypewriters. In the 1950s, "data" pretty much meant teletypewriter, and
there is a dedicated customer section on Western Union who used AT&amp;T circuits
to extend their own network. The first half-dozen topics are devoted to various
types of telegraph systems, including DC telegraphy (over continuous metallic
circuits) and carrier telegraphy (combining multiple telegraph channels onto one
private line using frequency division muxing).</p>
<p>312-8 is where we get to the good stuff, Data Sets. Data set was the term used
by AT&amp;T for what we now know as a modem, for example, the Bell 103 modem is
more properly the 103 Data Set. The term "data set" predates acoustic modems
and is somewhat more general, though, having been used by AT&amp;T to refer to
simple relay closure systems as well. Still, by the 1978 312-000-000 index
there are a range of different data sets available, covering different speeds
and applications. The systems covered in this section are intended for use
with a complete private line data system, that is, the lines themselves were
part of the system. They were typically private lines that terminated at
dedicated equipment in the telephone exchange, where a data bus was used to
carry signals between different lines.</p>
<p>It is in Division 313 that we find what we'll recognize today: Voice and
Voiceband Data Circuits. These are data systems that operate over voice-type
lines, using acoustic modem methods to encode data within the frequency
response of those lines. This division is actually surprisingly short, for
a reason. 313-100-100 I2 (1982) 1.01:</p>
<blockquote>
<p>Telephone company testing of circuits that terminate in customer premises
outlined in these sections includes only that portion of the circuit up to
the network interface. The circuit testing procedure does not include
customer premises terminal equipment.</p>
</blockquote>
<p>In practice, a lot of voiceband equipment would be provided by AT&amp;T, but they
are establishing a clear separation of concerns between a voice-type private
line and the modem used with it. It's important to understand that "network
interface" as used here has a specific meaning within the telephone industry,
it is the demarcation point between the telephone network and the customer's
equipment.</p>
<p>Higher-speed telephone modems like the Bell 201, capable of 2400bps, could
be used on four-wire circuits for full-duplex operation. Indeed, four-wire
circuits were quite common for data use as well because they enabled full
duplex operation. AT&amp;T was also not the only option for voiceband modems.
Division 314, Digital and Analogue Data Transmission Systems, is dedicated to
a variety of modems and I49 (1983) includes familiar names like IBM and
Data General.</p>
<p>This practice of using voiceband equipment to put data over private lines,
without requiring extensive special equipment at the central office, started
a shift in data communications practices that greatly blurred the lines between
different types of service. Some of the confusion of terms we encounter today
comes from these gray lines. The Bell 103 is a data set and can be used on
private lines, but it can also be used on a conventional dial line. Similarly,
the lines between voice and data were blurred. SAGE incorporated a nationwide
computer network that operated over voiceband modems, often considered the
first precursor to the modern internet. The digitization of the telephone
network would further complicate definitions.</p>
<hr />
<p>I have a long list of telephone-related topics to cover and I will probably
never get to all of them, although I sure will do my best. It takes quite a bit
of my free time to write these articles, and I'm also starting work on a more
ambitious project around telephone history in more of a reference format.  I'd
appreciate your support in pursuing these projects---consider <a href="https://ko-fi.com/jbcrawford">supporting me on
ko-fi</a>, which will also get you my
subscribers-only newsletter EYES ONLY.</p>
<hr />
<p>During the 1960s, AT&amp;T introduced TDM digital trunks to the telephone network.
Using digital technology, a large number of telephone calls could be digitized
into samples and those samples multiplexed onto a single high-speed data
connection between two telephone exchanges. This method of multiplexing was
more reliable and less prone to noise than analog FDM methods, and it could be
adapted to a wide variety of carriers. Over the following decades the telephone
network underwent a wholesale conversion to digital, and it is now typical that
the only "analog" parts of an analog telephone call are the last mile
connections between the exchange and the customer premises. Modern telephone
exchanges digitize calls at the line cards and it remains digital until
reaching a line card on the other end. We were doing data over voiceband, and
then we were doing voiceband over data.</p>
<p>The implied result, data over voiceband over data, was in fact very common and
the apex of dial-up internet standards (v.90/v.92) assume that the underlying
telephone connection is digital. The oddity of stacking multiple layers of
digitization was far more apparent on private lines, though, where there was
no requirement to retain compatibility with a standard telephone loop.</p>
<p>And thus the leased line was introduced. The closest thing I know of to a
technical difference between a "private line" and a "leased line" is that a
private line is assumed to be private over the entire span, while a "leased
line," in practice, refers only to the last-mile. The actual telephone network
is digital and using TDM or even packet switching methods, there is no need
for a dedicated physical connection between two central offices to carry data.
The leased line just allows a customer a way to insert data into the telephone
network.</p>
<p>While v.92 assumes a bidirectional digital connection, it is limited to 56kbps
because of properties of standard phone lines including the use of companding.
Leased lines didn't have to put up with this limitation, they could omit the
functions of a telephone line card and instead deliver a digital signal all
the way to the customer premises. While the telephone network itself went
through several major iterations of digital media (including the charmingly
named "plesiochronous" network), the last-mile digital connection to customers
has been well-standardized for a surprisingly long time: the T-carrier.</p>
<p>Let's talk quickly about the Digital Hierarchy, the scheme around which the TDM
telephone network was designed. TDM involves packing samples into time slots in
a round-robin fashion, so the digital hierarchy is similarly organized around
cycles. As a hierarchy, those cycles get larger and larger. A telephone call
consists of 8-bit samples at 8kHz, which multiples to 64kbps of data. That
64kbps channel, in the digital hierarchy, is referred to as DS0 or Digital
Signal 0. By the somewhat arbitrary but pragmatic design of the digital
hierarchy, 24 DS0s are multiplexed to form a DS1. These DS designations refer
to the actual payload, not to the carrier technology used to transmit it.
DS1, in the United States, was most commonly carried by Transmission System 1,
commonly called T1 or T-carrier. 24 64kbps channels adds up to 1.544mbps, and
that's exactly what a T1 delivered.</p>
<p>This connection was digital the entire way through, with no need for conversion
to analog and the resulting quantization noise and bandwidth limitations. T1
was originally designed for trunk connections between telephone equipment, but
it became quite natural to extend T1 connections to customer sites as a form of
high-speed (for the time) data. T1 was the typical format of the leased line
even into the '00s, and most people my age probably remember having dial-up
service and coveting the remarkable speed of a T1 connection.</p>
<p>This is not to say that leased lines were limited to T1. There were
higher-speed systems used within the telephone network that carried higher
steps on the digital hierarchy, DS2, DS3, and so on, and these could be
extended to customer premises as well. At the high end, larger businesses would
be placed directly on a SONET fiber-optic ring via add-drop multiplexers, an
arrangement capable of multi-gigabit speeds.</p>
<p>It's a little odd, actually, that the "leased line" we think of has very little
to do with actual telephones. It's a digital network connection much like we
use today, except that it functions on top of the provisioned-bandwidth,
synchronous, TDM network originally built in order to carry telephone calls.
There are probably still organizations today running ethernet over SDH for
their internet connection, and it won't feel much different from anything else
we use.</p>
<p>[1] The use of architecture as a symbol of corporate power, once a fundamental
part of the computer and telecommunications industry, is largely lost today.
There are more than a few reasons, but one of them is the development history
of Silicon Valley. It is remarkable how underwhelming the offices of today's
most powerful corporations are, consisting of scattered low-rise office parks
with no identity besides the earth tones and angles of the 1970s. Of course,
when modern tech companies do build prestige headquarters, they tend to be
unspeakably ugly. I'm speaking mostly of Meta, I will give Apple's effort a
mediocre grade.</p> ]]></description>
	</item>

 	<item>
		<title>2024-08-31 ipmi</title>
		<pubDate>Sat, 31 Aug 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-08-31-ipmi.html</link>
		<guid>https://computer.rip/2024-08-31-ipmi.html</guid>
		<description><![CDATA[ <p>I am making steady progress towards moving the <em>Computers Are Bad</em> enterprise
cloud to its new home, here in New Mexico. One of the steps in this process is,
of course, purchasing a new server... the current Big Iron is getting rather
old (probably about a decade!)  and here in town I'll have the rack space for
more machines anyway.</p>
<p>In our modern, cloud-centric industry, it is rare that I find myself comparing
the specifications of a Dell PowerEdge against an HP ProLiant. Because the
non-hyperscale server market has increasingly consolidated around Intel
specifications and reference designs, it is even rarer that there is much of a
difference between the major options.</p>
<p>This brings back to mind one of those ancient questions that comes up among
computer novices and becomes a writing prompt for technology bloggers. What
<em>is</em> a server? Is it just, like, a big computer? Or is it actually special?</p>
<p>There's a lot of industrial history wrapped up in that question, and the answer
is often very context-specific. But there are some generalizations we can make
about the history of the server: client-server computing originated mostly as
an evolution of time-sharing computing using multiple terminals connected to a
single computer. There was no expectation that terminals had a similar
architecture to computers (and indeed they were usually vastly simpler
machines), and that attitude carried over to client-server systems. The PC
revolution instilled a WinTel monoculture in much of client-side computing by
the mid-'90s, but it remained common into the '00s for servers to run entirely
different operating systems and architectures.</p>
<p>The SPARC and Solaris combination was very common for servers, as were IBM's
minicomputer architectures and their numerous operating systems. Indeed, one of
the key commercial contributions of Java was the way it allowed enterprise
applications to be written for a Solaris/SPARC backend while enabling code
reuse for clients that ran on either stalwarts like Unix/RISC or "modern"
business computing environments like Windows/x86. This model was sometimes
referred to as client-server computing with "thick clients." It preserved the
differentiation between "server" and "client" as classes of machines, and the
universal adherance of serious business software to this model lead to an
association between server platforms and "enterprise computing."</p>
<p>Over time, things have changed, as they always do. Architectures that had been
relegated to servers became increasingly niche and struggled to compete with
the PC architecture on cost and performance. The general architecture of server
software shifted away from vertical scaling and high-uptime systems to
horizontal scaling with relaxed reliability requirements, taking away much of
the advantage of enterprise-class computers. For the most part, today, a server
is just a big computer. There are some distinguishing features: servers are far
more likely to be SMP or NUMA, with multiple processor sockets. While the days
of SAS and hardware RAID are increasingly behind us, servers continue to have
more complex storage controllers and topologies than clients. And servers,
almost by definition, offer some sort of out of band management.</p>
<p>Out-of-band management, sometimes also called lights-out management, identifies
a capability that is almost unheard of in clients. A separate, smaller
management computer allows for remote access to a server even when it is, say,
powered off. The terms out-of-band and in-band in this context emerge from
their customery uses in networking and telecom, meaning that out of band
management is performed without the use of the standard (we might say "data
plane") network connection to a machine. But in practice they have drifted in
meaning, and it is probably better to think of out-of-band management as
meaning that the <em>operating system and general-purpose components are not
required.</em> This might be made clearer by comparison: a very standard example of
in-band management would be SSH, a service provided by the software on a
computer that allows you to interact with it. Out-of-band management, by
contrast, is provided by a dedicated hardware and software stack and does not
require the operating system or, traditionally, even the CPU to cooperate.</p>
<p>You can imagine that this is a useful capability. Today, out-of-band management
is probably best exemplified by the remote console that most servers offer.
It's basically an embedded IP KVM, allowing you to interact with the machine as
if you were at a locally connected monitor and keyboard. A lot of OOB
management products also offer "virtual media," where you can upload an ISO
file to the management interface and then have it appear to the computer proper
as if it were a physical device. This is extremely useful for installing
operating systems.</p>
<p>OOB management is an interesting little corner of computer history. It's not a
new idea at all; in fact, similar capabilities can be found through pretty much
the entire history of business computing. If anything, it's gotten simpler and
more boring over time. A few evenings ago I was watching a <a href="https://www.youtube.com/watch?v=J1NxcgasTIU">clabretro
video</a> about an IBM p5 he's gotten
working. As is the case in most of his videos about servers, he has to give a
brief explanation of the multiple layers of lower-level management systems
present in the p5 and their various textmode and web interfaces.</p>
<p>If we constrain our discussion of "servers" to relatively modern machines,
starting say in the late '80s or early '90s, there are some common features:</p>
<ul>
<li>Some sort of local operator interface (this term itself being a very old
one), like an LCD matrix display or grid of LED indicators, providing low-level
information on hardware health.</li>
<li>A serial console with access to the early bootloader and a persistent
low-level management system.</li>
<li>A higher-level management system, with a variable position in the stack
depending on architecture, for remote management of the machine workload.</li>
</ul>
<p>A lot of this stuff still hangs around today. Most servers can tell you on the
front panel if a redundant component like a fan or power supply has failed,
although the number of components that are redundant and can be replaced online
has dwindled with time from "everything up to and including CPUs" on '90s
prestige architectures to sometimes little more than fans. Serial management is
still pretty common, mostly as a holdover of being a popular way to do OS
installation and maintenance on headless machines [1].</p>
<p>But for the most part, OOB management has consolidated in the exact same way as
processor architecture: onto Intel IPMI.</p>
<p>IPMI is confusing to some people for a couple of reasons. First, IPMI is a
specification, not an implementation. Most major vendors have their own
implementation of IPMI, often with features above and beyond the core IPMI
spec, and they call them weird acronyms like HP iLO and Dell DRAC. These
vendor-specific implementations often predate IPMI, too, so it's never quite
right to say they are "just IPMI." They're independent systems with IPMI
characteristics. On the other hand, more upstart manufacturers are more likely
to just call it IPMI, in which case it may just be the standard offering from
their firmware vendor.</p>
<p>Further confusing matters is a fair amount of terminological overlap. The IPMI
software runs on a processor conventionally called the baseboard management
controller or BMC, and the terms IPMI and BMC are sometimes used
interchangeably. Lights-out management or LOM is mostly an obsolete term but
sticks around because HP(E) is a fan of it and continues to call their IPMI
implementation Integrated Lights-Out. The BMC should not be confused with the
System Management Controller or SMC, which is one of a few terms used for a
component present in client computers to handle tasks like fan speed control.
These have an interrelated history and, indeed, the BMC handles those functions
in most servers.</p>
<p>IPMI also specifies two interfaces: an out-of-band interface available over the
network or a serial connection, and an in-band interface available to the
operating system via a driver (and, in practice, I believe communication
between the CPU and the baseboard management controller via the low-pin-count
or LPC bus, which is a weird little holdover of ISA present in most modern
computers). The result is that you can interact with the IPMI from a tool
running in the operating system, like ipmitool on Linux. That makes it a little
confusing what exactly is going on, if you don't understand that the IPMI is a
completely independent system that has a local interface to the running
operating system for convenience.</p>
<p>What does the IPMI actually <em>do?</em> Well, like most things, it's mostly become a
webapp. Web interfaces are just too convenient to turn down, so while a lot of
IPMI products do have dedicated client software, they're porting all the
features into an embedded web application. The quality of these web interfaces
varies widely but is mostly not very good. That raises a question, of course,
of how you <em>get</em> to the IPMI web interface.</p>
<p>Most servers on the market have a dedicated ethernet interface for the IPMI,
often labelled "IPMI" or "management" or something like that. Most people would
agree that the best way to use IPMI is to put the management network interface
onto a dedicated physical network, for reasons of both security and reliability
(IPMI should remain accessible even in case of performance or reliability
problems with your main network). A dedicated physical network costs time,
space, and money, though, so there are compromises. For one, your "management
network" is very likely to be a VLAN on your normal network equipment. That's
sort of like what AT&amp;T calls a common-carrier switching arrangement, meaning
that it behaves like an independent, private network but shares all of the
actual equipment with everything else, the isolation being implemented in
software. That was a weird comparison to make and I probably just need to write
a whole article on CCSAs like I've been meaning to.</p>
<p>Even that approach requires extra cabling, though, so IPMI offers "sideband"
networking. With sideband management, the BMC communicates directly with the
same NIC that the operating system uses. The implementation is a little bit
weird: the NIC will pretend to be two different interfaces, mixing IPMI traffic
into the same packet stream as host traffic but using a <em>different MAC
address.</em> This way, it appears to other network equipment as if there are two
different network interfaces in use, as usual. I will leave judgment as to how
good of an idea this is to you, but there are obvious security considerations
around reducing the segregation between IPMI and application traffic.</p>
<p>And yes, it should be said, a lot of IPMI implementations have proven to be
security nightmares. They should never be accessible to any untrusted person.</p>
<p>Details of network features vary between IPMI implementations, but there is a
standard interface on UDP 623 that can be used for discovery and basic
commands. There's often SSH and a web interface, and VNC is pretty common for
remote console.</p>
<p>There are some neat basic functions you can perform with the IPMI, either over
the network or locally using an in-band IPMI client. A useful one, if you are
forgetful and keep poor records like I do, is listing the hardware modules
making up the machine at an FRU or vendor part number level. You can also
interact with basic hardware functions like sensors, power state, fans, etc.
IPMI offers a standard watchdog timer, which can be combined with software
running on the operating system to ensure that the server will be reset if
the application gets into an unhealthy state. You should set a long enough
timeout to allow the system to boot and for you to connect and disable the
watchdog timer, ask me how I know.</p>
<p>One of the reasons I thought to write about IPMI is its strange relationship to
the world of everyday client computers. IPMI is very common in enterprise
servers but very rare elsewhere, much to the consternation of people like me
that don't have the space or noise tolerance for a 1U pizzabox in their homes.
If you are trying to stick to compact or low-power computers, you'll pretty
much have to go without.</p>
<p>But then, there's kind of a weird exception. What about Intel ME and AMD ST?
These are essentially OOB management controllers that are present in virtually
all Intel and AMD processors. This is kind of an odd story. Intel ME, the
Management Engine, is an enabling component of Intel Active Management
Technology (Intel AMT). AMT was pretty much an attempt at popularizing OOB
management for client machines, and offers most of the same capabilities as
IPMI. It has been considerably less successful. Most of that is probably due to
pricing, Intel has limited almost all AMT features to use with their very
costly enterprise management platforms. Perhaps there is some industry in which
these sell well, but I am apparently not in it. There are open-source AMT
clients, but the next problem you will run into is finding a machine where AMT
is actually usable.</p>
<p>The fact that Intel AMT has sideband management capability, and that therefore
the Intel ME component on which AMT runs has sideband management capability,
was the topic of quite some consternation in the security community. Here is a
mitigating factor: sideband management is only possible if the processor,
motherboard chipset, and NIC are all AMT-capable. Options for all three devices
are limited to Intel products with the vPro badge. The unpopularity of Intel
NICs in consumer devices alone means that sideband access is rarely possible.
vPro is also limited to relatively high-end processors and chipsets. The bad
news is that you will have a hard time using AMT in your homelab, although some
people certainly do. The upside is that the widely-reported "fact" that Intel
ME is accessible via sideband networking on consumer devices is typically
untrue, and for reasons beyond Intel software licensing.</p>
<p>That leaves an odd question around Intel ME itself, though, which is certainly
OOB management-like but doesn't really have any OOB management features without
AMT. So why do nearly all processors have it? Well, this is somewhat
speculative, but the impression I get is that Intel ME exists mostly as a
convenient way to host and manage trusted execution components that are used
for things like Secure Boot and DRM. These features all run on the same
processor as ME and share some common technology stack. The "management"
portion of Intel ME is thus largely vestigial, and it's part of the secure
computing infrastructure.</p>
<p>This is not to make excuses for Intel ME, which is entirely unauditable by
third parties and has harbored significant security vulnerabilities in the
past. But, remember, we all use one processor architecture from one of two
vendors, so Intel doesn't have a whole lot of motivation to do better. Lest
you respond that ARM is the way, remember that modern ARM SOCs used in
consumer devices have pretty much identical capabilities.</p>
<p>It is what it is.</p>
<p>[1] The definition of "headless" is sticky and we have to not get stuck on it
too much. People tend to say "headless" to mean no monitor and keyboard
attached, but keep in mind that slide-out rack consoles and IP KVMs have been
common for a long time and so in non-hyperscale environments truly headless
machines are rarer than you would think. Part of this is because using a serial
console is a monumental pain in the ass, so your typical computer operator will
do a lot to avoid dealing with it. Before LCD displays, this meant a CRT and
keyboard on an Anthro cart with wheels, but now that we are an enlightened
society, you can cram a whole monitor and keyboard into 1U and get a KVM
switching fabric that can cover the whole rack. Or swap cables. Mostly swap
cables.</p> ]]></description>
	</item>

 	<item>
		<title>2024-08-19 mining for meteors</title>
		<pubDate>Mon, 19 Aug 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-08-19-mining-for-meteors.html</link>
		<guid>https://computer.rip/2024-08-19-mining-for-meteors.html</guid>
		<description><![CDATA[ <h2>Billboards</h2>
<p>Route 66 is often viewed through the lens of its billboards. The Jack Rabbit
Trading Post, a small store a few miles out of Joseph City, would hardly be
remembered were it not for its billboards spanning four states. The tradition
of far-advance billboards is still observed today. Albuquerque's roadside stop
operator Bowlin puts billboards six hours and two freeway exchanges out
from its combined gas station-Dairy Queens. One can ponder the mystery of "The
Thing" (near Benson, Arizona) throughout nearly the entire state of New Mexico.</p>
<p>So, if you have driven anywhere over a several-hundred-mile span of Interstate
40 (the modern-day successor to Route 66 in this area), you are probably aware
of the Meteor Crater [1]. At Meteor, Arizona, between Flagstaff and Winslow,
the Meteor Crater earns its nonspecific name: it was the first crater
definitely shown to have been the result of a meteor impact. It is a
spectacular sight, almost 4,000 feet wide and 600 feet deep. It is also known
by another name: Barringer Crater, for the family that has owned it for over
one hundred years.</p>
<p>Today, Meteor Crater is one of the few traditional Route 66 roadside
attractions to have held on to much of its vitality. A steady flow of visitors
pay admission to see the crater and its attached visitors center, somewhat
ostentatiously styled as the Barringer Space Museum. The museum focuses on the
crater's two major connections to space: first, that it was formed by a meteor
that came from there. Second, and most importantly, that the crater has been
used as a training site for astronauts since the Apollo era.</p>
<p>I assume it has been calculated that these are the two topics that draw
visitors, because the museum devotes <em>almost</em> no space at all to what I
consider the most fascinating of the crater's many stories: that the crater was
a mine. Forget space; the Meteor Crater is the greatest artifact of a
fascinating, but brief, chapter of American mining history.</p>
<h2>Meteorites</h2>
<p>But first, as dismissive as I may be, we must talk a bit about space. During
the 19th century, detailed observations of meteor showers established that
the bright trails of light seen in the night sky---previously assumed to be
some atmospheric phenomenon---must in fact be the collision of objects from
space with the atmosphere. The idea that rocks or something were flying out
of space and into the upper reaches of our world naturally suggests that some
of them might make it all the way down. Indeed, as early as 1803 such a
meteorite [2] had been found on the ground in France following a meteor
shower, although the idea that it had fallen from above was not universally
accepted until much later.</p>
<p>Chemists analyzed a number of meteorites recovered during that era and found
that almost all of them contained significant amounts of iron, and their
geological oddities (such as the inclusion of spherical globules of metal)
supported the idea that they had formed in space. By the turn of the 20th
century, it was generally understood that meteors were chunks of mostly iron
that came from somewhere out there and collided with our planet, and that some
of them made it all the way to the ground.</p>
<p>What was not well understood by that time was the fate of those meteorites.
Since the 17th century it was known that there were craters on the moon, and
the idea that they had been formed by impacts is nearly that old as well. Even
three hundred years later, though, it was far from a settled matter. Volcanic
activity was also a promising explanation, and one that many felt to be more
comfortably within the bounds of reason. In the year 1900, many astronomers
would have been quite dismissive of the moon-meteorite theory, as the opinion
of the day favored a moon with an active volcanic core.</p>
<p>The idea that impacts had formed vast craters on Earth must have been even
more far-fetched. Besides, the known meteorites were quite small, more prone
to knocking holes in roofs than in the very desert. It was in this context
that the Meteor Crater was first examined.</p>
<p>Okay, we're done with that boring space stuff. Let's talk about mines!</p>
<p>In 1891, a small mining firm based out of Albuquerque received a sample from a
prospector in Arizona. The prospector had found an ore vein in a remote part of
that territory that he believed to be quite valuable. Indeed, an assayer put
the included sample at a remarkable 77% iron, with a bit of lead, silver, and
gold to boot. Observing its purity and unusual structure, the assayer figured
that it had been melted in a furnace.</p>
<p>The mining firm, amazed by this new ore and skeptical of the assayer's
attribution to a furnace, circulated parts of their sample among a number of
business leaders---men who might put up the money for a large-scale mining
operation. One of them, James Williamson of Civil War fame and by then an
executive the Atlantic and Pacific Railroad, sent his sample to mineralogist
Albert E. Foote of Philadelphia for his thoughts on its commercial value and
exploitability. Foote, recognizing the sample's unusual structure, knew
immediately that the prospectors claim of a vein two miles long and up to forty
yards wide could not be entirely true [3]. It was a meteorite.</p>
<p>In truth, the "ore" was one of a number of small fragments (where "small" still
often surpassed 100 pounds) that could be found over a large area near Cañon
Diablo and a feature known to locals as "Crater Mountain." Foote described the
"so-called 'crater'", "the sides of which are so steep that animals that have
descended into it have been unable to escape and have left their bleached bones
at the bottom." Foote carefully searched the rim, but found no evidence of
volcanic activity, leaving the crater's origin a mystery. Foote's blindness to
the crater's connection with the meteorite fragments is almost hard to believe,
but he seems to have taken it as a coincidence.</p>
<blockquote>
<p>The remarkable quantity of oxidized black fragmental material that was found
at those points where the greatest number of small fragments of meteoric iron
were found, would seem to indicate that an extraordinarily large mass of
probably 500 or 600 pounds had become oxidized while passing through the air
and was so weakened in its internal structure that it had burst into pieces not
long before reaching the earth.</p>
</blockquote>
<p>Indeed, that same year, the chief geologist of the USGS visited the crater and
reached the same conclusion. The crater's proximity to the meteorites seemed to
be chance. The crater itself must be the result of a vast steam explosion,
which was, after all, not an unknown phenomenon in that part of the Arizona
territory.</p>
<p>The late 19th century saw the formation of the Division of Forestry (of the
Department of Agriculture) and a related reorganization of the General Land
Office, precursors to today's Forest Service and Bureau of Land Management.
Large areas of public land would be withdrawn from the GLO's management (which
consisted mostly of sale to private owners) and reserved as National Forests.
In eastern Arizona, this controversial task fell on GLO agent S. J. Holsinger.
He traveled the region extensively in studying forest issues and establishing
the boundaries of what would become the Coconino National Forest. Although he
never saw it for himself, he heard stories of an enormous crater, near what he
called Coon Mountain or Coon Butte. The stories held that fragments of iron,
from a meteor, had been found within and around it.</p>
<p>The record is unclear on where or how, but one day in October of 1902,
Holsinger found himself in a casual conversation with a mine engineer by the
name of Daniel Barringer. Perhaps they sat by each other in a saloon or a train
station. Perhaps Barringer told Holsinger that he worked in iron mining, and
Holsinger said something along the lines of "I'll tell you about some iron." In
any case, Holsinger described the meteorites, and the crater---as they had been
described to him. Some of the locals, Holsinger said, had a theory: that the
crater had actually been formed by the meteorites. That a single meteorite
larger than ever seen before had crashed into the earth, burying itself far
below, and leaving behind the feature known as Coon Mountain, or Crater
Mountain, or Sunset Knoll as the USGS had labeled it on maps.</p>
<p>Barringer was hooked. He wrote back to Holsinger for more information, he
researched the area, and he involved his friend Benjamin Tilghman, the inventor
of sandblasting and a general scientific type in the pattern of the era, in the
pursuit. They became convinced that the USGS had been wrong, and the locals
right: that it was not just a crater, but a Meteor Crater. So, they bought it.</p>
<h2>Iron</h2>
<p>Although Barringer was well-qualified as a geologist and was clearly fascinated
by the crater, his interest in it was not purely scientific. In a 1905 paper
making his argument for its meteoritic origin, Barringer reports that the iron
fragments from the meteorite had already been commercially exploited. A nearby
merchant had hired laborers to search the area around the crater for the "iron
ore," and they had found some pieces as heavy as 1,000 pounds. Of the smaller
pieces that they collected, the merchant estimated that between his efforts and
those of another businessman, perhaps 15 tons of the iron-rich material had
been shipped away for smelting.</p>
<p>Barringer and his business partners had found thousands of fragments, ranging
from over 200 pounds to less than an ounce. These "Cañon Diablo siderites"
could be over 90% iron, the rest being mostly nickel.</p>
<p>Among the evidence he cites for the meteoritic theory, Barringer observes that
every single one of these fragments had been found immediately on the surface,
and only a handfull had ever been found inside of the crater. They were not
themselves buried by impact; they seemed to be found right where they lay after
they were ejected from the crater with great force.</p>
<p>Barringer's tone in the 1905 paper is, well, critical, particularly where it
comes to the USGS geologist who had declared the crater a product of a steam
explosion, Gilbert. Barringer suggests that Gilbert's rejection of the
meteoritic theory could only be the result of a profound failure to notice
geological inconsistencies that ought to have been obvious. The presence of a
great amount of fine silica in every direction from the crater (sometimes
called "rock flour," this very fine sand is the result of impact forces
instantaneously shattering a large area of bedrock), the distribution of iron
fragments neatly centered on the crater, and the upturned and sideways
geological layers found in the rim all indicated that the rim had been "heaved
out" of the crater itself by a great force.</p>
<p>To definitively prove his theory, though, Barringer relied on trenches in
search of buried fragments. If Barringer was correct, there should be fragments
of the meteorite mixed randomly within the other ejecta of the crater. It took
some effort, but eventually Barringer reported clear cases, including a large
iron fragment found underneath a slab of sandstone that must have come from 400
feet below the surface.</p>
<p>Still, Barringer had bigger plans: he intended to find the original meteorite
itself--the core from all of the fragments had broken. With a horse-driven
drill, his employees sunk numerous shafts from 200 to over 1,000 feet deep.
Many seemed to strike iron material, but Barringer believed them to be mere
fragments, not the great cluster of broken iron that must be present somewhere
beneath the crater.</p>
<p>His belief in this huge iron core explains the mining patent he took out on the
behalf of his company, Standard Iron. He planned to find it and sell it. By his
estimate, based on a crude analysis he developed, it would have a market value
in the range of one billion dollars.</p>
<hr />
<p>If you have made it this far, you are either fairly committed to my writing or
very bored. Either way, would you consider supporting my work with a monthly
contribution? Contributors receive EYES ONLY, a special newsletter on special
topics (but mostly computer trivia).</p>
<p><a href="https://ko-fi.com/jbcrawford">https://ko-fi.com/jbcrawford</a></p>
<hr />
<h2>Shafts</h2>
<p>Barringer was enthusiastic about his meteor theory in a charmingly
turn-of-the-century way. He had invented, he said, the field of "meteoritics,"
or the study of meteors. A central concern of meteoritics was the ballistics
of a meteorite: what happened when a meteorite struck the ground.</p>
<p>Today, thanks in large part to World War II munitions research and the nuclear
weapons program, we have considerable theoretical and experimental information
on how solid objects penetrate soil and rock---a topic that Sandia Laboratory,
operating with similar zeal for the new frontiers of science, dubbed
"terradynamics." This work began in the 1930s; Barringer was too early to
benefit. He had to develop a similar theory on his own.</p>
<p>Understanding the geology of the crater involved a huge effort, particularly in
such a remote location. Barringer had to build out a considerable operation,
and he had to do so by remote. He was already a known figure in Arizona, having
launched a particularly successful mine, and he worried that anyone that got
word of his interest in the crater would attempt to jump his claim. Instead of
traveling to Arizona, be brought in relatives and friends as business partners
and even hired Holsinger away from the Division of Forestry to act as land
manager. Barringer and Holsinger chartered a railroad to the site, established
mill sites on the Little Colorado River and Oak Creek, and constructed a water
reservoir. A camp was established at the base of the rim, and an expanding
workforce started on holes, trenches, and a shaft straight down in the center
of the crater.</p>
<p>Barringer first reached the crater in 1904, where he learned that the central
shaft had been abandoned at 200 feet of depth due to the fine silica forming a
quicksand that quickly filled the excavation. He opted for a different
approach, buying a 4" drill and sinking five smaller shafts in the crater
floor.  Most of these shafts ended when they struck meteorite fragments too
hard for the drill. One, managing to dodge any large pieces of iron, found no
new meteoritic material after 550 feet, and reached intact bedrock at 1000
feet. These observations lead Barringer to conclude that the crater had
originally been deeper but had been partially filled back in by the material
that was thrown into the air.</p>
<p>Barringer thought that a steam hoist might allow them to excavate the quicksand
more quickly than it filled, allowing progress on the larger shaft in the
crater's center. Barringer called off the drilling effort and shifted focus
once again to the shaft, spending 1905 excavating a larger clean shaft to the
level of the quicksand. In 1906, the race against the soil was attempted, but
failed. The next year, drilling resumed at an accelerated pace, with sixteen
new boreholes completed in 1907. </p>
<p>Each bore found meteor fragments down to nearly 600 feet, but there was no
evidence of the meteorite itself. Barringer began to develop a new theory:
the meteorite was not directly below the crater.</p>
<p>He conducted a series of simple ballistic experiments: he shot a rifle into
the ground. The impact of the bullet into the desert soil, he reasoned, would
behave similarly to the impact of a meteorite into the same. His easy target
shooting lead to an interesting finding. Regardless of the angle at which he
shot the ground, the resulting hole appeared round. The meteor, he realized,
likely didn't come straight down; it struck at an angle.</p>
<p>Analyzing the rim, he came to believe that the meteorite would be found
somewhere beneath the south rim. More fragments were found to the north, and
the south rim had been lifted higher than in other directions. He had a new
target, but there was a problem... he was out of money. Barringer had started
the project with considerable wealth and several partners, but he had spent
$100,000 searching and Tilghman, his friend and investor in the project,
decided to back out.</p>
<p>Progress at Meteor Crater was slow for the next decade as Barringer marketed
the project to prospective investors. It was not until 1918 that he signed up
a new partner, the United States Smelting and Refining Company. USSRC signed
up to put $75,000 into exploratory drilling, but $60,000 was spent installing
supporting infrastructure, including a ten mile water pipe, before drilling
began from near the top of the south rim.</p>
<p>Progress was slow, and stopped entirely at 282 feet when the drill became
jammed and the $75,000 exhausted. Barringer seems to have pulled off some feat
of salesmanship by convincing USSRC to continue to support the project, and in
1921 a horizontal tunnel was dug from the interior of the crater to the end of
the drill, revealing the two iron balls that had stopped the work. While the
tunnel plan was innovative, it was not particularly effective, as the bore
turned out to be in poor shape and difficult to continue.</p>
<p>Drilling practically started over again, reaching 600 feet at the end of 1921.
Barringer's theory appeared to pan out: at 1,100 feet, well below the crater
floor, meteor fragments started to appear. The drill stuck again at 1,300 feet,
but not before finding dense a span dense with iron fragments. Drilling to that
depth had cost $200,000, and while promising, none of the material recovered
had been of any significant value. USSRC pulled out.</p>
<p>Barringer was back to fundraising, courting various mining companies including
one, United Verde Extension, who rejected the project based on their conclusion
that the crater was the result not of a meteor, but of a steam explosion. You
see, even in 1924, Barringer's theory of the crater's origin came off as
crackpot. The steam explosion theory was still the accepted one, and the crater
had attracted surprisingly little attention from professional scientists.
Barringer's abrasive response to the USGS survey was no doubt a factor; a USGS
geologist by the name of N. H. Darton who had worked for Gilbert during the
original survey took the cause to heart.</p>
<p>Throughout Barringer's mineral exploration of the crater, Darton published
papers arguing for a steam explosion and dismissing the meteorite theory. He
refused to change the name of the site on USGS maps away from Coon Butte until
1916, when he begrudgingly accepted "Crater Mound." Barringer's son, Brandon
Barringer, wrote that a USGS geologist once told him that he thought the meteor
theory to be correct but "it would cost me my job if I was heard saying so."</p>
<p>Drilling resumed again that year, the effort of a new stockholder corporation
that leased the crater from Standard iron and worked on the advice of Barringer.
This corporation, the Meteor Crater Exploration and Mining Company, was backed
in part by Boston business magnate Quincy Adams Shaw. Shaw would bring about
Barringer's vindication, and his downfall.</p>
<p>One of Daniel Barringer's sons, Daniel Barringer Jr., followed his father into
the study of meteoritics. He would make a discovery similar to his fathers: a
letter to a mining journal described the discovery, in 1921, of a large iron
meteorite near a hole in the area of Odessa, Texas.</p>
<p>Barringer Jr. arranged a deal with G. M. Colvocoresses, a smelter executive and
one of the investors in the Exploration Company, to inspect a set of mines in
Texas on his behalf. This provided a convenient excuse to travel through
Odessa, where he examined the area and found signs much like those near Cañon
Diablo. In 1926, he discovered the world's second known meteor impact crater,
the Odessa Crater.</p>
<p>Incidentally, something else had been discovered near Odessa: oil. The Odessa
Crater turned out to be owned by an oil interest, which was not interested in
selling.</p>
<h2>Fragments</h2>
<p>According to a pattern, the Exploration company completed a new shaft near the
USSRC effort, which struck water at 600 feet and about $200,000 in expense.
Shaw, beginning to question the project, engaged the services of an expert:
astronomer Forest Ray Moulton. Moulton was a distinguished scientist, a
professor at the University of Chicago, and even better, an expert in
ballistics, having been in charge of ballistics research at the Aberdeen
Proving Ground during the First World War.</p>
<p>Moulton seemed to accept the meteorite theory from the start. After all,
despite the objections of the USGS, General Electric cofounder and MIT
president Elihu Thomson had visited the crater and reached the same conclusion.
Still, it gives some of the flavor of the debate that Barringer sometimes
referred to his scientific supporters as "converts."</p>
<p>Moulton's report, published in 1929, was a mixed result for Barringer. He
concluded that the crater was indeed the result of an impact by a meteorite,
perhaps 50,000 to 3,000,000 tons. He also concluded that the meteorite would
never be found. The impact energy was more than enough to vaporize it entirely,
leaving only the fragments scattered across the plain.</p>
<p>Three months later, Daniel Barringer died.</p>
<p>His dreams of one billion dollars of iron and nickel buried beneath the crater
died with him. While his family carried on the effort, the Great Depression
ensured that only two further shafts would be drilled, after which mining
exploration ended.</p>
<p>By 1930, the passage of time, the discovery of the Odessa Crater, and Moulton's
report had solidified the meteorite theory to such a degree that the Meteor
Crater became generally accepted as just that. Meteor mining, though, was over
before it had started.</p>
<h2>Billboards</h2>
<p>If not valuable, meteor craters remain unique and fascinating geological
features. The Odessa Crater was leased by Ector County as a tourist site,
and efforts by various parties including the Texas Memorial Museum lead to
the discovery of a six-ton meteorite at the bottom of a second, smaller
crater nearby. Efforts by the Works Progress Administration and later the
Barringer Family to locate the meteorite that formed the larger main crater
failed, leaving a covered shaft still visible today.</p>
<p>At Meteor Crater itself, the mission has changed in a classic Route 66 fashion:
from mining to tourism. In 1953, the Standard Iron Company renamed itself to
the Barringer Crater Company. The Company continues to operate Meteor Crater as
a tourist attraction and scientific resource, hosting NASA training efforts and
geological experiments. Ongoing research at Meteor Crater produced many
conclusions about meteorites and impact events, including extensive research
by Eugene Shoemaker.</p>
<p>It was Shoemaker who directed NASA to the site for training purposes: he had
been considered as an astronaut himself, but excluded for medical reasons.
Instead, he took to Meteor Crater as a moon of his own. It is due in large part
to Shoemaker's comparisons between Meteor Crater and the lunar surface that we
now know the the craters of the Moon to be a result of meteorites as well.</p>
<p>Today, Meteor Crater appears as a roadside stop, not that different from Jack
Rabbit or "The Thing." Billboards precede it by hours, ignored by their
audience of long-haul truckers. It is dusty, and minimally staffed, and has the
feeling of a forgotten place; a spacesuited mannequin and a 4D Experience only
add to the impression of a tourist trap with a brighter past. The museum, with
so much focus on the only tangentially related Apollo program, forms a stark
contrast with brass plaques devoted to a hagiography of the Barringers. They
bring you out to see the crater, but then they say very little about what is
<em>in</em> the crater. You have to read between the lines to find the reason why:
Barringer never found what he was looking for.</p>
<p>Still, there are things that you cannot see anywhere else. The crater itself,
bigger than the Sedan Crater excavated by a nuclear weapon, can only hint at
the unthinkable energy released some 50,000 years ago. A huge fragment of the
meteorite, weighing 3/4 ton and named for Holsinger, is prominently displayed
in the museum. Apartments built for staff remind us of just how remote "between
Flagstaff and Winslow" once was.</p>
<p>Barringer's obsession with meteorites was sincere. Based on his writings, it
predated the discovery of the Meteor Crater, and he devoted as much of his life
to better understanding the crater as he did to mining it. His family and the
Barringer Crater Company continue to make grants in meteoritics research, and
each year they give the Barringer Medal. Its first honoree was Shoemaker, its
most recent Canadian geology professor John Spray, who studies deformation and
friction at extreme speeds and pressures.</p>
<p>Like so much of Route 66, it feels dated, and more than a little tacky. And
like Route 66 as well, it is a fascinating chapter in the story of the American
West. An eastern businessman, as eccentric as he was passionate, left for
Arizona in pursuit of a wild idea. He was only half right.</p>
<p>[1] Curiously, the most recent billboards for Meteor Crater mainly feature an
anthropomorphic rabbit, apparently a character designed for a "4D experience"
at the museum. The connection between the rabbit and the crater is left
unexplained; an odd mascot that you might be tempted to ascribe to furries
except that if the furries were in charge it would have looked better. I
cautiously speculate that it may have been intended as a reference to the Jack
Rabbit.</p>
<p>[2] A meteor is seen in the sky, a meteoroid is the actual object that burned
up to be seen as a meteor, and a meteorite hits the ground. It is said that you
can remember this by recalling that meteorites are "right" in that they
successfully made it all the way. As with so many memory devices, I think you
could also argue this one the other way, so it's not really that helpful.</p>
<p>[3] Foote's paper to the AAAS about the discovery makes it clear that this kind
of wild exaggeration was not unusual when coming from prospectors. "There were
some remarkable mineralogical and geological features which together with the
character of the iron itself, would allow of a good deal of self-deception in a
man who wanted to sell a mine."</p> ]]></description>
	</item>

 	<item>
		<title>2024-08-12 a pedantic review of the las vegas loop</title>
		<pubDate>Mon, 12 Aug 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-08-12-a-pedantic-review-of-the-las-vegas-loop.html</link>
		<guid>https://computer.rip/2024-08-12-a-pedantic-review-of-the-las-vegas-loop.html</guid>
		<description><![CDATA[ <p>Did you hear that Elon Musk dug a tunnel under the Las Vegas Convention
Center?</p>
<p>I think it is pretty universally known by now that the "Las Vegas Loop" is
impractical, poorly thought out, and generally an embarrassment to society and
industry. I will spare an accounting of the history and future of the system,
but I will give a bit of context for the unfamiliar reader. The Las Vegas Loop
is a (supposed) mass-transit system built and operated by The Boring Company
for the Las Vegas Convention and Visitors Authority at the Las Vegas Convention
Center. Besides four (ish) stations in the Convention Center, it has been
expanded to serve Resorts World as well. It will, according to plan, be
expanded to as many as 93 stops throughout the Las Vegas metropolitan area,
despite the mayor of Las Vegas calling it "impractical" and "unsafe and
inaccessible." This odd contradiction comes about because The Boring Company is
footing a very large portion of the construction cost, while much of the rest
is coming from casinos and resorts, making it extremely inexpensive for
regional government agencies.</p>
<p>In practice, the Loop consists of a set of mostly double-bore tunnels of small
diameter, which are traversed by Tesla Model 3 and Tesla Model X vehicles
manually driven by humans at up to 40 mph. They have more recently switched to
Model Y, but the operations manual I have predates that change, so let's stick
with the older models for consistency. Each vehicle seats up to four.  The
system is nominally a PRT, or personal rapid transit, as the drivers take you
to the specific station you request. The tunnel to Resorts World is single
bore, and can admit vehicles in only one direction. A simple signaling scheme
serves to prevent vehicles meeting head-on in single tunnels. While Loop and
Boring Company marketing focuses heavily on the single underground station, all
other stations are above ground. In the current state, I think it is actually
somewhat generous to call the Loop an underground system, as most maneuvers and
operations occur at surface level. It is perhaps best thought of as a taxi
system that makes use of underground connectors to bypass traffic.  Future
expansion plans involve significantly more tunnel length and more underground
stations, which will probably cause the system overall to feel more like a
below-surface transit system and less like an odd fleet of hotel courtesy cars.</p>
<p>I am not going to provide a general review of the system, because many others
have, and you can probably already guess what I think of it. Instead, I want to
focus on some aspects that have not been as heavily discussed in other
reporting: detailed operational practices, and safety and communications
technology.</p>
<p>We are fortunate that, as part of its fire safety permitting, the Loop has been
required to file its <a href="https://www.docdroid.net/Xh7jsov/cwpm-operations-manual-pdf">operations manual</a> with Clark County. Unfortunately, the
newest revision I can find online is 2021's Revision 7, which predates the
Resorts World station and may be out of date in other ways as well. Still, it
appears to be substantially correct, and much of what I will discuss is based on
Revision 7 of the manual alongside several trips I have taken in the system.</p>
<p>Interestingly, the operations manual refers to the system only as the
"Campus-Wide People Mover" or CWPM. This term seems to date to the original
solicitations by LVCVA, but is not used in marketing.</p>
<h2>Rules and Discipline</h2>
<p>Like most detailed policies, the operations manual is an interesting read for
the pedantic. Some parts are odd in a classically Elon Musk way, like the
manual's use of "What's Elon like?" as the first example of a question that
passengers might ask a driver. Other parts are weird in a more conventional
way, like a paragraph that says first that the tunnels are connected to the
operations control center (OCC) by single-mode fiber, second by two redundant
fibers, and third by two single-mode fibers taking separate paths. I am pretty
sure the correct interpretation of this paragraph is that there are two fiber
routes and they just said that three times to pad for length, but it's hard to
be totally sure. Why it's so important to clarify that it's single-mode is
anyone's guess, perhaps because during the review stage a regulator asked. You
find this kind of thing a lot in these sorts of policies, which are usually
edited extremely carefully for regulatory compliance and not at all for plain
reading.</p>
<p>One interesting aspect of the Loop, that has been rather heavily reported on,
is that the whole thing feels remarkably cheap. Perhaps that's not surprising,
as The Boring Company's central claim is to be able to construct underground
transit on a tight budget. They have indeed delivered on this promise in the
construction of the Loop, but it's hard not to feel like they did so more by
pinching pennies and eating development costs than by innovation. Nothing
speaks to this more than the photo of the OCC the manual provides, which
depicts two cheap office chairs at a long desk in a room with worn linoleum
floors and distinctly portable-building vibes. It is, by far, the most
underwhelming mass-transit control room I have ever seen. I strongly suspect
that the OCC is a reuse of the old on-site construction office. To be fair,
they will surely have to build something more sophisticated for future
expansion, as the current space only accommodates two operators. We will assume
it is temporary.</p>
<p>Early in the manual, hiring and training requirements are discussed. They are
fairly standard for transportation drivers, with hiring requirements mostly
amounting to a clean driving record and a clean drug test. Drivers must
undergo 10 hours of in-vehicle training, including a four hour half-shift
endurance exercise with mock passengers. There is classroom (or more likely
computer-delivered) training as well, but the manual doesn't enumerate it.
Drivers are required to wear a provided uniform with plain black shoes and
no jewelry or accessories, and are prohibited from initiating conversations
with passengers.</p>
<p>Which leads us somewhat naturally to the next section of the manual, on rules
and discipline. There is usually a wide gap between rules written into policy
and rules followed in practice, with "real" rules being determined by
enforcement behavior---what practically can be enforced, and what supervisors
choose to enforce. For example, the manual prohibits the drivers listening to
the radio in the cars, something that zero percent of the drivers I have had
complied with. There is no discipline outlined in the manual for this
infraction, so, is it even really a rule?</p>
<p>The speed limits for the system are 40 in straight tunnels, 30 in turns, 15 on
ramps, and 10 in stations. Most of this was unsurprising except for the "15 on
ramps" part, as my drivers have consistently taken full advantage of the
electric vehicle's torque, hitting 30 before the end of the descent ramp. This
would appear to be a violation of policy. But, it's interesting to note,
discipline (a "demerit") is only listed for a speed excursion of at least 5
seconds. Because of the short length of the ramps, it is likely not actually
possible to incur a demerit for violating the 15 mph ramp speed limit. I wonder
if the authors of this policy realized that.</p>
<p>Drivers are strictly prohibited from using any assistive driving features. This
is sort of a moot point in practice, as maintenance staff are required to
disable the assistive driving features of the vehicles before they are put into
service in the Loop. This isn't at all surprising considering the highly
regulated nature of transit operations, but it <em>is</em> pretty funny considering
that The Boring Company originally promised automation, and that closely
related Tesla has made self-driving a key part of their marketing.</p>
<h2>Emergency Procedures</h2>
<p>So, with the rules stuff out of the way, let's talk about emergency procedures.
One of the problems with underground transit is that tunnels can be very
dangerous: they are enclosed spaces where exits may be far away, and in a fire
they can quickly fill with toxic and opaque smoke. Many historic incidents have
illustrated the inherent danger of tunnels, and so modern tunnel designs
incorporate extensive safety measures which typically include smoke extraction
systems, evacuation guidance, emergency exits or refuges, and increasingly,
fire suppression systems.</p>
<p>The Loop has been widely criticized for incorporating very few of these
features. It does have a basic smoke evacuation system, but there is no
evacuation guidance in the tunnels (no signage to indicate the nearest exit in
low visibility conditions), and no evacuation points or refuges except for,
oddly enough, marked refuges at the end of some tunnels that seem to be largely
an ADA compliance measure because the ramps are too steep to be considered
ADA-compliant egress (they are remarkably steep!). </p>
<p>To be fair, I think some of these criticisms are somewhat overblown. It does
appear to be possible to open the car doors just about fully within the
tunnels, although I think they are likely to strike the walls. And the thing
is, the tunnels are very short! like really short! some tunnels seem to be
shorter than the typical interval between refuge points in modern highway
tunnels, and those that are longer probably aren't longer by much. The
expansion system may incorporate more extensive safety measures due to longer
tunnel runs.</p>
<p>Evacuation procedure basically consist of driving the car out of the tunnel,
via the next station. If evacuation must be made in the opposite direction, the
manual says the driver must await instructions from the OCC, as they are not
generally permitted to drive in reverse. This is probably an accommodation for
the poor rear visibility of the vehicle; drivers are normally prohibited by
policy from driving in reverse. The OCC would likely have to coordinate
vehicles reversing out by track warrant (tunnel warrant?) to avoid collisions.
This is a common pain point with evacuation of train tunnels, for example,
where there may not be a cab on the rear and even if there is there may not be
enough time for the operator to switch ends.</p>
<p>In the event it becomes necessary to abandon the vehicle, the driver is to have
passengers get out, and then lead them to the closest exit. The driver will
presumably have to know the nearest exit by heart, since there isn't clear
evacuation guidance in the tunnel. The manual addresses difficulty opening the
vehicle doors, a common concern with Teslas that have electrically operated
door releases. My understanding is that both the Model 3 and the Model X do
have a mechanical release for all passenger doors, although it's pretty hidden
on the rear doors. Oddly enough, the manual doesn't seem to know that, as the
Loop operations manual strongly implies that there is no manual release for the
rear doors of a Model 3. I would think that authorities would have immediately
noticed that implication, so it makes me wonder if the Model 3 rear door
release (a wire loop hidden under a panel) was simply ruled out as infeasible
to use in an emergency scenario. Of course, that's odd, because the operations
manual... just doesn't tell you what to do with rear passengers in a Model 3.
You are basically SOL, as far as the operations manual is concerned. Only
passengers of a Model X are allowed to escape in a scenario where the vehicle
loses power. To my eyes, that is by far the biggest unresolved problem with the
emergency operations plans. Perhaps a later revision of the manual addresses
it, because it seems like more of a documentation error than a real problem.</p>
<p>The manual does not address evacuation procedures. There may be a document with
that information that did not make it to the internet. At at least two points
in the tunnels I spotted golf carts (the type with four rows of seats) stashed
in corners, and I suspect they would be used if there was a need to retrieve
passengers from a disabled car. My husband pointed out that due to their larger
seating capacity and faster boarding/deboarding, the Loop would likely achieve
a higher capacity if they just shifted operations entirely to the Club Cars.</p>
<h2>Customer Service</h2>
<p>There is a section of the manual on customer service and interactions with
customers I actually don't think it's that unusual for this kind of policy, so
I don't want to mock it too much, but I will tell you this: drivers are told to
keep conversations as short as possible and give as little information about
themselves as possible. They are not to tell passengers how long they have
worked for The Boring Company, how old they are, their last names, or
information about TBC employee counts or pay rates, even if asked. There is a
surprisingly long (in comparison to other items) script for answering questions
about the flamethrower. Drivers are told to tell passengers that the Loop
operates at "about 35 average and 50 maximum," an interesting answer since 40
is the maximum speed in any segment and exceeding it for more than 5 seconds
would lead to a demerit (exceeding 50 for more than 5 seconds would lead to
suspension).</p>
<p>The correct answer to "What's Elon like" is "He's awesome [inspiring /
motivating / etc.]", and drivers are not sure how often he is around. There is
a whole section about how to answer questions about Elon Musk, including how to
respond to questions about his tweets. I enjoy that Elon Musk is a person such
that every employee of one of his companies needs some basic press training on
how to deal with his social media habits. Drivers are, if we take this script
more literally than its authors probably thought out, to say that they do not
have personal experience of Elon Musk smoking weed.</p>
<p>The operations manual spends more page length on answering questions about
Elon Musk and The Boring Company than it does on fires.</p>
<h2>Communications Technology</h2>
<p>So, what communications technology does the Loop employ? Communications in
tunnels is an interesting problem, especially in a life-safety critical
environment. Unfortunately, The Boring Company has opted for a pretty
boring approach that also seems... questionably safe?</p>
<p>From the manual and various public press, we can infer that the tunnel has some
type of LTE. "Leaky" cables are an interesting RF technique often used for
tunnels, but the Loop tunnels are short enough that directional antennas at the
end might be sufficient. Still, it seems very plausible that there is leaky
feeder embedded in the overhead light trough.</p>
<p>Loop operators are equipped with an iPad and a bluetooth headset. The iPad runs
a very basic looking app with a "call" button the operator can use to reach the
OCC. It's probably using a very straightforward third-party library to either
make a VoIP (e.g. WebRTC) call or to use the iOS dialer to do the same (does
iPad OS have a dialer? I don't know how these newfangled Apples work).</p>
<p>An interesting note - you might notice that Tesla vehicles characteristically
include a big touchscreen in the middle. The Las Vegas Loop vehicles do run
modified software, but based on the 2021 operations manual and the scant more
recent information I can find, it has been modified only to allow the OCC to
remotely access vehicle status information and cameras. There don't appear to
be any extra driver-facing features, leaving a need for the iPad.</p>
<p>But what about a failure? Well, there is an auxiliary system, one that is very
similar to that used in other types of underground transportation systems.  At
regular intervals in each tunnel, as well as at stations and other critical
points, there is a "blue light station." The blue light station consists of a
blue fire pull device that presumably reports an emergency to the OCC (but
probably does not automatically trigger an evacuation, as that can be very
bad), as well as a phone. The phone is configured as a "hotline" in the
modern-traditional sense, meaning it automatically dials the OCC when taken off
hook. This appears to be the only emergency communications system, although it
seems unlikely to me that they don't have a public safety repeater system in
the tunnels (e.g. for 900MHz P25), as fire authorities often require them.</p>
<p>Mass notification can serve as an important secondary communications system,
either when there are problems with primary systems or in emergencies that
require action as quickly as possible. Here, the Loop takes an interesting
approach. The operations manual includes a screenshot of the OCC operator
interface, and the amount of screen space devoted to controlling the tunnel's
RGB gamer lights seems odd (given that they use white lighting except for
during special events, it seems like this is more of a marketing concern than
an operations task). That makes a lot more sense when you discover that the
mass notification plan is all based on the lighting: in an evacuation scenario,
the lights will flash red and white in the intended direction of travel, and
remain solid red in the direction of danger. I honestly think this is a clever
use of the lighting equipment and I like how it indicates the direction of
evacuation, but I do worry a little about whether or not the color of the
lighting is set over a life-safety grade network, or just via the LTE or
something.</p>
<h2>Accessibility</h2>
<p>A quick side-note about accessibility. Transit enthusiasts probably know that
ADA requirements for public transportation are quite strict, and there's not
much of a way around offering wheelchair service. The Boring Company seems to
mostly address this question by saying that passengers are expected to transfer
from wheelchair into sedan, which... sucks and isn't going to pass ADA review.
So they have a secret: a GEM cart. GEM is a brand of Polaris, the parent
company of RZR and Indian Motorcycle and assorted other small vehicle brands,
aimed at institutional customers. GEM carts are golf-cart or neighborhood
electric vehicle (NEV) class electric vehicles with 72-volt systems, and they
make a wheelchair accessible version. Apparently the Las Vegas Loop now uses
one to ferry around wheelchair customers.</p>
<p>The punch line here is... remember what my husband said about the golf cart we
saw? GEM makes carts that seat five in addition to the driver, with a higher
seating position and open sides or optionally large doors for faster
board/deboard. Even with the 25mph stock speed limiter for NEV/LSV regulatory
compliance (and believe me, with some adjustments to the motor controller they
can go faster), I suspect that switching the Loop entirely to GEMs would
increase its total capacity. And the GEMs honestly suck, in the world of light
electric vehicles. They just kind of pulled off a regulatory capture move and
got the NEV rules written to pretty much require something that sucks as much
as they do for street legality.</p>
<h2>Subjective Experience</h2>
<p>So as I said, this is not a review, just trying to focus on some things of
interest to  transit, communications, and policy dweebs. Which I assume pretty
much describes my core readers. But I do want to point out a couple of oddities
that add to the "wow, this is cheap" sensation:</p>
<p>The ride is surprisingly rough, even in a Model Y with highway-grade
suspension. I am concerned that they may not be able to do much better when
paving in the confined tunnels, given that I don't think standard paving
equipment would fit in the loading gauge. The ride experience was not "oooo
electric car luxury," it was more on par with the Orlando Airport APM100s
with sketchy steering gear.</p>
<p>For the segment that requires tickets (to Resorts World), the ticketing system
is based on a QR code. The customer-side implementation is fine enough, but the
ticket checking is laughable. It's an iPad where you have to show a QR code to
the front-facing camera, meaning you have to present the QR code with your
phone facing away from you, looking at the image on the iPad for alignment. It
is very awkward and there is no reason for it besides cheapness.  Plus there's
not really any way for the attendant to see if the ticket is valid without
standing awkwardly close to you to look at the same iPad screen you are, and
indeed, I accidentally scored a free ride by merit of the attendant's inability
to see the actual result of the ticket check.</p>
<p>The stations are not especially well thought out. People walking in and out of
the stations have to cross the path of the Loop vehicles in some places. The
attendants are supposed to direct people and, for trips to Resorts World,
collect fare, but the design of some stations lacks a chokepoint at which to do
so. The attendants have to kind of chase people down after they've already
walked straight to a vehicle.</p>
<p>The tunnel to Resorts World is one-way. Its portal is connected to the West
LVCC station by a tunnel, but the station and the Resorts World portal are
actually in the same parking lot. They seem to have adopted a practice of cars
one way going through the tunnel, and cars the other way just driving... across
the parking lot. This is very funny to experience and contributes a lot to the
feeling that the Loop is only marginally an underground system. I doubt the
original designers intended for this outcome, it seems like the money spent on
the connecting tunnel was completely wasted, but I'm assuming that eliminating
one segment of single-track tunnel helped with throughput. Their approach to
managing traffic at the Resorts World portal also involves a sort of
approach-pattern-esque architecture where every car has to drive in a circle
around the portal before entering, which is funny.</p>
<p>This stuff matters in my mind because it gets to the question of what the
Loop...  is for? The capacity of the Loop is very low. The expansion plan calls
for a <em>lot</em> of tunnels, doubled up for capacity in places, but targets only 90k
passengers per day. That would put it at around 8x the current daily
ridership of the monorail, but with a <em>vastly</em> larger network of stations.
Presumably they will expand fare collection, and I would have to think that
tickets will actually become fairly expensive. so it's probably not intended
to be a high-capacity, low-cost option.</p>
<p>So what else could it be? Well, some press and discussion around the Loop
figures it as more of a luxury option: something that casinos can comp for high
rollers, that will spare people dealing with the general disaster of getting
around the strip. But it also doesn't feel like that. The outdoor stations,
need to quickly board and deboard a sedan, and general chaos level of the
stations (i.e. attendant chasing you down for ticket) make it feel more
"courtesy car" than "black car."</p>
<p>I don't know, they could totally dress it up a bit and make it feel fancier.
Some paint here and there, train the attendants better, do more to direct
traffic. They could! But right now I think the best way to describe the Las
Vegas Loop is... "cheap and amateurish." Surprisingly fitting with the Las
Vegas vibe, in a way.</p> ]]></description>
	</item>

 	<item>
		<title>2024-07-31 just disconnect the internet</title>
		<pubDate>Wed, 31 Jul 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-07-31-just-disconnect-the-internet.html</link>
		<guid>https://computer.rip/2024-07-31-just-disconnect-the-internet.html</guid>
		<description><![CDATA[ <p>So, let's say that a security vendor, we'll call them ClownStrike, accidentally
takes down most of their Windows install base with a poorly tested content
update. Rough day at the office, huh? There are lots of things you could say
about this, lots of reasons it happens this way, lots of people to blame and
not to blame, etc., etc., but nearly every time a major security incident like
this hits the news, you see a lot of people repeating an old refrain:</p>
<blockquote>
<p>these systems shouldn't be connected to the internet.</p>
</blockquote>
<p>Every time, I get a little twitch.</p>
<p>The idea that computer systems just "shouldn't be connected to the internet,"
for security or reliability purposes, is a really common one. It's got a lot of
appeal to it! But there's not really that many environments where it's done. In
this unusually applied and present-era article, I want to talk a little about
the real considerations around "just not connecting it to the internet," and
why I wish people wouldn't bring it up if they aren't ready for some serious
considerations.</p>
<h2>We Live in a Society</h2>
<p>In the abstract, computers can perform valuable work by doing, well,
computation. In practice, the computation is rarely that important. In
industry, there is a lot more "information technology" than there is
"computation." Information technology inherently needs to ingest and produce
information, and while that was once facilitated by a department of Operators
loading tapes, we have found the whole Operator thing to be costly and slow
compared to real-time communications.</p>
<p>In other words, the modern business computer is almost primarily a
communications device.</p>
<p>There are not that many practical line-of-business computer systems that
produce value without interconnection with other line-of-business computer
systems. These interconnections often cross organizational and geographical
boundaries.</p>
<p>I am thinking, for example, of the case of airline reservation and scheduling
systems disabled by the CrowdStrike, er, sorry, whatever I called them
incident. These are fundamentally communications systems, and have their
origins as replacements for the telephone and telegraph. It is not possible
to simply not internetwork them, because networking is inherent to their
function.</p>
<h2>Networking is important to maintenance and operations</h2>
<p>But let's consider systems that don't actually require real-time communications
to perform their business purpose. Network connectivity still tends to be really
valuable for these.</p>
<p>For one, consider maintenance: how does a system obtain software updates if you
have no internet connection? How is that system monitored?</p>
<p>And even if you think you can avoid those requirements by declaring a system
"complete" and without the need for any updates or real-time monitoring or
intervention, business requirements have the frustrating habit of changing over
time, and network connectivity reduces the cost of handling those changes
tremendously.</p>
<h2>What does it mean for a system to not be connected to the internet?</h2>
<p>First, we need to consider the fact that there are as many forms of "not
connected to the internet" as there are ways of being connected to the
internet. For this reason alone, proposing that a system shouldn't be
internet-connected is usually too nonspecific to really discuss. Let's consider
a menu of possibilities:</p>
<p>List 1:</p>
<ol>
<li>A single device with no network connection at all.</li>
<li>A system of devices that is "air-gapped" in the strictest sense, with no
connection to any network other than its private local-area one, where data
never crosses the security boundary.</li>
<li>That same system, but someone carries DVD-Rs across the security boundary to
introduce new data to the private network.</li>
<li>That same system, but a cross-domain solution or "data diode" allows
movement of data from a wider (or lower-security) network into the private (or
higher-security) network.</li>
<li>That same system, where the cross-domain solution does <em>not</em> have a costly
and difficult to obtain NSA certification.</li>
</ol>
<p>List 2:</p>
<ol>
<li>A system of devices which interconnect over a private wide-area network
using fully independent physical infrastructure with physical precautions
against tampering.</li>
<li>That same system, but the independent physical infrastructure is run through
commodity shared ducts.</li>
<li>That same system, but the infrastructure is leased dark fiber.</li>
<li>That same system, but the infrastructure is wavelengths on lit fiber.</li>
<li>That same system, but the infrastructure is "virtual private ethernet"
implemented by the provider using, let's say, MPLS.</li>
<li>That same system, but the infrastructure is "virtual private ethernet"
implemented by the provider using a tunneling solution with encryption and
authentication.</li>
</ol>
<p>List 3:</p>
<ol>
<li>A system of devices which interconnect over a common-carrier network (such
as, we might even dare say, the internet), where private network traffic is
tunneled through encryption and authentication performed by hardware devices.</li>
<li>That same system, but the hardware devices do not have a costly and difficult
to obtain NSA certification.</li>
<li>That same system, but the tunneling is performed by a software solution
that is well-designed such that it configures the operating system network
stack, at a low level, to prevent any traffic bypassing the tunnel, and this
has been validated by someone much smarter than me.</li>
<li>That same system, but not so well designed and validated by someone like me.</li>
<li>That same system, but the "software solution" is like Wireguard and an
iptables script that has been "thoroughly tested" by someone on Reddit.</li>
</ol>
<p>List 4:</p>
<ol>
<li>A system of devices which interconnect on a private network that has
interconnection to the internet that is strictly limited by policy-based
routing or other reliable methods, such that only very narrowly defined
traffic flows are possible.</li>
<li>That same system, but the permissible network flows are documented in
some old Jira tickets and some of them were, you know, just thrown in to
make it work.</li>
<li>That same system, but it's basically protected by a firewall that's
pretty liberal about outbound flows (maybe with IPS or something), and
pretty restrictive about inbound flows.</li>
</ol>
<p>List 5:</p>
<ol>
<li>An AWS private VPC without any routing elsewhere.</li>
<li>An AWS private VPC with PrivateLinks and other AWS networking baubles
that allow it communicate with other private VPCs.</li>
<li>That same system, but some of the interconnected VPCs can route traffic
to/from the internet.</li>
<li>An AWS private VPC with NAT GW and IGW but the security groups are set up
pretty tight in both directions.</li>
</ol>
<p>These are all things that I have seen described as non-internet-connected.
Take a moment to work through each list and mark the point at which you think
that is no longer a reasonable claim. It's okay, I'll wait.</p>
<p>I'm not going to provide threat modeling for all of these scenarios because it
would go on for pages, but you can probably see that pretty much every option
is at least slightly different in terms of attack surface and risk.</p>
<p>This might seem like an annoying or pedantic argument, but this is actually the
biggest reason I get irritated when people say that something should never be
connected to the internet. What do they <em>mean</em> by that?  When someone says that
an airline reservation system shouldn't be internet-connected, they clearly
don't actually mean the strictest form of that contention (no network
connection at all) unless their name is Adama and they liked when airline
reservation centers had big turntables of paper cards they spun around to check
off your seat. They must mean one of the midpoints presented above, which are
pretty much all coherent positions, but all positions with different
practical considerations.</p>
<p>This ambiguity makes it hard to actually, seriously consider the merits of
dropping internet connectivity.</p>
<h2>Non-internet connected systems are so very, very annoying</h2>
<p>In my day job, I work with a wide variety of clients with a wide variety of
cultures, IT architectures, and so on. Some of them are in highly regulated
industries or defense or whatever, and so they actually conduct software
operations in networks with either no internet connectivity or tightly
restricted internet connectivity.</p>
<p>When I discover this to be the case, I mentally multiply all of the
schedule/cost estimates by a factor of, I would say, 3 to 10, depending on
where they fall on the above lists (usually 3x to 5x for list 5 and 10x to a
bajillion times forever for list 1, just rule of thumb).</p>
<p>Here's the thing: virtually the entire software landscape has been designed
with the assumption of internet connectivity. Your operating system wants to
obtain its updates from online servers. If you are paying for expensive
licenses for your operating system, the vendor probably offers additional
expensive licenses for infrastructure to perform updates within your private
network. If you are getting your operating system for free-as-in-beer, there's
a good bet you can figure it out yourself, but if you're using anything too new
and cutting-edge it might be a massive hassle.</p>
<p>But that just, you know, scratches the surface. You probably develop and deploy
software using a half dozen different package managers with varying degrees of
accommodation for operating against private, internal repositories. Some of them
make this easy, some of them don't, but the worst part is that you will have to
figure it out about fifty times because of the combinatorial complexity of
multiple package managers, multiple ways of invoking them, and multiple
environments in which they are invoked.</p>
<p>If you are operating a private network, your internal services probably don't
have TLS certificates signed by a popular CA that is in root programs. You will
spend many valuable hours of your life trying to remember the default password
for the JRE's special private trust store and discovering all of the other
things that have special private trust stores, even though your operating
system provides a perfectly reasonable trust store that is relatively easy to
manage, because of Reasons. You will discover that in some tech stacks this is
consistent but in others it depends on what libraries you use.</p>
<p>A bunch of the software you use will want to perform cloud licensing and get
irritated when it cannot phone home for entitlements. You will have to go back
and forth with your vendors to figure out a workaround somewhere between "add
these ninety seven /16s to your firewall exceptions" and "wait six months while
we figure out the internal process to issue you a bespoke licensing scheme."</p>
<p>All of your stuff that requires updates or content updates will have some
different process you have to follow to obtain those updates and then provide
them internally. Here's a not at all made up example, but a real one I have
personally lived through: you will find that a particular (and particularly
hated) enterprise software vendor provides content updates for offline use only
through a customer support portal that is held over from three acquisitions
ago, and that it is only possible to get an account in that customer support
portal by getting an entitlement manually added in a different customer support
portal held over from two acquisitions ago. It will take over three months of
support tickets and escalations through your named account executive to get
accounts opened in successively older customer support portals until you can
finally get into the right one, which incidentally has an invalid TLS cert you
are reassured is not something to worry about. Once you download your offline
content update, you will find that the documented process to apply it no longer
works, and it will take a long email chain with one of the engineers to get the
right instructions. You paid a five-figure sum for a 1-year license to this
software and it has now nearly elapsed while you figured out how to use it. You
will of course get an extension on that license pro bono, because this is
enterprise software sales and what is a quarter worth of my salary between
friends, but they won't manage to issue the extension license until after
your original one has already expired, causing a painful interruption in CI
pipelines and a violent revolution by the developers.</p>
<p>I am sorry, you are not my therapist, I will try to stop remembering that dark
time in my career. Don't worry, the software in question seems to have fallen
out of favor and cannot hurt you.</p>
<p>So, like, that's an over-the-top example (but seriously, a real one!), but you
get the point. It's not really that any individual part of operating in an
offline environment is hard---I mean some of them are, but most of them aren't.
It's a death by a thousand cuts. Every single thing you ever do is harder when
you do not have internet connectivity, and you will pay for it in money and
time.</p>
<p>The largest problem by far is that almost everyone who develops software
assumes that their product will not need to operate in an offline environment,
and if they find out that it does they will fix that with duct tape and shell
scripts because it only matters for a small portion of their customers. You,
the person with the offline environment, will become the proud owner of their
technical debt.</p>
<p>None of this really <em>needs</em> to be that way, it's just how it is! There are not
really that many offline environments, and they tend to be found in big
institutions that have adapted to the fact that they make everything cost more
and take longer, and are surprisingly tolerant of vendors who perform a three
stooges routine every time you say "air-gap," because that's what pretty much
every vendor does. Except for like Red Hat, I genuinely think Red Hat is pretty
good about this, but you betcha what you save in time you are paying in cash.</p>
<h2>Not many people do this</h2>
<p>That's kind of the point, right? The problem with non-internet-connected
environments is that they are rare. The stronger versions, things from List 1
and List 2, are mostly only seen in defense and intelligence, although I have
also seen some banks with pretty impressive practices. You will note that
defense and intelligence, and even banks, are also famously industries where
everything costs way too much and takes way too long. These correlations are
probably not coincidences.</p>
<p>Even the weaker forms tend to be limited to highly-regulated industries
(finance and healthcare are the big ones), although you see the occasional
random software company that just takes security really seriously and keeps
things locked down. Occasionally.</p>
<h2>Okay, let's stop just complaining</h2>
<p>Here's the thing: I genuinely do not think that "fewer systems should be
connected to the internet" is a bad idea. I really wish that things were
different, and that every part of the software industry was more prepared and
more comfortable operating in environments with no or limited internet
connectivity. But that is not the world that we currently live in! So let's
get optimistic, what should we be doing right now?</p>
<ol>
<li>
<p>Apply restrictive network policy on as much of your stuff as possible.
Cloud providers generally make this easier than it has ever been before, it's
not all that easy but it's also not all that hard to operate a practical
non-internet-routed environment in AWS. If you stay within the lanes of all
the AWS managed services, it's mostly pain-free. You will pay for this, but,
you know, AWS always gets their check anyway.</p>
</li>
<li>
<p>Build software with offline environments in mind. Any time that you need
to phone home to get something, provide a way to disable it (if practical)
or a way to override the endpoint that will be used. If the latter, keep in
mind that you will also need to come up with a way for a customer to feasibly
host their own endpoint. If you keep to simple static files, that's really
easy, just nginx and a directory or whatever. If it's an API or something,
well, you're probably going to have to ship your internal implementation.
Brace yourself for the maintenance overhead.</p>
</li>
<li>
<p>Try to think about the little assumptions that go into connecting to other
services that become more complex in an offline environment. Please, for the
love of God, do not assume you can reach LetsEncrypt. But that's not the only
TLS problem, offline environments virtually always imply internal certificate
authorities. <em>Use the system trust store.</em> Please. I am begging you.</p>
</li>
<li>
<p>Avoid fetching any kind of requirements or dependencies at deploy time. One
of the advantages Docker supposedly brought us was making all of the
requirements of a given package self-contained, but then I still run into
Docker containers that can't start if they can't reach the npm repos or
something. And now I have yet another place to fix configuration and trust
store and etc., in your stupid Docker container. It has made things more
difficult instead of less.</p>
</li>
</ol>
<p>Have I mentioned that Docker, paradoxically, actually makes offline
environments <em>more</em> difficult to manage? Yeah, because virtually every
third-party Docker container has <em>at least</em> a TLS trust store you'll have to
modify. Docker is, itself, a profound example of how the modern software
industry simply assumes that everything is running On The Internet.</p>
<h2>Anyway</h2>
<p>I wrote this out in a bit of a huff because I have seen "why were they
connected to the internet at all?" like four times in response to the
CrowdStrike incident. I know, I am committing the cardinal sin of taking things
that people on the internet say seriously, but I feel obligated to point out:
internet connectivity is pretty much completely orthogonal to what happened.
CrowdStrike content updates are the kind of thing that, in a perfect world,
you would promptly make available in your offline environment. In practice, an
internal CrowdStrike update mirror would probably lag days, weeks, months, or
years behind, because that's what usually ends up happening in "hard" offline
environments, but that's a case of two wrongs making a right.</p>
<p>Which they do, more often than you would think, in the world of information
technology.</p>
<p>Don't worry, I'll be back next time with something more carefully written and
less relevant to the world we live in. I just got in a mood, you know? I just
spent like half the day copying Docker images into an offline environment and
then fixing them all. I have to find something to occupy the time while a
certain endpoint security agent pegs the CPU and makes every "docker save" take
ten minutes.</p> ]]></description>
	</item>

 	<item>
		<title>2024-07-20 minuteman missile communications</title>
		<pubDate>Sat, 20 Jul 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-07-20-minuteman-missile-communications.html</link>
		<guid>https://computer.rip/2024-07-20-minuteman-missile-communications.html</guid>
		<description><![CDATA[ <p>A programming note: I am looking at making some changes to how I host things
like Computers Are Bad and <a href="https://wikimap.wiki">wikimap</a> that are going to
involve a lot more recurring expense. For that and other reasons, I want to see
if y'all would be willing to throw some money my way. If everyone reading this
gave $3 a month, we could probably buy Jimbo Wales a nice lunch or something.</p>
<p>I do not intend to paywall anything that I post here. Instead, I'm going to
take some of the things that tend to be very long Mastodon threads (how this
article originally started!) and send them out to supporters, probably about
once a month. They'll be things that I wouldn't post here, usually because
they're too short or don't quite have a hook to make a while article
interesting.</p>
<p>So <a href="https://ko-fi.com/jbcrawford">consider supporting me on ko-fi</a>---or don't,
it's your decision, and I respect that. I'll probably work a plug into each
article but I promise not to be annoying about it. I'm also going to be making
some new YouTube videos and I'll probably make those available to supporters
first, so that's something else to look forward to.</p>
<p>Speaking of annoying, this is kind of a long and dry one. I started looking
into something called HICS after visiting a historic site, posted about it a
bit on Mastodon, realized that there just wasn't a lot of good historic
information about it in general. And I felt like if I was going to talk about
communications in Minuteman missile fields, I also had to cover how they would
get their war orders.</p>
<p>Look on the  bright side: it's got pictures!</p>
<p><img alt="Blast door of LCC" src="https://computer.rip/f/minuteman/door.jpg" /></p>
<h2>Minuteman Missiles</h2>
<p>Since the early days of the Cold War, the United States has maintained a
nuclear triad: independent capabilities to deliver nuclear weapons from land,
sea, and air. The sea and air components are straightforward, consisting of
submarines carrying submarine-launched ballistic missiles (SLBMs) and
long-range strategic bombers. The land leg is more often forgotten: the
intercontinental ballistic missiles, or ICBMs.</p>
<p>The US ICBM arsenal currently consists of 400 Minuteman III missiles emplaced
throughout the midwest: the Air Force's 90th Missile Wing, 150 missiles,
Wyoming Nebraska, and Colorado; the 91st Missile Wing, 150 missiles, in North
Dakota; and the 341st Missile Wing, 100 missiles, in Montana. Historically,
there were as many as 1,000 active Minuteman missiles, to say nothing of
retired missile programs like Titan and Atlas. At least three Minuteman missile
facilities are now historic sites open to the public, a somewhat incongruous
experience considering their broad similarity to the facilities still in active
use. Many others are abandoned, typically in various states of permanent
destruction to satisfy treaty obligations. Fifty Minuteman IIIs of the 341st
are currently held in an inactive "reserve" state, out of service but ready for
future emplacement, a notable situation given the strict limits Cold War
treaties place on stockpiled ICBMs.</p>
<p>Minuteman employed a significantly different launch configuration from earlier
ICBM programs. Large facilities were difficult to protect from a first strike;
distance was the only effective protection from increasingly accurate Soviet
weapons. Scattered single facilities were difficult and costly to staff.
Minuteman selected a compromise point: clusters of ten independent Launch
Facilities (LFs), spaced miles apart and called a "flight," are remotely
monitored and operated from a single Missile Alert Facility (MAF). Groups of
four to five missile flights constitute a squadron, and about four squadrons
compose a wing, which is supported by an Air Force Base.</p>
<p>In each Missile Alert Facility, a Missile Combat Crew Commander (MCCC) and
Deputy Missile Combat Crew Commander (DMCCC) lock themselves into an
underground capsule called a Launch Control Center (LCC) for each 24 hour
watch. Originally designed by Boeing, the LCC resembles the interior of an
aircraft more than a building, fitting its crew of Air Force officers. The
LCC is an isolated, self-contained system with all of the equipment needed to
monitor, configure, and launch the missiles. A surface building above contains
a security control center and quarters for the security force, responsible not
only for the MAF but also for the ten LFs under its supervision. Still, the
surface building is both powerless over and largely unneeded by the LCC beneath
it. In the event of nuclear war, it was assumed, surface structures in these
sparsely populated but strategically critical parts of the country would be
wiped cleanly away from the earth. Only the hardened infrastructure would
remain: the LCCs, the LFs, and communications infrastructure.</p>
<p><img alt="Missile Alert Facility" src="https://computer.rip/f/minuteman/quebec.jpg" /></p>
<p>Minuteman missile combat crews had more duties than just to wait. They
performed remote tests on the missiles, launch, and control equipment; they
monitored alarm systems that reported malfunctions and remotely supervised the
work of maintenance crews; and they monitored the security systems that
protected the unmanned LFs, authorizing access and dispatching security forces
on the surface to any unknown intrusion. Still, their primary responsibility,
the one for which we all know them, is the disposition of Emergency War Orders
(EWO).</p>
<p>In fine Air Force tradition, actions that may very well presage the end of the
world as we know it are presented in the form of a checklist. EWOs are
authenticated against codes and secrets. Two keys, just like in the movies, are
inserted. The missiles are enabled by remote command. Targeting information is
transmitted to the missiles. The keys are turned, the launch code sent, and the
rest is automated, under the control of the missiles themselves. The missile
crew are miles away, so the Real Thing, the Shit Hitting the Fan, must feel
pretty anticlimactic. The only apocalyptic horsemen they'll see are a series of
indicator lights on the MCCC's console: LCH CMD. LCH IN PROC. MISSILE AWAY.</p>
<p>The realities of ICBM operation are fascinating and incline one towards drama.
Somehow the work of submarine and bomber crews seems more ordinary; they are at
least "out there," in or near enemy territory. Missile crews are sealed in a
very small room buried below a small building in a corner cut out of a farm
field near, but not too near, to a highway for logistical convenience. They are
entirely dependent on electronic communications, not only to receive their
orders, but even to use their weapons. They have the original email job: since
1962, they have served primarily to send and receive messages, mostly by text.</p>
<p>With the rather purple introduction complete, I am going to talk about this
communications technology. But first, just a little more preface.</p>
<p>Most of the reduction of the Minuteman force has been a direct response to
treaties, which imposed progressively lower caps on the nuclear stockpile.
Some reductions were more of a historical accident. In the 1980s, political
considerations lead to a decision to "temporarily" deploy the Peacekeeper
missile, with 10 MIRV (multiple independent reentry vehicle) warheads, to a set
of fifty silos of the 90th Missile Wing's 400th Missile Squadron, in Wyoming.
The Peacekeeper, fielded late in the Cold War, was a profoundly controversial
program. The fifty Peacekeepers retrofitted into Minuteman silos would be the
only ever installed, and their temporary homes became permanent. Most of the
missile's warheads were removed for compliance with Start II treaty, which
Russia never ratified and the United States withdrew from. Still, for
cost-savings reasons, the odd-duck Peacekeeper program was terminated. The last
Peacekeeper missiles were retired in 2005.</p>
<p>I got it into my head to write a detailed description of the missile field
communications system because of my visit to QUEBEC-01, one of the five MAFs
associated with these Peacekeeper missiles, and now a Wyoming State Historical
Site. For that reason, I will most closely describe the communications system
as installed in the 90th Missile Wing, the remainder of which is still active
today. Minuteman missile fields were built over a period of years by different
contractors and have since been through multiple modernization programs. Each
change has introduced inconsistencies. While I will point out some of the more
interesting variations between Minuteman installations, this is best taken as a
description of the "average" Minuteman squadron, one that is typical of the
others but does not exactly exist.</p>
<p>Although I am not exactly aiming for academic rigor, this information is based
mostly on documents available through the Defense Technical Information Center,
which include both original documentation from the Minuteman program and more
recent documents related to modernization programs, proposed changes, and the
retirement of many Minuteman facilities. I have supplemented those documents
with recollections by former Air Force personnel when available, and as always,
I welcome any corrections or additional information. One of the pleasures of
writing about military history is the tendency of veterans to reach out to me
with corrections and stories; I apologize that I am not always good about
getting back to people, particularly phone calls.</p>
<p>The Minuteman III, a fairly direct evolution of the original Minuteman
design, is still in active service. Many detailed materials about the
Minuteman program are probably currently classified, most of the others are
formerly classified and thus have not consistently made their way to archives.
Certain basic questions remain frustratingly unanswered. That's just how it
goes.</p>
<p>I'm also going to try really hard not to be too annoying with the acronyms, but
it's not easy.</p>
<h2>Personnel</h2>
<p>While there were originally three-person crews, Minuteman LCCs have had two
crew members for many decades, the MCCC and DMCCC. The MCCC is superior to the
DMCCC, but the nature of missile operations and such a small crew mean that
their roles are somewhat more complex than commander and deputy. Missile crews
operate according to the "two-person concept," a general prohibition on any
person working alone. This rule is intended to improve safety, reduce mistakes,
and most importantly, mitigate the risk of an unauthorized launch. There are
additional safeguards against unauthorized launch in the Minuteman system which
will be discussed later. Both the MCCC and the DMCCC are required to initiate a
launch.</p>
<p>The MCCC sits at a console that is focused around monitoring and control of the
launch facilities. They have ready access to procedures and documentation. The
DMCCC sits at a separate console that is focused on communications. Their chair
slides on rails, allowing them to access the equipment racks and teleprinters
to the sides of the DMCCC position. The DMCCC is primarily responsible for
communications, so we are most interested in the equipment under their control.</p>
<p>Missile Alert Facilities were designed with a goal of self-containment for
survivability. Most communications equipment is within the LCC itself, readily
available to the DMCCC so that they can at least diagnose problems, if not make
a repair. To this end, DMCCCs receive significant training on technical details
of the communications and computer systems.</p>
<p>Should a problem occur outside of the LCC, the MCC would request assistance
from the Air Force Base. Several Air Force ratings had expertise in
communications equipment, ranging from communications technicians that would
investigate problems within the LF to cable splicers that would repair damage
in the outside plant.</p>
<p>The Minuteman program is somewhat unusual in the extensive construction of
long-distance communications equipment by the Air Force.  AT&amp;T's role in the
missile fields was surprisingly limited; most communications followed routes
fully under the control of the Air Force.</p>
<h2>External Communications</h2>
<p>We can generally divide Minuteman communications systems into two categories:
external and internal. External communications systems are primarily used by
the Missile Combat Crew (MCC) to receive orders, including Emergency War Orders
authorizing the use of nuclear weapons. Internal communications systems are
used within the missile field, primarily to allow the MCC to communicate with
the launch facilities under their control. Some details blur the lines: for
example, there are communications systems which allow the MCC to contact their
Air Force Base, where support facilities and maintenance crews are found. I
will consider these internal systems, but you could argue for the opposite.</p>
<p>The external communications systems available to Minuteman crews have varied
over time. Perhaps the most exotic was the Survivable Low Frequency
Communications System (SLFCS), based on the LF equipment used by the Navy for
communications with submarines. Missile facilities are not underwater, but
nuclear detonations cause significant disruption to the atmosphere that greatly
interferes with radio propagation in the HF range. LF communications are
expected to be less affected in a nuclear combat environment. SLFCS
specifically operated between 14kHz and 60kHz. Some, but not all, Minuteman
MAFs were equipped with a magnetic loop antenna, about 6' in diameter,  buried
shallowly underground.</p>
<p>All MAFs were equipped with HF antennas, although they were decommissioned in
the 1980s. The HF antennas are described as hardened, but it is not feasible to
truly harden an HF antenna. They must be fairly large, and HF does not
penetrate the ground well, making it impractical to bury them. Instead,
hardened HF antennas are perhaps better described as "hidden" HF antennas.  The
typical design is a monopole that stores in a long, narrow silo underground,
awaiting post-attack deployment. MAFs had two separate hardened HF antennas,
one for transmit, and one for receive.</p>
<p>The receive antenna was the most critical, as it would be needed to receive war
orders over the High Frequency Global Communications System (HFGCS). HFGCS is
one of the primary ways that an Emergency War Order would be distributed to Air
Force units including both missiles and bombers. The hardened HF receive
antenna assembly actually included six 160' monopoles: one was extended for
normal use, but in the event of nuclear attack, the five others were stored
telescoped in a silo about 30' deep and could be deployed by a small explosive
charge. There were, in the parlance apparently used by the Air Force, five
"reloads."</p>
<p>The HF transmit antenna, being less important in an attack scenario, had only
one replacement. A "soft" HF transmit antenna of conventional design was in normal
use but backed up by a single 120' hardened antenna stored in a separate silo.
A 50' radius buried ground plane surrounded the hardened antenna.</p>
<p>Near the MAF surface building, a small, white metal cone protrudes from the
ground. The cone consists of a huge cast steel blast deflector with a
depression in the center, which is covered by a fiberglass cone. The cone
houses a compact UHF antenna. This antenna, a 1970s upgrade, can receive war
orders via several satellite systems or directly from an aircraft such as the
E-6. The E-6 airborne command post can serve in various roles, including as a
"Looking Glass" airborne command post (taking control of nuclear forces in the
event of a loss of ground-based command posts) or a "TACAMO" Take Charge and
Move Out relay, transmitting a war order from elsewhere to forces in the field.</p>
<p><img alt="Hardened UHF antenna" src="https://computer.rip/f/minuteman/uhf.jpg" /></p>
<p>UHF communications are essentially line of sight, especially with the use of a
partially in-ground hardened antenna.  In a scenario with extensive loss of
communications infrastructure, particularly with ASAT warfare disabling
military communications satellites, an E-6 or another similarly equipped
aircraft would fly over Minuteman missile fields and deliver an emergency war
order directly to each LCC.</p>
<p>Complimenting this capability, in-service Minuteman launch facilities have
themselves been equipped with a similar UHF antenna. Looking Glass and TACAMO
aircraft actually have Air Force missileers on board who serve as an Airborne
Launch Control Center (ALCC). In the event of a loss of most of the LCCs in a
missile field, an ALCC can issue launch instructions directly to each LF
without any need for the regular missile crews or internal communications
infrastructure.</p>
<p>During the early '90s, a super high frequency (SHF) small satellite terminal
was installed at each active MAF. It is housed in a small, white radome at the
top of a pole near the surface building. SHF is widely used by modern military
satellite systems, such as the Air Force's Wideband Global SATCOM.</p>
<p><img alt="Satellite terminal" src="https://computer.rip/f/minuteman/sat.jpg" /></p>
<p>Each of these antennas is connected by buried conduits to radio equipment in
the LCC's radio racks. At the DMCCC's position, the telephone console allows
the DMCCC to talk or listen on each radio by selecting it with a pushbutton.
Over time, the radios were also attached to more modern digital systems.
Depending on the year, teleprinters or computer displays would receive text
messages via the UHF and SHF radio systems.</p>
<p>The external radio systems were actually all backup or secondary. The primary
means of nuclear C2 within the Strategic Air Command and, later, Global Strike
Command, has long been a digital computer network. While Minuteman
installations began with just a teleprinter to receive orders via leased
telephone line, the late '60s saw the introduction of the Strategic Automated
Command and Control System (SACCS), which was itself replaced by the Strategic
Air Command Digital Network (SACDIN). A small (for the era) computer in a rack
to the right of the DMCCC's station allowed two-way messaging with SAC
headquarters over leased telephone lines. Reportedly, these and other Bell
System telephone lines to LCCs were carried by buried telephone cable to small
hardened exchange buildings serving each missile field. I have not yet
researched this topic closely.</p>
<p>In Peacekeeper LCCs as well as Minuteman LCCs from the '70s to the '90s, the
specific computer used for this purpose was called the Command Data Buffer
(CDB). The CDB was connected to both SACCS/SACDIN and the internal
communications network, in order to accurately relay targeting information to
the missiles. This will be discussed later in the context of rapid retargeting.
In the 1990s, the REACT system was installed for a similar purpose.</p>
<p>Quebec-01 LCC was equipped with a teleprinter with a selector between SACDIN
and AFSAT (UHF satellite) receivers. I'm not sure why the teleprinter was
retained after the CDB upgrade, very possibly just for redundancy.</p>
<p><img alt="SACDIN Teleprinter" src="https://computer.rip/f/minuteman/teleprinter.jpg" /></p>
<p>Somewhere between external and internal, each LCC had access to two dial
telephone lines. These connected directly to the PSTN. Some circumstantial
evidence leads me to think these dial lines were shared with the surface
building, which probably explains the need for two.  The dial lines were mostly
used to contact support crews at the Air Force Base for routine maintenance
issues.</p>
<h2>HICS Digital Communications</h2>
<p><img alt="HICS cable splice" src="https://computer.rip/f/minuteman/splice.jpg" /></p>
<p>I was most interested in the internal communications systems, which have
garnered less historical documentation than the external links. In most cases,
there is only really one: the Hardened Intersite Cable System, HICS.</p>
<p>HICS consists of multipair pressurized telephone cables trenched between
Minuteman facilities. HICS carries digital traffic for C2, and many Air Force
documents use the term "HICS" exclusively to mean the digital channel, but the
same cables carried multiple voice pairs. Let's consider the digital capability
first, though.</p>
<p>HICS, as originally installed, operated at 1.3Kbps. Details on the actual
encoding are hard to come by, but given the time period I assume it was
generally similar to the AFSK schemes used by other early telephone data links.
The punch line, of course, is that the 1.3Kbps stuck---based on some Air Force
journal articles on options for upgrades, it seems that contemporary Minuteman
III fields still communicate over HICS at 1.3Kbps. Remember that when we get to
retargeting.</p>
<p>The topology of the digital HICS network is rather interesting. It was designed
for redundancy and reliability, but prior to most of our modern understanding
of computer networking. There's a mix of a few different ideas.</p>
<p>One of the things I'm not completely confident of is the size of the collision
domain within HICS, or how much of the cable network was a common bus. From
reading between the lines of some different reports and considering the overall
design, I'm fairly confident that the entire digital HICS network was a single
shared bus within each flight, and I think it is likely that it was a shared
bus within each squadron. This bus would be tens of miles long with multiple
branches, a challenging electrical situation that perhaps explains why the Air
Force has repeatedly found it to be infeasible to make HICS faster.</p>
<p><img alt="HICS cable map" src="https://computer.rip/f/minuteman/hics.jpg" /></p>
<p>Thanks to <a href="https://minutemanmissile.com">minutemanmissile.com</a> for this image
of the Warren AFB/90th Missile Wing HICS map, which is far more legible
than the photo I had taken.</p>
<p>Each of the LCCs, denoted in the map by open rectangles, is connected to four
"loops" of HICS cable. The legs of adjacent loops to the LCC are shared,
though, so it's perhaps easier to describe this way: each LCC is surrounded by
a ring of HICS cable, to which it is connected by four legs spread roughly 90
degrees apart. This design gives a fair amount of redundancy, a break anywhere
in the ring or even breaks of more than one of the LCC's legs would still leave
a working path.</p>
<p>This latter scenario was probably one of the designer's greatest concerns, as
the LCCs would be obvious targets for inbound nuclear attacks. The cable layout
provides four-times redundancy on the cables to the LCC, but no redundancy at
all on the cables to individual LFs. That tells you a lot about their threat
modeling. Facilities were spaced far enough apart that a precision strike on an
LF would probably disable only that single LF; a precision strike on an LCC,
though, could potentially disable ten LFs at once. As the accuracy and power of
nuclear warheads improved, it seemed more likely that a first strike would
succeed in disabling at least some LCCs. A lot of the complexity of HICS is
intended to account for that possibility.</p>
<p>Each of the LFs is connected to the ring via a single leg. In some cases,
multiple LFs are along the same leg. These are often the same long runs that
connect the rings of two different LCCs together. In general, each LCC ring is
connected to two of its neighbors, although sometimes it will instead have two
redundant connections to a single neighbor. Based on the map and situation on
the ground, these inter-flight connections don't seem to have required any
active equipment, only a splice case. That supports the theory that an entire
squadron was a shared bus, although it's possible that a separate pair to the
"foreign" LCC ring would home-run to the LCC to allow separately sending
messages to either. The network doesn't seem to have that kind of selective
routing capability, though, so I find it unlikely.</p>
<p>Digital messages do seem to have been packetized, and were distributed through
the network on a "flood fill" basis. That is, every active node on the digital
network repeated every message it received. You might wonder about flow control
and the avoidance of cycles; only a very primitive method was used. Each node,
after transmitting a message, would "lock out" the cable it was transmitted on
for a long enough period for the node on the other end to finish repeating the
message.</p>
<p>This explanation is a little more difficult to understand, though, when applied
to the actual layout of the HICS system. What exactly constitutes a node? You
will note that the map distinctly shows intersquadron connections, with both
thicker lines and open circles where they connect to an LCC ring. My theory is
that these intersquadron connections where the only places where active
repeating of messages was required. Whether or not repeating messages between
squadrons was selective is unclear. Did a message from an LCC to one of its
nearby LFs get repeated across squadrons to the opposite end of the field?  If
repeating had originally been completely non-selective, I suspect that was
changed as part of the work done to facilitate retargeting.</p>
<p>We can infer certain things about the HICS network from the equipment in
Quebec-01.  For example, HICS must have had a fair number of active repeaters.
along a path.</p>
<p><img alt="HICS diagnostic panel" src="https://computer.rip/f/minuteman/hics_equip.jpg" /></p>
<p>Inside of the LCC, we find something like a tiny long-distance telephone test
desk. A rack includes pressurization alarms for five cables (we know of four
legs to the LCC ring, is the fifth perhaps a cable to the local telephone
exchange?), and a fault isolation panel. When a cable seemed to have been lost,
the DMCCC could use this panel to locate the problem along the cable. This
probably relied on a loopback test feature of the repeaters, but I'm not sure
exactly the operating principle. Further down in the rack is what appears to
be a cable power supply.</p>
<p>Repeaters were definitely installed inside the LCCs, but the number of
selections on this test panel makes me suspect that there were also in-line
repeaters on the cable, perhaps taking power from that power supply. This is
entirely speculative, but A repeaters may have been located along the LCC ring
and B repeaters on legs, making the two-knob selector arrangement useful to
test a specific repeater on a specific leg.</p>
<p><img alt="HICS termination point" src="https://computer.rip/f/minuteman/hics_term.jpg" /></p>
<p>In the equipment side of the LCC, where the generator and chiller are located,
we also find the terminations of the HICS cables. Note the mostly empty rack
that would have held repeater equipment, and the air dryer and flow gauges
for cable pressurization.</p>
<p>Finally, I should talk a bit about the exception to all of this: the 321st
Missile Wing, in North Dakota, was built later than the 90th and 91st and by a
different contractor. Sylvania, not Boeing, won the bid to build the LCCs and
LFs. Much of the equipment is the same, but Sylvania did inject a few of their
own ideas, and one of them was radio redundancy for HICS. The 321st apparently
had a simplified HICS topology; I'm not sure of the details but I would guess
that they may not have provided the four redundant cables to each LCC.</p>
<p>To make up for it, each LCC and LF in the 321st is equipped with a large,
buried antenna, a grid-like arrangement of crossing dipoles that took up an
area similar to the sewage lagoons outside of the fence. These antennas made up
a medium-frequency, ground-wave communications network that could be used as an
alternative to HICS. The 321sts redundant radio system, apparently called
"Deuce" at the time, could be viewed as a precursor to the later nationwide
GWEN radio C2 network. It seems to have carried the same digital messages as
HICS, and the DMCCC had selectors to choose whether messages would be sent by
cable or radio.</p>
<h2>HICS Voice Communications</h2>
<p>Now, let's take a close look at the DMCCC's communications console, which tells
us a lot about the voice capabilities.</p>
<p><img alt="DMCCC communications console" src="https://computer.rip/f/minuteman/telephone.jpg" /></p>
<p>They get a lot of buttons! There appear to be two separate busses, I'm not
completely sure of the significance of that layout. I know from an airman's
anecdote that the DMCCCs could conference together LF phones and the dial
telephone lines, and sometimes did so that maintenance crews stuck at an LF
overnight could make apologies to their families. This makes me think that it
is not a matter of "one selection per bus," but rather probably indicates that
lines can only be joined within a bus. The logic behind that design is not
clear to me.</p>
<p>Anyway, let's see if we know enough to explain all of these buttons. Some are
easy: for the speaker and handset, there are selectors each of the radios.
The "LF Lines" correspond to each of the ten LFs, numbered 2-11 since the LCC
is numbered as site 1. We see the two dial lines, regular telephone lines
provided at each site, and they are even labeled with their phone numbers. The
five-digit notation dates this hand-written addition to the 2L-5N era, which
probably persisted unusually late in rural Wyoming.</p>
<p>The rest of the buttons correspond to specific pairs that would emerge in
different places in the HICS network. The "SCC" button likely allows
communications with the security command center in the surface building, just
up the elevator from the LCC. The "LCC" button I am less sure of; perhaps it
was a party line of other LCCs in the squadron?  The "LCC Ring" selections must
correspond to the four HICS cable rings extending from the LCC, but I'm not
sure which devices would be found on those pairs. They may be "order wires,"
available in the splice boxes and as jacks at sites and normally used only by
maintenance crews working on cables outside of the LFs.</p>
<p>The EWO buttons are interesting. EWO is, of course, Emergency War Order, but in
the context of Minuteman was also the term used for party lines connecting the
Air Force Base to the LCCs. These could be used, of course, as a redundant way
to deliver EWOs, as well as for general communications across the missile
field. There are two for redundancy: one was routed via AT&amp;T infrastructure,
following a cable from the LCC to a telephone exchange. The other was routed
via HICS. I am not sure why only one merits a "RNG" button, that could apply
ringing voltage to get the attention of other stations.</p>
<p>I am assuming, by the magic of speculation, that the "OPR" button on the left
of each bus probably selected which bus the headset was connected to. There is
also a dial, for use with the dial lines.</p>
<p>These voice connections within the field were of critical importance because of
Minuteman's strict security posture. The unattended LFs were equipped with
intrusion alarms for physical security, initially a bistatic radar system more
similar to that used at Titan, and later a DSP-based monostatic radar system
called the IMPSS. Any personnel or, reportedly, large rabbits approaching an LF
would cause an alarm, and security forces were dispatched to investigate unless
the intruder used a HICS voice circuit to authenticate themselves to the LCC.
The process of "penetrating" a secure LF could take a missile crew thirty
minutes or more, and involved multiple calls to the LCC as different alarms
were triggered.</p>
<h2>HICS Outside Plant</h2>
<p>Old aerial images and historical documents from the Air Force give us some
insight into the construction of HICS. HICS cables were installed in open
trenches and then covered, rather than placed directly with a vibratory plow as
would become common later. Splices were done in large holes with scotch-lok
connectors and cast iron splice casings. Over time, many of the splice casings
had to be replaced due to premature corrosion, and different materials were
tried before settling on brass. A cathodic protection system was installed
as a permanent solution to the problem.</p>
<p>I have done my best to trace some of the HICS cables along their routes. The
holes used for splicing are sometimes visible as scars, but it does not appear
that any manholes were installed; instead splices were made near RoW markers
and will have to be excavated for repairs. The lack of manholes suggests that
there may not be active equipment along the cable routes, I'm not really sure.</p>
<p>The RoW markers used by the Air Force are substantially similar to the style
used by AT&amp;T at the time. They are round wooden posts, about 6' tall, with
metal bands around the top. Unlike AT&amp;T, the Air Force used white bands, and it
seems that there are always two. Shorter markers are used in some places, I
suspect where there are splice cases not near road crossings. Where the cables
cross roads, the Air Force usually installed gates with sturdy metal posts in
the roadside fences. Sometimes these gates are the easiest evidence to find in
aerial photos.</p>
<p>One of my biggest questions is about the inter-squadron relays. The map depicts
them as nodes, but they aren't located at LFs or other facilities. I wondered
if there might be active equipment, but I found one of the locations where in
inter-squadron cable takes off from an LCC ring and there is no indication of
even a manhole. In case you might be interested, <a href="https://computer.rip/f/minuteman/90th-mw.kmz">here is a KML</a>
with the cable tracks I have worked out so far.</p>
<h2>Cross-Flight Communications</h2>
<p>HICS served primarily to allow an LCC to communicate within the ten missiles
in its flight. However, the entire squadron and then the entire missile wing
were interconnected, and Minuteman took advantage of this capability for
several purposes.</p>
<p>First, a specific LCC in each squadron was designated as the Squadron Command
Post (SCP). The SCP was capable of sending launch orders to any missile within
the squadron, and of countermanding launch orders issued by any of the
squadron's LCCs. This provided a measure of protection against a destroyed or
compromised LCC.</p>
<p>Over time, the security of Minuteman missiles was further enhanced by the
addition of a "vote to launch" system. Minuteman missiles can only be launched
if launch orders are sent by at least two LCCs, requiring a total of four
individuals.</p>
<p>In some Minuteman fields (and all current fields), a Wing command post serves
in a role similar to the SCP but across the entire wing. It provides one
central point where the entire wing's missile inventory can be monitored and,
if necessary, controlled.</p>
<h2>Alarms</h2>
<p>Besides missile C2 and voice communications, one of the main functions of the
HICS was the reporting of alarms within unattended LFs to the LCC. Some alarms,
particularly related to the missile itself, would be sent by the missile
guidance computer over the HICS digital network. These alarms would be printed
by the teleprinter below the DMCCC's desk, along with confirmations of commands
received and other routine traffic.</p>
<p>There was also a second, dedicated alarm system called the Voice Reporting
Status Assembly or VRSA. VRSA seems to have relied on its own pairs in the HICS
cables, and resembled the simple alarm reporting telephones coming into use by
AT&amp;T. The DMCCC could select an LF and press a button to send a tone, which
triggered a device at the LF to "read back" any status alarms via voice
recordings. At the time this almost certainly involved some interesting
magnetic tape equipment, but I haven't found much information on the LF end of
the VRSA. Toggle switches on the VRSA console allowed the DMCCC to reset the
alarm device in the LF, clearing any recorded alarms that weren't active.</p>
<p>The VRSA was an upgrade over the original Minuteman installations, which used a
very similar panel to send a safe/arm tone to the LFs as part of the launch
process. Since part of standard testing practice was for DMCCCs to flip the
toggle switch for an LF to remove the "safe" tone and arm the site, it was a
fairly obvious evolution to have the site report any faults in response to a
tone. Reportedly, the VRSA panels were the original safe/arm panels with
modifications.</p>
<h2>Retargeting</h2>
<p>When Minuteman was originally installed, each missile's targeting data was
loaded from tape using equipment in the LF. To retarget a missile, a
maintenance crew had to travel to the site, access it, run the new target tape
through equipment in the LF that sent the data to the guidance computer, and
complete a recalibration of the inertial reference platform in the missile.
This was something like a 12-hour process overall, and retargeting a squadron
would take weeks.</p>
<p>Fixed targets were practical when the "enemy" was self-evidently the Soviet
Union and any attack would be all-out. Over the 70-year lifespan of the
Minuteman program, though, the geopolitical and military environment has
changed. There are now other nuclear adversaries, and their military assets are
increasingly mobile. The biggest challenge to Minuteman's targeting was the
Soviet Union's development of road-mobile ICBMs like the RT-21.  To eliminate
the USSR's nuclear capability, we would have to fire on these mobile systems
wherever they were located. Aerial and satellite surveillance could be
surprisingly effective in keeping track of these large, slow-moving TELs, but
the Minuteman missiles could not be retargeted to keep up with that
intelligence.</p>
<p>In response, a series of enhancements were made (often as part of the Minuteman
II program) to introduce "rapid retargeting." Rapid retargeting allowed the
missiles to be retargeted from within the LCC. During the 1970s, a computer
system called the Command Data Buffer (CDB) was installed in each LCC. The CDB
could receive targeting parameters from SAC and then transmit them to the LFs.
It was theoretically possible to retarget missiles shortly before launch. In
practice, the "shortly" wasn't very achievable</p>
<p><img alt="DMCCC station with CDB" src="https://computer.rip/f/minuteman/dmccc.jpg" /></p>
<p>The HICS network was capable of 1.3Kbps, and because of the "flood" design of
the network, that was essentially 1.3Kbps of total capacity across a single
collision domain. In other words, only 1.3Kbps of total traffic could be
handled, with far less available from point to point when the network is under
heavy use. Further, enhancements to the Minuteman system added cryptographic
authentication of messages over HICS and, later, encryption of the messages
themselves. The added overhead of the cryptographic system further reduced
network capacity.</p>
<p>Retargeting a squadron of Minuteman missiles via the CDB took over 20 hours.
Retargeting a single missile could take 30 minutes.</p>
<p>CDB represented a major step forward in Minuteman C2, particularly with its
real-time messaging capability. Retargeting was still a severely limited
capability, though. </p>
<p>During the 1990s, active Minuteman sites were upgraded to the Rapid Execution
and Combat Targeting System, or REACT. More than just an upgrade for
retargeting, REACT brought a completely new control system that significantly
changed the layout of LCCs. Instead of sitting at opposite ends of the tube,
REACT put the MCCC and DMCCC directly alongside each other and centralized
almost all control functionality onto computer displays.</p>
<p>It also further refined retargeting: retargeting an entire squadron now takes
only ten hours. More radically, though, a single missile can be retargeted in
only a couple of minutes, making it feasible to retarget a missile just before
firing in a limited attack scenario.</p>
<h2>The future of HICS</h2>
<p>While both over budget and behind schedule, the Sentinel program is expected to
replace the Minuteman missiles. Sentinel will likely be an in-place upgrade,
installing new missiles and control systems in the existing Minuteman silos.
It has been clear for decades now that HICS isn't capable of meeting modern
expectations, so Sentinel will include a complete replacement.</p>
<p>Various options including DSL over HICS cables and radio were considered, but
the current plan is to trench new fiber-optic cables across the launch fields.
They're less interesting, but fiber optic cables have both capacity and
reliability advantages over telephone cables, and could easily remain in
service for the life of the Sentinel program.</p> ]]></description>
	</item>

 	<item>
		<title>2024-07-13 the contemporary carphone</title>
		<pubDate>Sat, 13 Jul 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-07-13-the-contemporary-carphone.html</link>
		<guid>https://computer.rip/2024-07-13-the-contemporary-carphone.html</guid>
		<description><![CDATA[ <p><a href="https://www.youtube.com/@CathodeRayDude">Cathode Ray Dude</a>, in one of his
many excellent "Quick Start" videos, made an interesting observation that
has stuck in my brain: sophisticated computer users, nerds if you will, have
a tendency to completely ignore things that they know are worthless. He was
referring to the second power button present on a remarkably large portion of
laptops sold in the Windows Vista era. Indeed, I owned at least two of these
laptops and never gave it any real consideration.</p>
<p>I think the phenomenon is broader. As consumers in general, we've gotten very
good at completely disregarding things that don't offer us anything worthwhile,
even when they want to be noticed. "Banner blindness" is a particularly acute
form of this adaptation to capitalism. Our almost subconscious filtering of our
perception to things that seem worth the intellectual effort allows a lot of
ubiquitous features of products to fly under the radar. Buttons that we just
never press, because sometime a decade ago we got the impression they were
useless.</p>
<p>I haven't written for a bit because I've been doing a lot of traveling.
Somewhere in the two thousand miles or so we covered, my husband gestured
vaguely at the headliner of our car. "what <em>is</em> that button for?" He was
referring to a button that I have a learned inability to perceive: the friendly
blue "information" button, right next to the less friendly red "SOS" button.
Most cars on the US market today have these buttons, and in Europe they're
mandatory (well, at least the red one, but I suspect the value-add potential of
the blue on is not one that most automakers would turn down). And there's a
whole story behind them.</p>
<p>It all started in 1996 at General Motors. Wikipedia tells us that it actually
started with a collaboration of General Motors (GM), Electronic Data Systems
(EDS), and Hughes Electronics. That isn't incorrect, but misses the interesting
point that EDS and Hughes were both <em>subsidiaries of GM</em> at the time. GM was a
massive company, full of what you might call vim and vigor, and it happened to
own both a major IT services firm (EDS) and a major communications technology
company (Hughes). It was sort of inevitable that GM would try integrating these
into some sort of sophisticated car-technology-communications platform. They
went full-steam ahead on this ambitious project, and what they delivered is
OnStar.</p>
<p>But first, a brief tangent into corporate history. I won't say much about GM
because I am not an automotive history person at all, but I will say a bit
about EDS and Hughes. Hughes was obviously the product of a notable and often
enigmatic figure of history, Howard Hughes. GM owned Hughes Electronics because
Howard Hughes had cleverly placed his business ventures under the ownership of
a massive and brazen tax shelter called the Howard Hughes Medical Institute.
Howard Hughes died without leaving a will or any successor as trustee of HHMI,
putting HHMI into an awkward legal and organizational struggle as it abruptly
pivoted from "Howard Hughes' personal tax scheme" to "independent foundation
that incidentally owned a major defense contractor." HHMI ultimately made the
decision to turn Hughes' business empire into an endowment, and sold Hughes
Aircraft. General Motors was the high bidder. A set of confusing details of the
Hughes amalgamation, like the fact that Hughes Aircraft didn't own all of the
Hughes aircraft, lead to the whole thing becoming the Hughes Electronics
subsidiary of GM.</p>
<p>After GM essentially stripped it for parts, Hughes Electronics lives on today
under the name DirecTV. The satellite internet company that actually markets
under the name Hughes is, oddly enough, one of the parts that GM stripped off.
Hughes Communications became part of EchoStar, operator of Hughes Electronics
competitor DISH Network, which then spun Hughes Communications out, and then
bought it back again. You can't make this stuff up. The point is that the
strange legacy of Howard Hughes, the HHMI, and GM's ownership of Hughes
Aircraft mean that the name "Hughes" is now sort of randomly splashed across
the satellite communications industry. It's sort of like how Martin Marietta
still paves freeways in Colorado.</p>
<p>Electronic Data Systems is not quite as interesting, but it was run by two-time
minor presidential candidate Ross Perot, so that's something. GM dumped EDS
almost immediately after launching OnStar. EDS eventually became part of
Hewlett Packard, which, by that time, had become a sort of retirement home for
enterprise technology companies. It more or less survives today as part of
various large companies that you've never heard of but have nonetheless secured
9-digit contracts to do ominous things for the Department of Defense.</p>
<p>What a crowd, huh? It's a good thing that nothing strange and terrible happened
to General Motors in approximately 2008.</p>
<p>So anyway, OnStar. OnStar was, basically, a straightforward evolution of the
carphone backed by a concierge-like telephone service center. In that light,
it's an unsurprising development: the carphone was just on its way out in the
mid-'90s, falling victim to increasingly portable handheld phones. Hughes, by
its division Hughes Network Systems, was an established carphone manufacturer
but seems to have had few or no offerings in the mobile phone space [1]. To
Hughes long-timers, OnStar was probably an obvious way to preserve the
popularity of carphones: build them into the car at the factory, with
factory-quality finish.</p>
<p>GM had their own goals. Ironically, it is in large part due to GM's efforts
that built-in telephony is so common (and yet so ignored) in cars today. The
situation was much different in 1996: OnStar was a new offering, only available
from GM. It had the promise of competitive differentiation from other
automakers, but for that to work, GM would have to differentiate it from the
carphones widely available on the aftermarket. This tension, the conflict between
"we built a carphone in at the factory" and "carphones are going out of style,"
probably explains why OnStar marketing focused on safety and security.</p>
<p>"General Motors has come up with the ultimate safety system" lead a '96
newspaper article. Marketing materials prominently positioned roadside
assistance, automatic emergency calls on airbag deployment, remote door unlock,
and locating stolen cars. These were features that your average carphone
couldn't offer, because they required closer integration with the vehicle
itself. OnStar was more than a carphone, it was a telematics system.</p>
<p>"Telematics" is one of those broad, cross-discipline concepts that we don't
really talk about any more because it's become so ubiquitous as to be
uninteresting. Like Cybernetics, but without a tantalizing but lost historical
promise in Chile. Telematics has often been more or less synonymous with
"putting phones into cars," but is more broadly concerned with communications
technology as it applies to moving vehicles. There is a particular emphasis on
the vehicle part, and telematics has always been interested in vehicle-specific
concerns like positioning, navigation, and the collection of real-time data.</p>
<p>Telematics was already a developed field by the '90s, although the high cost
and large size of communications equipment made it less universal than it is
today. OnStar would lead one of the biggest changes in the modern automotive
industry: the extension of telematics from commercial and industrial equipment
to consumer automobiles. In doing so, it would introduce select GM drivers to
an impressive set of benefits, almost a form of ambient computing. It would
also start a cascade of falling dominoes that lead, rather directly, to a
remarkable lack of privacy in modern vehicles and getting an email that
something mysterious is wrong with your car two to four hours after the tire
pressure light comes on. The computer gives and takes.</p>
<p>And what of EDS? EDS provided the other half of OnStar's differentiation from a
mere carphone. OnStar was not only integrated into your vehicle, it was backed
by a team of Service Advisors with training and tools to use that integration.
The OnStar equipment included a GPS receiver, still a fairly cutting-edge
technology at the time, and continuously provided your location to the OnStar
service center in Michigan. Advisors had access to maps and travel directories
and the ability to dispatch tow trucks and emergency responders. They could
even send a limited set of remote commands to OnStar vehicles. The
infrastructure to support this modern telematic call center was built by EDS,
and the staff of human advisors provided a friendly face and a level of
flexibility that was difficult to achieve by automation alone.</p>
<p>Besides emergencies and roadside assistance, the advisors could solve one of
the most formidable problems in automotive technology: navigation. When GM's
advertising and press coverage strayed from emergency assistance, they focused
on concierge-like services focused around navigation. OnStar could direct you
to gas or food. They could not only reserve a hotel room, but get you to the
hotel. If you have seen the wacky turn-by-turn navigation technology that
proliferated in the late 20th century, you might wonder how exactly that worked.
Did an advisor stay awkwardly on the line? No, of course not, that would be
both awkward and costly. They read out driving directions, which the OnStar
equipment recorded for playback.</p>
<p>I really wish I could find a complete description of the user experience,
because I suspect it was bad. The basic idea of recording spoken guidance and
playing it back for reference is a common feature in aviation radios, but
that's mostly for dealing with characteristically terse and fast-talking ground
controllers, and usually consists of a short playback buffer that always starts
from the beginning. Given the technology available, I suspect the OnStar
approach was similar, but just with a... longer playback buffer. Thinking about
listening through the directions over and over again to find one turn gives
me anxiety, but it was 1996.</p>
<p>Technology advanced like it always does, and by the mid '00s at least some GM
vehicles had the ability to display turn-by-turn instructions, provided by
OnStar, as the driver needed them. Fortunately there are videos from this era,
so I know that the UX was... better than expected, but strange. It's odd to see
an LCD-matrix radio display, with no promise of navigation features, start
displaying large turn arrows and distances after an OnStar call. One of the
interesting things about OnStar is that the "human in the loop" nature of
OnStar features makes it sort of a transitional stage between cassette tapes
and Apple CarPlay. OnStar allowed human operators and remote computer systems
to do the hard parts, allowing cars to behave in a way that seemed very ahead
of their time.</p>
<p>One of the interesting things about OnStar, given the constant mention of
satellites in its marketing, was the lack of actual satellite communications.
Hughes, a satellite technology company, was involved. Articles about OnStar
coyly refer to satellite technology, or say it's "powered by satellites." Of
course, OnStar cost $22.50 a month in 1996, and $22.50 a month didn't entitle
you to so much as look at a satellite phone in 1996. The satellite technology
was limited to the GPS receiver; all voice communications were cellular. AMPS,
specifically. The first several generations of OnStar, into the early '00s,
relied on AMPS.</p>
<p>Telematics, telemetry, and the applications we now call "IoT" often struggle
with the realities of communications networks. AMPS, often just referred to as
"analog," was the first cellular communications standard to reach widespread
popularity. For over a decade, everything cellular used AMPS. Then CDMA and GSM
and even, may we all shed a tear, iDEN took over. These were digital standards
with improved capacity and capabilities. It was inevitable that they would
replace AMPS, and with the short lifespan of a consumer cellular phone, devices
without support for digital networks naturally faded away... except for a bunch
of them. OnStar and burglar alarms are two famous AMPS-retirement scandals. The
deactivation of AMPS networks in 2008 left cars and alarm communicators across
the country unable to communicate, and prompted a series of replacement
programs, lawsuits, trade-in deals, lawsuits, and more lawsuits that are
influential on how cellular networks are retired today (meaning: as rarely as
possible).</p>
<p>The obsolescence of OnStar equipment in older vehicles by AMPS retirement left
a black mark on OnStar's history that still hangs over it today. It was, I
think, a vanguard of the larger impacts of fast-changing technology being
integrated into cars. While vehicles have indeed become more reliable over
time, there is an ever-present anxiety that new cars are more like consumer
electronics, built for a three-year replacement cycle. The forced retirement of
half a million OnStar buttons is probably one of the most visible examples of
automotive equipment failing due to industry change rather than age.</p>
<p>In 2022, 2G cellular service was retired in the United States. With it went
another generation of OnStar-equipped vehicles. For a combination of reasons,
though, both a more conservative approach to 2G retirement in the cellular
industry and likely GM's planning further ahead, only two model years were
impacted.</p>
<p>Incidentally, Ford also had an offering very much like OnStar, called RESCU and
introduced in 1996 as well. It was pretty universally agreed by automotive
journalists at the time that RESCU was more primitive than OnStar and amounted
mostly to a knee-jerk "we also have one of those" response to GM's launch.
RESCU is perhaps worth  mentioning, though, for its contribution to the lineage
of Ford's SYNC platform, at least in the form of gratuitous all-caps.</p>
<p>In 2002, GM offered OnStar for licensing to other automotive manufacturers.
Subarus, among others, began to sprout blue buttons in the overhead. But what
had happened to competitive differentiation? Well, automotive technology tends
to go through two phases: First, it differentiates. Second, it's mandated. The
originating manufacturer can make quite a bit of money off of both.</p>
<p>In 1995, a year prior to the launch of OnStar, the National Highway
Transportation Safety Administration (NHTSA) was already investigating the
possibility of an Automated Collision Notification (ACN) system. ACN would
automatically call 911 in the event of a dangerous crash, improving driver
safety. As far as I can tell, GM is not the origin of the ACN concept. NHTSA's
work on ACN started with the National Automated Highway System (NAHS), an
ambitious technology development program launched in 1991 that imagined a very
different self-driving car from the ones that we see today. The NAHS involved
mesh networking between automated vehicles to form "platoons," close-following
cars (for fuel efficiency) that synchronized their control actions. The mesh
network would extend to road-side signaling systems, and would lead eventually
to the end of traffic signals as cars automatically negotiated intersection
time slots.</p>
<p>The NAHS never came to be and probably never will, but the NHTSA's
retro-futuristic graphics of '90s sedans linked by blue waves echo through my
childhood like they do through the pages of <em>Popular Mechanics</em> and the
academic literature on self-driving. Or, they did, until a new generation of
Silicon Valley companies coopted self-driving for their own purposes. This is
not an entirely fair take on the history, I am certainly applying rose-colored
(or is it cerulean blue?) glasses to the NAHS, but I think it is hard to argue
that there has not been a loss of ambition in our vision of the self-driving
future. For one, we stopped drawing blue waves on everything.</p>
<p>Anyway, GM may not have created ACN. If anyone, I think that honor might fall
on Johns Hopkins University. But they sure did get involved: by 1996, the year
OnStar launched, Delco Electronics was building ACN prototypes for NHTSA. Delco
Electronics, a division of GM (Delco's history is closely intertwined with that
of Hughes in this period, parts of Delco were and would be parts of Hughes and
vice-versa). Over the following years, GM really jumped in: OnStar was ACN, and
ACN should be mandatory.</p>
<p>Here's the thing: it's never really worked. The move of introducing a
technology and then pushing for it to become mandatory is a fairly well-known
one in the automotive industry, and to its credit, has lead to numerous safety
advancements in consumer cars and no doubt a meaningful reduction on fatalities
(to its discredit, it is often cited as one of the reasons for steeply rising
prices on new cars).</p>
<p>Universal OnStar has come tantalizingly close. Europe mandated "eCall,"
functionally identical to ACN, in 2018. I'm not sure how directly GM was
involved, but there are GM patents in the licensing pool required to implement
eCall, so it's at least more than "not at all." But despite its increasing
presence, ACN isn't required in the US. Automakers aren't even consistent on
whether it's standard or a paid add-on.</p>
<p>GM is still hacking away at this one... as recently as last year, GM was taking
federal grants to study ACN and propose standards. In collaboration with CDC,
GM developed a system called AACN that uses accelerometer data to predict the
severity of injury to occupants and difficulty of rescue. It's installed in
newer OnStar vehicles, and Ford has even licensed it for Ford SYNC, but the
data rarely goes anywhere at all... 911 PSAPs that receive the calls from ACN
systems aren't equipped to receive the extra metadata; extensions to E911 to
facilitate AACN data exchange are another thing that GM is actively involved
in.</p>
<p>GM really seems to have put a 30-year effort into mandating OnStar in the US,
but they just can't get it over the finish line. In the mean time, OnStar has
stopped mattering.</p>
<p>GM's program to license OnStar to other automakers was short-lived. I'm not
sure exactly why, but GM also gave up on their "OnStar For My Vehicle"
aftermarket product. Even as OnStar continued to gain features, its ambition
waned. I think that the problem was simple: by the mid-2000s, putting a phone
into a car was becoming pretty easy. Besides, the "connected car" offered too
many advantages for any automaker to turn down. Can you imagine the benefits of
storing location history for the entire fleet of vehicles you've sold? You can
sell that to the insurance industry! You know GM did, of <em>course</em> they did, and
of course it's the subject of an ongoing class-action lawsuit. </p>
<p>OnStar just stopped being special. I was actually a little surprised to notice
that the blue button in the overhead of my modern Subaru <em>isn't</em> an OnStar
button; Subaru stopped licensing OnStar in '06. It's just another manifestation
of Subaru StarLink, a confusing menagerie of vaguely-telematic features that
are mostly built on contract by Samsung. Once the car has an LTE modem for
remote start and maintenance telemetry and selling your driving habits to
LexisNexis, throwing in a button that makes a phone call is hardly an
engineering achievement.</p>
<p>You know, sometimes it feels like smartphones can only <em>incidentally</em> make
phone calls. With the move to VoLTE, it's not even really a deeply-embedded
functionality any more. "Phone" is just an application on the thing that, for
reasons of habit, we call a "phone."</p>
<p>The legacy of OnStar is much the same: of course your car can make phone calls,
GM shoved a carphone in the trunk in 1996 and it's still in there somewhere.
It's just one of a million things modern vehicle telematics do, and frankly,
it's one of the least interesting ones. Ironically, GM is taking the carphone
back out: in 2022, GM discontinued the OnStar telephone service. It's no longer
possible to have a phone number assigned to your car and use OnStar for routine
calls. Everyone uses an app on their phone for that.</p>
<p>[1] I am excluding here their satellite phones, although they were surprisingly
advanced for the mid-'90s and probably would have competed well with cellular
phones if the service wasn't so costly.</p> ]]></description>
	</item>

 	<item>
		<title>2024-06-08 dmv.org</title>
		<pubDate>Sat, 08 Jun 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-06-08-dmv.org.html</link>
		<guid>https://computer.rip/2024-06-08-dmv.org.html</guid>
		<description><![CDATA[ <p>The majority of US states have something called a "Department of Motor
Vehicles," or DMV. Actually, the universality of the term "DMV" seems to be
overstated. A more general term is "motor vehicle administrator," used for
example by the American Association of Motor Vehicle Administrators to address
the inconsistent terminology.</p>
<p>Not happy with merely noting that I live in a state with an "MVD" rather than a
"DMV," I did the kind of serious investigative journalism that you have come to
expect from me. Of These Fifty United States plus six territories, I count 28
DMVs, 5 MVDs, 5 BMVs, 2 OMVs, 2 "Driver Services," and the remainder are hard
to describe succinctly. In fact, there's a surprising amount of ambiguity
across the board. A number of states don't seem to <em>formally</em> have an agency or
division called the DMV, but nonetheless use the term "DMV" to describe something
like the Office of Driver Licensing of the Department of Transportation.</p>
<p>Indeed, the very topic of <em>where</em> the motor vehicle administrator is found is
interesting. Many exist within the Department of Transportation or Department
of Revenue (which goes by different names depending on the state, such as DTR
or DFA). Some states place driver's licensing within the Department of State.
One of the more unusual cases is Oklahoma, which recently formed a new state
agency for motor vehicle administration but with the goal of expanding to other
state customer service functions... leaving it with the generic name of Service
Oklahoma.</p>
<p>The most exceptional case, as you'll find with other state government functions
as well, is Hawaii. Hawaii has deferred motor vehicle administration to counties,
with the Honolulu CSD or DCS (they are inconsistent!) the largest, alongside
others like the Hawaii County VRL.</p>
<p>So, the point is that DMV is sort of a colloquialism, one that is widely
understood since the most populous states (CA and TX for example) have proper
DMVs. Florida, third most populous state, actually has a DHSMV or FLHSMV
depending on where you look... but their online services portal is branded
MyDMV, even though there is no state agency or division called the DMV. See how
this can be confusing?</p>
<p>Anyway, if you are sitting around on a Saturday morning searching for the name
of every state plus "DMV" like I am, you will notice something else: a lot
of...  suspicious results. guamtax.com is, it turns out, actually the website
of the Guam Department of Revenue and Taxation. dmvflorida.org is not to be
confused with the memorable flhsmv.gov, and especially not with
mydmvportal.flhsmv.gov. You have to put "portal" in the domain name so people
know it's a portal, it's like how "apdonline.com" has "online" in it so you
know that it's a website on the internet. </p>
<p>dmvflorida.org calls itself the "American Safety Council's Guide to the Florida
Department of Motor Vehicles." Now, we have established that the "Florida
Department of Motor Vehicles" does not exist, but the State of Florida itself
seems a little confused on that point, so I'll let it slide. But that brings us
to the American Safety Council, or ASC.</p>
<p>ASC is... It's sort of styled to sound like the National Safety Council (NSC)
or National Sanitation Foundation (NSF), independent nonprofits that publish
standards and guidance. ASC is a different deal. ASC is a for-profit vendor of
training courses. Based on the row of badges on their homepage, ASC wants you
to know not only that they are "Shopper Approved," "Certifiably Excellent (The
Stats To Prove It)," they have won a "5-Star Excellence Award" (from whom not
specified), and that the Orlando Business Journal included their own John
Comly on its 2019 list of "CEOs of the Year."</p>
<p>This is the most impressive credential they have on offer besides accreditation
by IACET, an industry association behind the "continuing education units" used
by many certifications, and which is currently hosting a webinar series on "how
AI is reshaping learning landscapes from curriculum design to compliance." This
does indeed mean that, in the future, your corporate sexual harassment training
will be generated by Vyond Formerly GoAnimate based on a script right out of
ChatGPT. The quality of the content will, surprisingly, not be adversely
affected. "As you can see, this is very important to Your Company. Click Here
[broken link] to read your organization's policy."</p>
<p>In reality, ASC is a popular vendor of driver safety courses that businesses
need their employees to take in order to get an insurance discount. Somewhere
in a drawer I have a "New Mexico Vehicle Operator's Permit," a flimsy paper
credential issued to state employees in recognition of their having completed
an ASC course that consisted mostly of memorizing that "LOS POT" stands for
"line of sight, path of travel." Years later, I am fuzzy on what that phrase
actually means, but expanding the acronym was on the test.</p>
<p>We can all reflect on the fact that the state's vehicle insurance program is
not satisfied with merely possessing the driver's license that the state itself
issues, but instead requires you to pass a shorter and easier exam on actually
driving safely. Or knowing about the line of sight and the path of travel, or
something. I once took a Motorcycle Safety Foundation course that included a
truly incomprehensible diagram of the priorities for scanning points of
conflict at an intersection, a work of such information density that any
motorcyclist attempting to apply it by rote would be entirely through the
intersection and to the next one before completing the checklist. We were,
nonetheless, taught as if we were expected to learn it that way. Driver's
education is the ultimate test of "Adult Learning Theory," a loose set of
principles influential on the design of Adobe Captivate compliance courses, and
the limitations of its ability to actually teach anyone anything.</p>
<p>This is all a tangent, so let's get back to the core. ASC sells safety courses
and... operates dmvflorida.org?</p>
<p>Here's the thing: running DMV websites is a profitable business. Very few
people look for the DMV website because they just wanted to read up on driver's
license endorsements. Almost everyone who searches for "&lt;state name&gt; DMV" is on
the way to spending money: they need to renew their license, or their
registration, or get a driving test, or ideally, a driver's ed course or
traffic school.</p>
<p>The latter are ideal because a huge number of states have privatized them, at
least to some degree. Driver's ed and traffic school are both commonly offered
by competitive for-profit ventures that will split revenue in exchange for
acquiring a customer. I would say that dmvflorida.org is a referral scam, but
it's actually not! it's even better: it's owned by ASC, one of the companies
that competes to offer traffic school courses! It's just a big, vaguely
government-looking funnel into ASC's core consumer product.</p>
<p>In some states, the situation is even better. DMV services are partially
privatized or "agents" can submit paperwork on the behalf of the consumer.
Either of these models allow a website that tops Google results to submit your
driver's license renewal on your behalf... and tack on a "convenience fee" for
doing so. Indeed, Florida allows private third-parties to administer the
written exam for a driver's license, and you know dmvflorida.org offers such an
online exam for just $24.95.</p>
<p>You can, of course, renew your driver's license online directly with the state,
at least in the vast majority of cases. so how does a website that does the
same thing, with the same rates, <em>plus</em> their own fee, compete? SEO. Their best
bet is to outrank the actual state website, grabbing consumers and funneling
them towards profitable offerings before they find the <em>actual</em> DMV website.</p>
<p>There's a whole world of DMV websites that operate in a fascinating nexus of
SEO spam, referral farm, and nearly-fraudulent imitation of official state
websites. This has been going on since, well, I have a reliable source that
claims since 1999: dmv.org.</p>
<p>dmv.org is an incredible artifact of the internet. It contains an <em>enormous</em>
amount of written content, much of it of surprisingly high quality, in an
effort to maintain strong search engine rankings. It used to work: for many
years, dmv.org routinely outranked state agency websites for queries that were
anywhere close to "dmv" or "renew driver's license" or "traffic school." And it
was all in the pursuit of referral and advertising revenue. Take it from them:</p>
<blockquote>
<p>Advertise with DMV.ORG</p>
<p>Partner with one of the most valuable resource for DMV &amp; driving - driven by
85% organic reach that captures 80% of U.S drivers, DMV.ORG helps organize
the driver experience across the spectrum of DMV and automotive- related
information. Want to reach this highly valued audience? </p>
</blockquote>
<p>dmv.org claims to date back to 1999, and I have no reason to doubt them, but
the earliest archived copies I can find are from 2000 and badly broken. By late
2001 the website has been redesigned, and reads "Welcome to the Department of
Motor Vehicles Website Listings." If you follow the call to action and look up
your state, it changes to "The Department of Motor Vehicles Portal on the
Web!"</p>
<p>They should have gone for dmvportal.org for added credibility.</p>
<p>In 2002, dmv.org takes a new form: before doing pretty much anything, it asks
you for your contact information, including an AOL, MSN, or Yahoo screen name.
They promise not to sell your address to third parties but this appears to be a
way to build their own marketing lists. They now prominently advertise vehicle
history reports, giving you a referral link to CarFax.</p>
<p>Over the following months, more banner ads and referral links appear: vehicle
history reports, now by edriver.com, $14.99 or $19.99. Driving record status,
by drivingrecord.org, $19.99. Traffic School Online, available in 8 states,
dmv-traffic-school.com and no price specified. The footer: "DMV.ORG IS
PRIVATELY OPERATED AND MAINTAINED FOR THE BENEFIT OF ITS USERS."</p>
<p>In mid-2003, there's a rebranding. The header now reads "DMV Online Services."
There are even more referral links. Just a month later, another redesign, a
brief transitional stage, before in September 2003, dmv.org achieves the form
familiar to most of us today: A large license-plate themed "DMV.ORG" logotype,
referral links everywhere, map of the US where you can click on your state.
"Rated #1 Site, CBS Early Show."</p>
<p>This year coincides, of course, with rapid adoption of the internet. Suddenly
consumers really are online, and they really are searching for "DMV." And
dmv.org is building a good reputation for itself. A widely syndicated 2002
newspaper article about post-marriage bureaucracy (often appearing in a
Bridal Guide supplement) sends readers to dmv.org for information on updating
their name. <em>The Guardian,</em> of London, points travelers at dmv.org for
information on obtaining a handicap placard while visiting the US.</p>
<p>You also start to see the first signs of trouble. Over the following years, an
increasing number of articles both in print and online refer to dmv.org as if
it is <em>the</em> website of <em>the</em> Department of Motor Vehicles. We cannot totally
blame them for the confusion. First, the internet was relatively new, and
reporters had perhaps not learned to be suspicious of it. Second, states
themselves sometimes fanned the flames. In a 2005 article, the director of
driver services for the Mississippi Department of Transportation tells the
reporter that you can now renew your driver's license online... at dmv.org.</p>
<p>dmv.org was operated by a company called eDriver. It's hard to find much about
them, because they have faded into obscurity and search results are now
dominated by the lawsuit that you probably suspected is coming. The "About Us"
page of the dmv.org of this period is a great bit of copywriting, complete with
dramatic stories, but almost goes out of its way not to name the people
involved. "One of our principals likes to say..."</p>
<p>eDriver must not have been very large, their San Diego office address was a
rented mail box. Whether or not it started out that way is hard to say, but
by 2008 eDriver was a subsidiary of Biz Groups Inc., along with Online Guru
Inc and Find My Specialist Inc. These corporate names all have intense "SEO
spam" energy, and they seem to have almost jointly operated dmv.org through
a constellation of closely interlinked websites. In 2008, eDriver owned dmv.org
but didn't even run it: they contracted Online Guru to manage the website.</p>
<p>Biz Groups Inc was owned by brothers Raj and Ravi Lahoti. Along with third
brother David, the Lahotis were '00s domain name magnates. They often landed on
the receiving end of UDRP complaints, ICANN's process for resolving disputes
over the rightful ownership of domain names. Well, they were in the business:
David Lahoti owns UDRP-tracking website udrpsearch.com to this day.</p>
<p>Their whole deal was, essentially, speculating on domain names. Some of them
weren't big hits. An article on a dispute between the MIT Scratch project and
the Lahotis (as owners of scratch.org) reads "Ravi updated the site at
Scratch.org recently to includes news articles and videos with the word scratch
in them. It also has a notice that the domain was registered in 1998 and
includes the dictionary definition of scratch."</p>
<p>Others were more successful. In 2011, Raj Lahoti was interviewed by a Korean
startup accelerator called beSuccess:</p>
<blockquote>
<p>My older brother Ravi was the main inspiration behind starting OnlineGURU.
Ravi owned many amazing domain names and although he didn't build a website
on every one of his domains, he DID build a small website at www.DMV.org and
this website started doing well. Well enough that he saw an opportunity to do
something bigger with it and turn it into a bigger business.</p>
</blockquote>
<p>And he is clear on how the strategy evolved to focus on SEO farming:</p>
<blockquote>
<p>Search Engine Marketing and Search Engine Optimization has definitely been
most effective in my overall marketing strategy. The beautiful thing about
search engines is that you can target users who are looking for EXACTLY what
you offer at the EXACT moment they are looking for it. Google Adwords has so
many tools, such as the Google Keyword Tool where you can learn what people
are searching for and how many people are searching the same thing. This has
allowed me to learn about WHAT the world wants and gives me ideas on how I
can provide solutions to help people with what they are looking for.</p>
</blockquote>
<p>Also, San Diego Business Journal named Raj Lahoti "among the finalists of the
publication's Most Admired CEO award" in 2011. So if he ever meets John Comly,
they'll have something to talk about.</p>
<p>The thing is, the relationship between dmv.org and actual state motor vehicle
administrators became blurrier over time... perhaps not coincidentally, just as
dmv.org ascended to a top-ranking result across a huge range of Google queries.
It really was a business built entirely on search engine ranking, and they
seemed to achieve that ranking in part through a huge amount of content (that
is <em>distinctly</em> a cut above the nearly incoherent SEO farms you see today), but
also in part through feeding consumer confusion between them and state
agencies. I personally remember ending up on dmv.org when looking for the
actual DMV's website, and that was probably when I was trying to get a driver's
license to begin with. It was getting a bit of a scammy reputation, actual DMVs
were sometimes trying to steer people away from it, and in 2007 they were sued.</p>
<p>A group of website operators in basically the same industry, TrafficSchool.com
Inc and Driver's Ed Direct, LLC, filed a false advertising suit against the
Online Guru family of companies. They claimed not that dmv.org was fraudulent,
but that it unfairly benefited from pretending to be an official website.</p>
<p>Their claim must have seemed credible. At the beginning of 2008, before the
lawsuit made it very far,  dmv.org's tagline changed from "No need to stand IN
LINE. Your DMV guide is now ON LINE!" to "Your unofficial guide to the DMV."
This became the largest indication that dmv.org was not an official website,
supplementing the small, grey text that had been present in the footer for
years.</p>
<p>The judge was not satisfied.</p>
<p>See, the outcome of the lawsuit was sort of funny. The court agreed that
dmv.org was engaging in false advertising under the Lanham Act, but then found
that the plaintiffs were doing basically the same thing, leaving them with
"unclean hands." Incidentally, they would appeal and the appeals court would
disagree on some details of the "unclean hands" finding, but the gist of the
lower court's ruling held: the plaintiffs would not receive damages, since
they had been pursuing the same strategy, but the court did issue an injunction
requiring dmv.org to add a splash screen clearly stating that it was not an
official website.</p>
<p>The lawsuit documents are actually a great read. The plaintiffs provided the
court with a huge list of examples of confusion, including highlights like a
Washington State Trooper emailing dmv.org requesting a DUI suspect's Oregon
driving records. dmv.org admitted to the court that they received emails like
this on "a daily basis," many of them being people attempting to comply with
mandatory DUI reporting laws by reporting their recent DUI arrest... to Online
Guru.</p>
<p>The court noted the changes made to dmv.org in early 2008, including the
"Unofficial" heading and changing headings from, for example, "California DMV"
to "California DMV Info." But those weren't sufficient: going forward, users
would have to click "acknowledge" on a page warning them.</p>
<p>It is amusing, of course, that the SEO industry of the time interpreted the
injunction mainly in the SEO context. This was, after all, a website that lived
and died by Google rankings, part of a huge industry of similar websites. Eric
Goldman's <a href="https://blog.ericgoldman.org/archives/2008/09/dmvorg_hit_with.htm">Technology and Marketing Law
Blog</a> wrote
that "My hypothesis is that such an acknowledgment page wrecks DMV.org’s
search engine indexing by preventing the robots from seeing the page content."</p>
<p>The takeaway:</p>
<blockquote>
<p>This suggests a possible lesson to learn from this case. The defendants had a
great domain name (DMV.org) that they managed to build nicely, but they may
have too aggressive about stoking consumer expectations about their
affiliation with the government.</p>
</blockquote>
<p>It's wild that "get a good domain name and pack it with referral links" used to
be a substantial portion of the internet economy. Good thing nothing that vapid
survives today! Speaking of today, what happened to dmv.org?</p>
<p>Well, the court order softened over time, and the acknowledgment page
ultimately went away. It was replaced by a large, top-of-page banner, almost
comically reminiscent of those appearing on cigarettes. "DMV.ORG IS A
<strong>PRIVATELY OWNED</strong> WEBSITE THAT IS <strong>NOT</strong> OWNED OR OPERATED BY ANY STATE
GOVERNMENT AGENCY." Below that, the license plate dmv.org logotype, same as
ever.</p>
<p>Besides, they reformed. At
<a href="https://sustainablebrands.com/read/organizational-change/paying-it-forward-how-dmv-org-is-taking-sustainability-to-the-streets">sustainablebrands.com</a>
we read:</p>
<blockquote>
<p>Over our 10-year history, DMV.org’s mission has shifted entirely from profit
to purpose. We not only want to bring value to our users by making their DMV
experience easier, we ultimately want to reduce transportation-related
deaths, encourage eco-friendly driving habits, and influence other businesses
to reduce their carbon footprints and become stewards of change themselves.</p>
</blockquote>
<p>This press-release-turned-article says that they painted "the company’s human
values on our wall, to remind ourselves every day what we’re here for and why"
and that, curiously, dmv.org "potentially aim[s] to" "eliminate Styrofoam from
local eateries." The whole thing is such gross greenwashing, bordering on
incoherent, that I might accuse it of being AI-generated were it not a decade
old.</p>
<p>dmv.org lived by Google and, it seems, it will die by Google. Several SEO blogs
report that, sometime in 2019, Google seems to have applied aggressive manual
adjustments to a list of government-agency-like domain names that include
irs.com (its whole own story) and dmv.org. Their search traffic almost
instantaneously dropped by 80%.</p>
<p>dmv.org is still going today, but I'm not sure that it's relevant any more. I
tried a scattering of Google queries like "new mexico driver's license" and
"traffic school," the kind of thing where dmv.org used to win the top five
results, and they weren't even on the first page. Online Guru still operates
dmv.org, and "dmv.org is NOT your state agency" might as well be the new
tagline. Phrases like that one constantly appear in headings and sidebars.</p>
<p>They advertise auto insurance, and will sell you online practice tests for $10.
Curiously, when I look up how to renew my driver's license in New Mexico,
dmv.org sends me to the actual NM MVD website. That's sort of a funny twist,
because New Mexico does indeed allow renewal through private service providers
that are permitted to charge a service fee. I don't think dmv.org makes enough
money to manage compliance with all these state programs, though, so it's actually
returned to its roots, in a way: just a directory of links to state websites.</p>
<p>Also, there's a form you can fill out to become a contributor! <em>Computers Are
Bad</em> has been fun, but I'm joining the big leagues. Now I write for dmv.org.</p> ]]></description>
	</item>

 	<item>
		<title>2024-06-02 consumer electronics control</title>
		<pubDate>Sun, 02 Jun 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-06-02-consumer-electronics-control.html</link>
		<guid>https://computer.rip/2024-06-02-consumer-electronics-control.html</guid>
		<description><![CDATA[ <p>In a previous episode, I discussed <a href="https://computer.rip/2024-01-21-multi-channel-audio-part-1.html">audio
transports</a>
and mention that they have become a much less important part of the modern home
theater landscape. One reason is the broad decline of the component system:
most consumers aren't buying a television, home theater receiver, several
playback devices, and speakers. Instead, they use a television and perhaps
(hopefully!) a soundbar system, which often supports wireless satellites if
there are satellites at all. The second reason for the decline of audio
transports is easy to see when we examine these soundbar systems: most connect
to the television by HDMI.</p>
<p>This is surprising if you consider that soundbars are neither sources nor sinks
for video. But it's not so surprising if you consider the long-term arc of HDMI
[1], towards being a general-purpose consumer AV interconnect. HDMI has become
the USB of home theater, and I mean that as a compliment and an insult. So,
like USB, HDMI comes in a confusing array of versions with various mandatory,
optional, and extension features. The capabilities of HDMI vary by the devices
on the ends, and in an increasing number of cases, even by the specific port
you use on the device.</p>
<p>HDMI actually comes to this degree of complexity more honestly than USB. USB
started out as a fairly pure and simple serial link, and then more use-cases
were piled on, culminating in the marriage of two completely different
interconnects (USB and Thunderbolt) in one physical connector. HDMI has always
been a Frankenstein creation. At its very core, HDMI is "DVI plus some other
things with a smaller connector."</p>
<p>DVI, or really its precursors that established the actual video format, was
intended to be a fairly straightforward step from the analog VGA. As a result,
the logical design of DVI (and thus HDMI) video signals are pretty much the
same as the signals that have been used to drive CRT monitors for almost as
long as they've existed. There are four TMDS data lines on an HDMI cable,
each a differential pair with its own dedicated shield. The four allow for
three color signals (which can be used for more than one color space) and a
clock. Two data pins plus a shield, times four, means 12 pins. That's <em>most,</em>
but not all, of the 19 pins on an HDMI connector.</p>
<p>A couple of other pins are used for an I2C connection, to allow a video source
to query the display for its specifications. A couple more are used for the
audio return channel or ethernet (you can't do both at the same time) feature
of HDMI. There's a 5V and a general signal ground. And then there's the CEC
pin.</p>
<p>The fact that CEC merits its own special pin suggests that it is an old part
of the standard, and indeed it is. CEC was planned from the very beginning,
although it didn't get a full specification as part of the HDMI standard until
HDMI 1.2a. Indeed, CEC is older than HDMI, dating to at least 1998, when it
was standardized as part of SCART. But let's take a step back and consider the
application.</p>
<p>One of the factors in the decline of component stereo systems is the remote
control. In the era of vinyl, when you had to get off the couch to start a
record anyway, remote controls weren't such an important part of the stereo
market. The television changed everything about the way consumers interact
with AV equipment: now we all stay on the couch.</p>
<p>I think we all know the problem, because we all lived through it: the
proliferation of remotes. When your TV, your VCR, and your home theater
receiver all have remote controls, you end up carrying around a bundle
of cheap plastic. You will inevitably drop them, and the battery cover
will pop off, and the batteries will go under the couch. This was one of
the principal struggles faced by the American home for decades.</p>
<p>There are, of course, promised solutions on the market. Many VCR remotes had
the ability to control a TV, and often the reverse as well. If you bought your
TV and VCR from the same manufacturer this worked. If you didn't, it might not,
or at least setup will be more complex. This is because the protocols used by
IR remotes are surprisingly unstandardized. Surprisingly unstandardized in that
curious way where there are few enough IR transceiver ICs that a lot of devices
actually are compatible (consider the ubiquitous Philips protocol), but no one
documents it and detailed button codes often vary in small and annoying ways.</p>
<p>So we got the universal remote. These remotes, often thrown in with home
theater receivers as a perk, have some combination of a database of remote
protocols pre-reverse-engineered by the manufacturer and a "learn" mode in
which they can record unknown protocols for naive playback. Results were...
variable. I heard that some of the expensive universal remotes like Logitech
Harmony (dead) and Control4 (still around) were actually pretty good, but
they required some emphasis on the word "expensive." Universal remotes were
sort of a mixed bag, but they were fiddly enough to keep working that consumer
adoption doesn't seem to have been high.</p>
<p>So, another approach came to us from the French. In the Europe of the 1970s,
there was not yet a widely accepted norm for connecting a video source to a TV
(besides RF modulation). France addressed the matter by legislation, mandating
SCART in 1980. Over the following years, SCART became a norm in much of Europe.
SCART is a bit of an oddity to Americans, as it never achieved a footprint on
this continent. That's perhaps a bit disappointing, because SCART was ahead of
its time.</p>
<p>For example, much like HDMI, SCART carried bidirectional audio. It supported
multiple video formats over one cable. Most notably, though, SCART was designed
for daisy chaining. Some simple aspects of the SCART design provided a basic
form of video routing, where the TV could bidirectionally exchange video
signals with one of several devices in a chain. The idea of daisy-chainable
video interconnects continuously reappears but seldom finds much success, so
I'd call this one of the more notable aspects of SCART.</p>
<p>That's not why we're here, though. Another interesting aspect of SCART was its
communications channel between devices. The core SCART specification included a
basic system of voltage signaling to indicate which device was active, but in
1998 CENELEC EN 50157-1 was standardized as a flexible serial link between
devices over the SCART cable. Most often called AV.link, this channel could be
used for video format negotiation, but also promised a solution to multiplying
remotes: the AV.link channel can transmit remote control commands between
devices. For example, your TV remote can have play/pause buttons, and when you
push them the TV can send AV.link play/pause commands to whichever video source
is active.</p>
<p>AV.link is a very simple design. A one wire (plus ground) serial bus operates
at a slow (but durable) 333bps with collision detection. Devices are identified
by four-bit addresses chosen at random (but checked for collision). Messages
have a simple format: a one-byte header with the sending and receiving
addresses, a one-byte opcode, and then whatever bytes are expected as
parameters to the opcode.</p>
<p>AV.link is one of those standards that never quite got its branding together.
Unlike, say, USB, where a consistent trademark identity is used, AV.link goes
by different names from different vendors. Wikipedia offers the names
nexTViewLink (horrible), SmartLink (mediocre), Q-Link (lazy), and EasyLink
(mediocre again). One wonders if consumers were confused by these different
vendor brands for the same thing, it's not a situation that happens very often
with consumer interconnects.</p>
<p>When HDMI was developed, the provision of a pin for AV.link was pretty much
copied over from SCART. Originally, the functionality wasn't even really
specified, and just assumed to be similar to SCART. Later HDMI versions
included a much more complete description of CEC as a supplement. Hardware
support for CEC is mandated for devices like TVs as part of the HDMI
certification process, but curiously, software support isn't really included.
As a result, it is very common, but not universal, that TVs fully support CEC.
Other AV devices like home theater receivers almost universally have CEC
support. Computers almost universally do not, as cost and licensing
considerations mean that GPUs do not provide a CEC transceiver.</p>
<p>Inconsistent implementations are not the only way that CEC is a little sketchy.
Remember how different vendors referred to SCART AV.link by different names?
CEC has the same problem. I won't bother with the whole list, but the names
you're more likely to have seen include Samsung Anynet+, LG SimpLink, and...
well, Philips EasyLink is still with us. In practice, a lot of people seem to
ignore these names, and CEC is a lot more common than Anynet+ when discussing
Samsung TVs. That doesn't stop Samsung from pushing their own branding in their
menus and port labeling, though.</p>
<p>Because CEC inherits the AV.link features designed for SCART, it has a
surprisingly rich featureset. For example, if you have an HDMI switch with real
CEC support (these don't seem to be that common!) and a TV with software
support, the TV can discover the topology of connected devices and remote
control the switch to use the switch inputs as an extension of its own input
selection menu.</p>
<p>Most CEC features are more prosaic, though. Considering the list of high-level
features in the specification, "One Touch Play" means that a device can
indicate that it has video to show (causing a TV to turn on and select that
input) while "System Standby" means that a device being turned off can tell the
other devices on the bus to turn off as well. "One Touch Record," "Deck
Control," "Tuner Control," "Device Menu Control," and "System Audio Control"
are all variations on devices forwarding simple remote commands (play, pause,
up, down, etc) to other devices that might care about them more. For example,
when you use a TV with a stereo receiver or soundbar, it should forward volume
up/down commands from the remote to the audio device via System Audio Control.</p>
<p>Considering the decline of component systems, there are basically two common
scenarios where CEC is used today. These are really the same scenario in a
lot of ways, but they vary in the details.</p>
<ul>
<li>The connection of a television to a home theater receiver. In these kinds of
configurations, the home theater receiver is often used for at least some video
switching. That means that the receiver is sending video to the TV, receiving
audio from the TV via audio return channel (ARC), and both are exchanging commands.
For example, the receiver can use CEC to turn on the TV when a video input is
selected. Conversely, the TV can turn on the receiver when it is turned on.</li>
<li>The connection of a television to a soundbar. In this case, ARC is used to
send audio from the TV to the soundbar. There really is no video involved in this
scenario, so in a sense the HDMI cable is mostly unused. CEC is used by the TV to
control the soundbar. Because soundbars don't often have a remote that the user
cares to keep around, this control tends to be mostly unidirectional, used by the
TV to turn the soundbar on and off.</li>
</ul>
<p>It is interesting, isn't it, that an interconnect with four very-high-speed
serial video channels is often put into use in a scenario where those channels
are useless. Instead, the much lower-rate ARC and CEC channels are the
important ones. Well, think about USB as a power connector... these things
happen.</p>
<p>CEC could be used in much more complex scenarios. For example, if you had a DVR
connected to your TV via CEC, you could browse the electronic program guide
(EPG) on your TV and choose a program to record. This would cause the TV to use
CEC Timer Programming to send the program details from the TV EPG to the DVR to
schedule the recording. How widely was this ever used? I don't know, I suspect
not very, because these days DVRs are almost invariably provided by the cable
or satellite company, who expect you to use the DVR's EPG rather than your TVs
anyway.</p>
<p>This is actually one of the scenarios where you see ARC used for reasons other
than synchronizing control of an audio output: set-top boxes (STBs). Media
companies that distribute STBs, mostly cable and satellite operators, tend to
be in a bit of a war to own your television watching experience. They face
stiff competition from "Smart TVs." I have a suspicion that the complete
proliferation of smart TVs is largely an artifact of the television
manufacturers trying to win advertising surface area away from the STB
manufacturers, who have traditionally held most of it via the EPG.</p>
<p>As some evidence of this fight, consider the case of Xfinity Xumo (formerly
Flex), the compact STB that Xfinity offers to its internet customers for free.
Since it's advertised to people who don't necessarily have any TV service from
Xfinity, it's not really a conventional STB. It's more of a
slightly-weird-but-free Roku or Amazon Fire Stick. It doesn't really offer
anything that your TV doesn't already, but unlike your TV, it's controlled by
Comcast. This gives them the opportunity to upsell you on IPTV services, but
Comcast never seems to have pursued this route that far. Mostly it gives them
the opportunity to advertise to you, and to grab some partner revenue from
various streaming apps.</p>
<p>Anyway, that was a bit of a digression. The point is that Comcast and Dish
Network and all of their compatriots don't want you using your TV, they want
you using your STB. So they give you a big chunky remote ("With Voice
Control!") and the STB attempts to use CEC to control the TV so you never have
to touch its small, svelte remote ("With Voice Control!") and split their
sponsored content revenue with LG.</p>
<p>That's an interesting detail of this whole landscape, isn't it? CEC was
developed as a solution to a technical problem: people had multiple devices,
and hauling around multiple remotes was frustrating. Over the decades since, it
has evolved into a strategy to address a business problem: everyone that sells
you AV equipment prefers that you passionately navigate their on-screen menus
while completely forgetting about those of your other components.</p>
<p>That's pretty much what's happening with the audio devices as well. TV
manufacturers want to capture as much of your entertainment attention and
budget as possible, so ideally they sell you a TV and their matching soundbar
system (which can be fairly inexpensive since it is closely coupled to the TV
and needs very little of its own control logic). CEC here is an under-the-hood
implementation detail, something that happens behind the scenes to make your
soundbar do the few things it does.</p>
<p>Say you're a higher-end customer, though, with a home theater receiver. The AV
receiver industry has been surprisingly unambitious about capturing Platform
Revenue, probably because soundbars have pretty much eliminated everything but
higher-end, "audiophile"-focused brands. These companies either lack the
technical resources to develop a good Entertainment Platform or don't think
their customers will respond well to yet another remote with a Pluto TV button.
I would like to say it's mostly the latter, but given my experience with the
on-screen design and mobile apps of several leading AV receiver manufacturers,
I suspect it's mostly the former.</p>
<p>So CEC functions perhaps the most as it was originally intended: you can mainly
interact with your TV, and CEC carries control messages to the receiver as
needed so that you don't need to find its remote to select the right input.
Conceptually you can even use the TV to control non-video functions. For
example, my particular combination of a Samsung TV and Yamaha receiver
implements CEC completely enough that I can turn on the receiver, select the
turntable preamp input, and control the volume via the TV if I want to. Then I
still have to get up to actually put a record onto the turntable, and now the
TV is just on the whole time, so this isn't that appealing in practice. I am
still rummaging fro the receiver's own remote, that or using its terrible
Android app.</p>
<p>In the STB scenario, something like Xfinity X1 or Dish Hopper, we have an
inversion of control: the only remote you'll need, they hope, is the STB's
remote. It will remote control the TV via CEC as needed. This inevitably sets
up a power struggle where your Smart TV gets lonely and wants attention. I am
mostly kidding about this emotional interpretation of the situation, but
obviously the TV manufacturer does have an incentive to distract your attention
from the STB, which probably has something to do with the tendency of Smart TVs
to pop up a lot of on-screen chrome whenever you turn them on.</p>
<p>The coolest thing about CEC, in my mind, is that unlike HDMI it is multi-drop.
That is, when you connect a bunch of HDMI sources to a multi-input TV or
receiver or another switching device, they can all be connected to the same
unified CEC bus. That means that HDMI devices can communicate with each other
via CEC even when there's no active video or audio connection.</p>
<p>CEC even has a fairly complex addressing mechanism to take advantage. CEC
physical addresses are assigned based on bus topology, and a mapping protocol
is used to advertise a correspondence between new physical addresses and
logical addresses. Logical addresses, the same 4-bit addresses from AV.link,
are assigned based on capabilities. Typically logical address 0 will be the TV,
1 and 2 will be recorders, 3 will be a tuner, 4 a playback device, 5 a home
stereo receiver. You can have a fairly large component setup where everything
is controllable by sending CEC messages to standard logical addresses.</p>
<p>And other aspects of CEC are designed to accommodate these kinds of more complex
networks. For example, when the user selects a device to watch on their TV, the
TV can send a "Set Stream Path" message (opcode 0x86). The parameter on this
message is the physical CEC address of the desired device, and any CEC switches
in the path are expected to see the message and select the appropriate input to
form a path from the selected device to the TV. It's a little bit of
centrally-controlled circuit switching right in your entertainment center.
Neat!</p>
<p>You can even do broadcast messaging across the entire CEC topology. TVs often
use this to discover what devices they're connected to, saving the user some
menu setup.  That's about the only time you'd notice it, though: like CEC's
other more advanced capabilities, routing and multi-device messaging are rarely
used outside of its very simplest application.</p>
<p>I want to pass some sort of tidy moral judgment, but CEC is a hard case. It's
kind of a mixed bag. The more basic functionality tends to work well and adds
convenience. The more complex functionality tends to either not work or be
buried deeply enough in configuration menus that no one uses it. It inevitably
leads to some weird, inelegant behavior. My husband will put a cab ride video
on the TV and then Spotify Cast to the receiver. But then what if you want to
listen to the video audio? Easiest way to get the receiver switched back to the
TV audio is, of course, to turn the TV off and on again. When you turn it on,
it uses CEC "One Touch Play" to signal the receiver to select it again.
The particular convergence of technologies here leads to a strange tic, sort
of a superstitious behavior, that works fine but feels bad.</p>
<p>If you're a weirdo like me, you use your TV heavily as a monitor for a
computer. You might find the gap here rather conspicuous: when I wiggle the
mouse to wake the computer up, the TV doesn't turn on. HDMI keeps gaining
features, video games are a big driver of high-end PC and television sales,
there is an inevitable convergence happening between "monitor" and "TV," and
between "video source" and "computer." But the computer video industry is,
well, a little slow to catch on.</p>
<p>You might remember that it took an awkwardly long time for PC GPUs to have
consistent support for HDMI audio, and then it was still weird and sketchy for
a good few years. Well, we haven't even quite made it to that point on the CEC
front. I don't think <em>any</em> conventional PCs have CEC transceivers. The
solution, if you are mad enough to want one, is a USB CEC adapter. They're
basically passthrough devices for HDMI, they just tap the CEC pin and hook it
up to a UART. Not many companies make them but they're cheap enough. Software
support is... minimal, but it'll let Kodi turn your TV on.</p>
<p>It's fun to think about, though. You know how CEC is multi-drop? You could hook
up multiple computers to an HDMI switch and they could talk to each other with
CEC. You could use some vendor-specific opcodes to convey IP. You could log
onto the internet over HDMI, at 333bps. You could put OpenSC over IP over HDMI
CEC and turn your lights on via your stereo receiver. What a dream! I was going
to say you could do DMX-512 over CEC but actually at CEC's slow speed the
register-broadcast model of DMX would become a pretty significant problem.</p>
<p>You could also log onto the internet over HDMI at 100Mbps, but that's using
different pins, your GPU definitely doesn't support it, and I don't even know
of a way to do HDMI Ethernet from a PC. CEC may be a bit of an awkward cousin
but at least it's more popular than HDMI Ethernet.</p>
<p>[1] pun not intended</p> ]]></description>
	</item>

 	<item>
		<title>2024-05-25 grc spinrite</title>
		<pubDate>Sat, 25 May 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-05-25-grc-spinrite.html</link>
		<guid>https://computer.rip/2024-05-25-grc-spinrite.html</guid>
		<description><![CDATA[ <p>I feel like I used to spend an inordinate amount of time dealing with suspect
hard drives. I mean, like, back in high school. These days I almost never do,
or on the occasion that I have storage trouble, it's a drive that has
completely stopped responding at all and there's little to do besides replacing
it. One time I had two NVMe drives in two different machines do this to me the
same week. Bad luck or quantum phenomenon, who knows.</p>
<p>What accounts for the paucity of "HDD recovery" in my adult years? Well, for
one, no doubt HDD technology has improved over time and modern drives are
simply more reliable. The well-aged HDDs I have running without trouble in
multiple machines right now support this theory. But probably a bigger factor
is my buying habits: back in high school I was probably getting most of the
HDDs I used second-hand from the Free Geek thrift store. They were coming
pre-populated with problems for my convenience.</p>
<p>Besides, the whole storage industry has changed. What's probably more
surprising about my situation is how many "spinning rust" HDDs I still own.
Conventional magnetic storage only really makes sense in volume. These days I
would call an 8TB HDD a small one. The drives that get physical abuse, say in
laptops, are all solid state. And solid state drives... while there is no doubt
performance degradation over their lifetimes, failure modes tend to be
all-or-nothing.</p>
<p>I was thinking about all of this as I ruminated on one of the "holy grail"
tools of the late '00s: SpinRite, by Gibson Research Corporation.</p>
<p>The notion that HDDs aren't losing data like they used to is supported by the
dearth of data recovery tools on the modern shareware market. Well, maybe
that's more symptomatic of the complete hollowing out of the independent
software industry by the interests of capitalism, but let's try to dwell on the
positive. Some SEO-spam blog post titled "Best data recovery software of 2024"
still offers some classic software names like "UnDeleteMyFiles Pro," but some
items on the list are just backup tools, and options like Piriform Recuva and
the open-source PhotoRec still rank prominently... as they did when I was in
high school and my ongoing affection for Linkin Park was less embarrassing [1].</p>
<p>Back in The Day, freeware, shareware, and commercial (payware?) data recovery
software proliferated. It was advertised in the back of magazines, the sidebar
banner ads of websites, and even appeared in the electronics department of
Fred Meyer's. You also saw a lot of advertisements for services that could
perform more intensive methods, like swapping an HDD's controller for one from
another unit of the same model. These are all still around today, just a whole
lot less prominent. Have you ever seen an Instagram ad for UnDeleteMyFiles Pro?</p>
<p>First, we should talk a bit about the idea of data recovery in general. There
are essentially two distinct fields that we might call "data recovery":
consumers or business users trying to recover their Important Files (say,
accounting spreadsheets) from damaged or failed devices, and forensic analysts
trying to recover Important Files (say, the <em>other</em> accounting spreadsheets)
that have been deleted.</p>
<p>There is naturally some overlap between these two ventures. Consumers sometimes
accidentally delete their Important Files and want them back. Suspects sometimes
intentionally damage storage devices to complicate forensics. But the two
different fields use rather different techniques.</p>
<p>Let's start by examining forensics, both to set up contrast to consumer data
recovery and because I know a lot more about it. One of the quintessential
techniques of file system forensics is "file carving." A file carving tool
examines an arbitrary sequence of bytes (say, from a disk image) and looks for
the telltale signs of known file formats. For example, most common file formats
have a fixed prefix of some kind. ZIP files start with 0x504B0304, the
beginning of which is the ASCII "PK" for Phil Katz who designed the format.
Some formats also have a fixed trailer, but many more have structure that can
be used to infer the location of the end of the file. For example, in ZIP files
the main header structure, the "central directory," is actually a trailer found
at the end of the file.</p>
<p>If you can find the beginning and end of a file, and it's stored sequentially,
you've now got the whole file. When the file is fragmented in the byte stream
(commonly the case with disk images), the problem is a little tougher, but
still you can find a lot of value. A surprising number of files <em>are</em> stored
sequentially because they are small, some filetypes have internal structure
that can be used to infer related blocks and their order, and even finding a
single block of a file can be useful if it happens to contain a spreadsheet row
starting "facilitating payments to foreign officials" or, I don't know,
"Fiat@".</p>
<p>You end up doing this kind of thing a lot because of a detail of file systems
that all of my readers probably know. It's often articulated as something like
"when you delete a file, it's not deleted, just marked as having been deleted."
That's not exactly wrong but it's also an oversimplification in a way that
makes it more difficult to understand <em>why</em> that is the case. There's a whole
level of indirection due to block allocation, updating the bitmap on every file
delete is a relatively time-consuming process that offers little value, actually
overwriting blocks would be even more time consuming with even less value, etc.
Read Brian Carrier for the whole story.</p>
<p>Actually, screw Brian Carrier, I've written before about the adjacent topic of
<a href="https://computer.rip/2021-07-26-rip-those-bits-to-shreds.html">secure erasure of computer
media</a>.</p>
<p>My point is this: these forensic methods are performed on a fully functional
storage device (or more likely an image of one), where "recovery" is necessary
and possible because of the design of the file system. The storage device, as
hardware, is not all that involved. Well, that's <em>really</em> an
oversimplification, and points to an important consideration in modern data
recovery: storage devices have gotten tremendously more complex, and that's
especially true of SSDs.</p>
<p>Even HDDs tend to have their own thoughts and feelings. They can have a great
deal of internal logic dedicated to maintaining the disk surface, optimizing
performance, working around physical defects on the surface, caching,
encryption, etc. Pretty much all of this is proprietary to the manufacturer,
undocumented, and largely a mystery to the person performing recovery. Thinking
of the device as a "sequence of bytes" throws out a lot of what's really going
on, but it's a necessary compromise.</p>
<p>SSDs have gone even further. Flash storage is less durable than magnetic
storage but also more flexible. It requires new optimizations to maximize life
and facilitates optimizations for access time and speed. Some models of SSDs
vary from each other only by their software configuration (this has long been
suspected of some HDDs as well, but I have no particular insight into Western
Digital color coding). Even worse for the forensic analyst, the TRIM command
creates a whole new level of active management by the storage device: SSDs know
which blocks are in use, allowing them to constantly remap blocks on the fly.
It is impossible, without hardware reverse engineering techniques, to produce a
true image of an SSD. You are always working with a "view" of the SSD mediated
by its firmware.</p>
<p>So let's compare and contrast forensic analysis to consumer data recovery. The
problem for most consumers is sort of the opposite: they didn't delete the
file. If they could get the sequence of bytes off the storage device, they
could just access the file through the file system. The problem is that the
storage device is refusing to produce bytes at all, or it's producing the wrong
ones.</p>
<p>Techniques like file carving are not entirely irrelevant to consumer data
recovery because it's common for storage devices to fail only partially. There
are different ways of referring to the physical geometry of HDDs, and besides,
modern storage devices (HDDs and SSDs alike) abstract away their true geometry.
Different file systems also use different terminology for their own internal
system of mapping portions of the drive to logical objects.  So while you'll
find people say things like "bad cluster" and "bad sector," I'm just going to
talk about blocks. The block is the smallest elementary unit by which your file
system interacts with the device. The size of a block is typically 512B for
smaller devices and 4k for larger devices.</p>
<p>A common failure mode for storage devices (although, it seems, not so much
today) is the loss of a specific block: the platter is damaged, or some of the
flash silicon fails, and a specific spot just won't read any more. The storage
device can, and likely will, paper over this problem by moving the block to a
different area in the storage medium. But, in the process, the contents of the
block are probably lost. The new location just contains... whatever was there
before [2]. Sometimes the bad block is in the middle of a file, and that
sucks for that file. Sometimes the bad block is in the middle of a file system
structure like the allocation table, and that sucks for all of the files.</p>
<p>More complicated file systems tend to incorporate precautionary measures
against this kind of thing, so the blast radius is mostly limited to single
files. For example, NTFS keeps a second copy of the allocation table as a
backup. Journaling can also provide a second source of allocation data when
the table is damaged.</p>
<p>Simpler file systems, like the venerable FAT, don't have any of these tricks.
They are, after all, old and simple. But old age and simplicity gives FAT a
"lowest common denominator" status that sees it widely used on removable
devices. PhotoRec, while oriented towards the consumer data recovery
application, is actually a file carving tool. It's no coincidence that it's
called PhotoRec. Removable flash devices like SD cards have simple controllers
and host simple file systems. They are, as a result, some of the most
vulnerable devices to block failures that render intact files undiscoverable.</p>
<p>What about the cases where file <em>isn't</em> intact, though? Where the block
that has become damaged is part of the file that we want? What about cases
where a damaged head leaves an HDD unable to read an entire surface?</p>
<p>Well, the news isn't that great. Despite this being one of the most common
types of consumer storage failure for a decade or more, and despite the
enormous inventory of software that promises to help, your options are limited.
A <em>lot</em> of the techniques that software packages used in these situations lack
supporting research or are outright suspect. Let's start on solid ground,
though, with the most obvious and probably safest option.</p>
<p>One of the problems you quickly encounter when working with a damaged storage
device is the file system and operating system. File systems don't like damaged
storage devices, and operating systems don't like file systems that refuse to
give up a file they say exists. So you try to copy files off of the bad device
and onto a good one using your daily-driver file browser, and it hits a block
that won't read and gets stuck. Maybe it hangs almost indefinitely, maybe you
get an obscure error and the copy operation stops. Your software is working
against you.</p>
<p>One of the best options for data recovery from suspect devices is an
open-source tool called ddrescue. ddrescue is very simple and substantially
similar to dd. It has one critical trick up its sleeve: when reading a block
fails, ddrescue retries a limited number of times and then moves on. With that
little adaptation, you can recover all of the working blocks from a device and
so likely recover all of the files but a few.</p>
<p>Besides, just retrying a few times has value. Especially on magnetic devices,
the result of reading the surface can be influenced by small perturbances.  An
unreadable sector might be readable every once in a while. This doesn't seem to
happen as much with SSDs due to the dynamics of flash storage and preemptive
correction of weak or ambiguous values, but I'm sure it still happens every once
in a while.</p>
<p>At the end of the day, though, this method still means accepting the loss of
some data. Losing some data is better than losing all of it, but it might not
be good enough. Isn't there anything we can do?</p>
<p>HDDs used to be different. For one, they used to be bigger. But there's more to
it than that. Older hard drives used stepper motors to position the head stack,
and so head positioning was absolute but subject to some mechanical error.
Although this was rarely the case on the consumer market, early hard drives
were sometimes sold entirely uninitialized, without the timing marks the
controller used to determine sector positions. You had to use a special tool
to get the drive to write them [3]. It was common for older drives to come
with a report (often printed on the label) of known bad sectors to be kept
in mind when formatting.</p>
<p>We now live in a different era. Head stacks are positioned by a magnetic coil
based on servo feedback from the read head; mechanical error is virtually
absent and positioning is no longer absolute but relative to the cylinder being
read. Extensive low-level formatting is required but is handled completely
internally by the controller. Controllers passively detect bad blocks and
reallocate around them. Honestly, there's just not a lot you can do. There are
too many levels of abstraction between even the ATA interface and the actual
storage to do anything meaningful at the level of the magnetic surface. And all
of this was pretty much true in the late '00s, even before SSDs took over.</p>
<p>So what about SpinRite?</p>
<p>SpinRite dates back to 1987 and is apparently still under development by its
creator Steve Gibson. Gibson is an interesting figure, one of the "Tech
Personalities" that contemporary media no longer creates (insert comment about
decay in the interest of capitalism here). Think Robert Cringely or Leo Laporte,
with whom Gibson happens to cohost a podcast. In my mind, Gibson is perhaps
most notable for his work as an early security researcher, which had its misses
but also had its hits. Through the whole thing he's run Gibson Research
Corporation. GRC offers a variety of one-off web services, like a password
generator (generated, erm, server-side) and something that displays the TLS
fingerprint of a website you enter. There's a user-triggered port scanner called
ShieldsUp, which might be interesting were it not for the fact that its port
list seems limited to the Windows RPC mapper and some items of that type...
things that were major concerns in the early '00s but rarely a practical
problem today.</p>
<p>It's full of some gems. Consider the password generator...</p>
<blockquote>
<p>What makes these perfect and safe?
Every one is completely random (maximum entropy) without any pattern, and the cryptographically-strong pseudo random number generator we use guarantees that no similar strings will ever be produced again.
Also, because this page will only allow itself to be displayed over a snoop-proof and proxy-proof high-security SSL connection, and it is marked as having expired back in 1999, this page which was custom generated just now for you will not be cached or visible to anyone else. 
...
The "Techie Details" section at the end describes exactly how these super-strong maximum-entropy passwords are generated (to satisfy the uber-geek inside you).</p>
</blockquote>
<p>You know I'm reading the Techie Details. They describe a straightforward
approach using AES in CBC mode, fed by a counter and its own output. It's
unremarkable except that just about any modern security professional would have
paroxysms at the fact that he seems to have implemented it himself.  Sure,
there are better methods (like AES CTR), but this is the kind of thing where
you shouldn't even really be using methods. "I read it from /dev/urandom" is a
far more reassuring explanation than a block diagram of cryptographic
primitives. /dev/urandom is a well-audited implementation, whatever is behind
your block diagram is not. Besides, it's server side!</p>
<p>My point is not so much to criticize Gibson's technical expertise, although I
certainly think you could, but to say that he doesn't seem to have updated his
website in some time. A lot of little details like references to WEP and the
fact that the PDFs are Corel Ventura output support this theory. By association,
I suspect that GRC's flagship product, SpinRite, doesn't get a lot of active
maintenance either.</p>
<p>Even back around 2007 when I first encountered SpinRite it was already a little
questionable, and I remember a rough internet consensus of "it likely doesn't
do anything but it probably doesn't hurt to try." A little research finds that
"is SpinRite snake oil?" threads date back to the Usenet era. It doesn't help
that Steve Gibson's writing is pervaded by a certain sort of... hucksterism.  A
sort of ceaseless self-promotion that internet users associate mostly with
travel influencers selling courses about how to make money as a travel
influencer.</p>
<p>But what does SpinRite even claim? After a charming disclaimer that GRC is
opposed to software patents but nonetheless involved in "extensive ongoing
patent acquisition" related to SpinRite, a document titled "SpinRite: What's
Under the Hood" gives some details. It's undated but has metadata pointing at
1998. That's rather vintage I see several reasons to think that there have been
few or no functional changes in SpinRite since that time.</p>
<p>SpinRite is a bootable tool based on FreeDOS. It originated as an interleaving
tool, which I won't really explain because it's quite irrelevant to modern
storage devices and really just a historic detail of SpinRite. It also
"introduc[ed] the concept of non-destructive low-level reformatting," which I
won't really explain because I don't know what it means, other than it seems to
fall into the broad category of no one really knowing what "low level
formatting" means. It's a particularly amusing example, because most modern
software vendors use "low level formatting" to refer explicitly to a
destructive process.</p>
<p>SpinRite "completely bypasses the system's motherboard BIOS software when used
on any standard hard disk system." I assume this means that SpinRite directly
issues ATA commands, which probably has some advantages, although the specific
ones the document calls out seem specious.</p>
<p>In reference to SpinRite's data recovery features, we read that "The DynaStat
system's statistical analysis capability frequently determines a sector's
correct data even when the data could never be read correctly from the mass
storage medium." This is what I remember as the key claim of SpinRite marketing
over a decade ago: that SpinRite would attempt rereading a block a very large
number of times and then determine on a bit-by-bit basis what the most likely
value is. It seems reasonable on the surface, but it wouldn't make much sense
with a drive with internal error correction. That's universal today but I'm not
sure how long that's been true, presumably in the late '90s this was a better
idea.</p>
<p>That's probably the high point of this document's credibility. Everything from
there gets more suspect. It claims that SpinRite has a proprietary system that
models the internal line coding used by "every existing proprietary" hard
drive, an unlikely claim in 1998 and an impossible one today without a massive
reverse engineering effort. Consider also "its second recovery strategy of
deliberately wiggling the drive's heads." It seems to achieve this by issuing
reads to cylinders on either side of the cylinder in question, but it's
questionable if that would even work in principle on a modern drive. You must
then consider the use of servo positioning on modern drives, which means that
the head will likely oscillate around the target cylinder before settling on it
anyway.</p>
<p>This gives the flavor of the central problem with SpinRite: it claims to
perform sophisticated analysis at a very low level of the drive's operation,
but it claims to do that with hard drives that intentionally abstract away
all of their low level details.</p>
<p>A lot of the document reads, to modern eyes, like pure flimflam, written by
someone who knew enough about HDDs to sound technical but not enough to really
understand the implications of what they were saying. The thing is, though,
this document is from '98 and the software was already a decade old at the
time! The document does note that SpinRite 3.0 was a complete rewrite, but I
suspect it was the last complete rewrite and probably carried a lot of its
functionality over from the first two versions.</p>
<p>I think that SpinRite probably does implement the functionality that it claims
and that those features <em>might</em> have been of some value in the late '80s and
much of the '90s. Then technology moved on and SpinRite became irrelevant.
Probably the only thing that SpinRite does of any value on a modern drive is
just rewriting the entire addressable area, which gives the controller an
opportunity to detect bad blocks and remap them. That should also happen in the
course of normal operation, though, and even tools dedicated to that purpose
(like the open-source badblocks) are becoming rather questionable in comparison
to the preemptive capabilities of modern HDDs. This type of bad-block-detecting
rewrite pass is probably only useful in pathological cases on older devices, but
it's also the only real claim of the vast majority of modern "hard drive repair"
software.</p>
<p>It seems a little mean-spirited to go after GRC for their old software, but
they continue to promote it at a cost of $89. The FAQ tells us that "SpinRite
is every bit as necessary today as it ever was — maybe even more so since
people store so much valuable personal 'media' data on today's massive drives."
I resent the implication of the scare-quoted "media," Mr. Gibson, but what I do
with my hard drives in my own home is none of your business.</p>
<p>The FAQ tells us "SpinRite is often credited with performing "true miracles" of
data recovery," but is oddly silent on the topic of SSDs. Some dedicated
Wikipedia editor rounded up a number of occasions on which Gibson said that
SpinRite was of limited or no use with SSDs, and yet the GRC website currently
includes the heading "Amazingly effective for SSDs!" There is no technical
explanation offered for how SpinRite's exceptionally platter-centric features
affect an SSD, nor mention of any new functionality targeting flash storage.
Instead, there's just anecdotal claims that SpinRite made SSDs faster and a
suggestion that the reader google a well-known behavior of flash storage for
which SSD controllers have considerable mitigations.</p>
<p>It is an odd detail of the GRC website that most of the new information about
the product is provided in the form of video. Specifically, videos excerpted
from recent episodes of Gibson and Laporte's podcast "Security Now." Security
Now is weekly, so I don't think that SpinRite promotional material makes up a
large portion of it, but it does seem conspicuous that Gibson uses the podcast
as a platform for 15 minute stories about how SpinRite worked miracles. These
segments, and their mentions of how SpinRite is a very powerful tool that one
shouldn't run on SSDs too often, absolutely reek of the promotional techniques
behind Orgone accumulators, Hulda Clark's "Zapper," and color therapy. It is,
it seems, quack medicine for the hard drive.</p>
<p>I don't think SpinRite started as a scam, but I sure think it ended as one.</p>
<p>A lot of this was already apparent back in the late '00s, and I can't honestly
say that bootleg copies of SpinRite every improved anything for me. So why did
I love it so much? The animations!</p>
<p>SpinRite's TUI was truly a work of art. <a href="https://youtu.be/Hm5LU2bslt0?t=105">Just watch it go!</a>.</p>
<p>[1] I recently bought the 20th anniversary vinyl box set of <em>Meteora</em>, which
emphasizes that (1) 20 years have passed and (2) I am still a loser.</p>
<p>[2] This kind of visible failure seems uncommon with SSDs, likely because SSD
controllers tend to read out the flash in a critical, suspicious way and take
preemptive action when the physical state is less than perfectly clear cut. In
a common type of engineering irony, the fact that flash storage is less
reliable than magnetic media requires aggressive management of the problem that
makes the overall system more reliable. Or at least that's what I tell myself
when another SSD has gone completely unresponsive.</p>
<p>[3] Honestly this doesn't seem to have been typical with any hard drives by the
microcomputer era, which makes perfect sense if you consider that these hard
drives were sold with bad sector lists and therefore must have been factory
tested. The whole "low level formatting" thing has been 70% a scam and 30%
confusion with the very different technical tradition of magnetic diskettes,
since probably 1990 at least.</p> ]]></description>
	</item>

 	<item>
		<title>2024-05-15 catalina connections</title>
		<pubDate>Wed, 15 May 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-05-15-catalina-connections.html</link>
		<guid>https://computer.rip/2024-05-15-catalina-connections.html</guid>
		<description><![CDATA[ <p>Some things have been made nearly impossible to search for. Say, for example,
the long-running partnership between Epson and Catalina: a query that will
return pages upon pages of people trying to use Epson printers with an old
version of MacOS.</p>
<p>When you think of a point of sale printer, you probably think of something like
the venerable Epson TM-T88. A direct thermal printer that heats small sections
of specially coated paper, causing it to turn black. Thermal paper of this type
is made in various widths, but the 80mm or 3 1/8" used by the TM-T88 is the
most common. The thermally-reactive coating on the paper incorporates some,
umm, questionable chemicals, but moreover, the durability of direct thermal
prints is poor. The image tends to fade over not that long of a timespan.
Besides, the need for special paper is an irritation.</p>
<p>So, there are other technologies available. Thermal transfer, in which a ribbon
of ink (I suspect actually a thermoplastic) is pressed against the paper and
heated to cause the ink to stick, is often used for more durability-sensitive
applications like warehouse labeling. The greater flexibility of paper (or
plastic) stock sees thermal transfer used in specialty applications as well,
like conference attendee badges. Thermal transfer printers tend to be more
expensive and more complex than direct thermal, though, and are rarely used at
the POS.</p>
<p>Impact printers are actually fairly common in a POS-adjacent application. These
printers punch metal pins against an inked ribbon, pushing it against the paper
to leave a mark. Impact printers were actually the norm for receipt printing
prior to the development of inexpensive thermal printers. They remain popular
in restaurant kitchens: the plain paper they use is less readily damaged by
oils, and won't turn entirely black if exposed to too much heat, as might
happen when a ticket is clipped above a grill. Impact receipt printers today
are often referred to as kitchen ticket printers as a result.</p>
<p>Impact receipt printers, and many impact printers in general, have a neat
trick: you can manufacture an ink ribbon in two colors, say, black on one half
and red on the other. By either using two sets of impact pins or shifting the
position of the impact head, either black or red can be printed. Dual-color
printers with black and red ribbons became ubiquitous for kitchen tickets,
although the red doesn't tend to reproduce well from an old, dry ribbon.</p>
<p>The ability of impact printers to use plain paper had another advantage: slip
printing. A slip printer is a device intended to print characters on a small
piece of paper inserted into it. Historically they were often used by bank
tellers to print account and reference numbers onto deposit slips, for later
auditing. In other applications they functioned as more sophisticated
"received" stamps, adding not just the time and date but customer account or
transaction numbers to received paperwork. The legal profession has a tradition
of "Bates numbering," which traces its history to a rather different printing
device, but Bates numbers could be applied by slip printers as well. In this
case, of course, we would need to refer to them as Generic Sequential Page
Numbers, Compare to Bates (TM).</p>
<p>A variant of the slip printer, really a receipt printer (often thermal) and
slip printer (often impact) married into one box, is known as a check
validator. Very common in grocery stores until recently, these printers both
produced receipts and printed an audit number and endorsement on the back of
the check a customer might offer in payment. It's difficult to imagine paying
for groceries with a check, but it used to be a common practice. For many
years, the practicalities of accepting checks were a major driver of POS
technology. When a cashier rung you up, there were two options: they pushed the
cash button, and the POS "bumped" the cash drawer open, or they pushed the
check button, and the POS sent an endorsement to the check validator. The
close coupling of these two features means that cash drawer bumping is
traditionally the task of the receipt printer, and cash bump outputs are
common to this day.</p>
<p>But where, exactly, is this tour of POS printing technology taking us? Well,
you might notice the absence of the humble inkjet. It might seem surprising:
inkjet mechanisms can actually be quite compact, and they tend to be a natural
evolution of impact printing. Well, there are indeed inkjet printers in the
receipt printer class, but there are some practical considerations. Moving a
smaller print head across the paper in bands requires a more complex mechanism,
and it's slow compared to printing in one pass. Inkjet heads large enough to
span the whole width of the receipt tape are fairly expensive.</p>
<p>And after all that, inkjet seems high maintenance compared to the almost
bulletproof reliability of direct thermal printers. Consider the state of the
average gas pump "CRIND" (Card Reader In Dispenser) receipt, and then consider
that the small thermal mechanism is still managing to produce that output after
many years in the harsh conditions of the outdoors. Inkjets tend to quickly
malfunction without some sort of automated mechanical cleaning, and that's
under office conditions.</p>
<p>So, to put it succinctly, inkjet receipt printers just aren't popular.</p>
<p>You could make similar comments about office printers, where inkjet suffers
in many ways when compared to laser or LED printers. But they have been a
tremendous success at the lower end of the market. There are a few reasons
for this outcome, but one of the bigger ones is color: for a laser or LED
printer to produce color used to be rather complicated. In the '00s, many
inexpensive color laser printers were "four-pass" printers: the page had to
be looped through the print engine four times, one for each color! It saved
a lot of parts but made printing more than four times slower. Inkjets were
far from this problem. It's a fairly simple matter to make an inkjet print
head that serves multiple colors in one assembly!</p>
<p>The same ideas are applicable to receipt printers. If you, for some reason,
want a full-color receipt, inkjet is the way to go. But no one wanted a
full-color receipt. Even dual-color impact printers disappeared into the
kitchen.</p>
<p>And then a company called Catalina came along. Catalina keeps a somewhat low
profile among consumers, certainly lower than the MacOS release. Search results
suggest lower even than the island off of Los Angeles, for which the company,
and the MacOS release, are named. There's no Wikipedia article about Catalina,
and their own About Us is brief and made up mostly of nonsense like this:</p>
<blockquote>
<p>Transforming data into insights, and insights into action through a seamless
consumer experience that drives results.</p>
</blockquote>
<p>Catalina is one of those companies that you never think about, but that is
constantly thinking about you. Today we would call it ad-tech.</p>
<p>Catalina is tough to research. Obviously they did not intentionally choose a
name that would become a MacOS release; they were using the Catalina name many
years earlier. But it does seem like they have participated in a bit of
obfuscation. Today, they continue to advertise a charming phone number:
1-800-8-COUPON. This "translates," of course, to 1-800-826-8766. During the
1990s they ran numerous classified ads using this phone number, but the numeric
version instead of the easier to remember "vanity" representation. The ads were
for advertising associate positions, but curiously did not mention the name of
the company at all.</p>
<p>Actually, some of these ads give a slightly different phone number,
1-800-826-8768. It is quite conceivable that both phone numbers were issued to
the company, given the different toll-free number industry of the '90s. But the
fact that OCR frequently confuses these two numbers leads one to suspect that
some of the 8768 ads may have been a copy mistake.</p>
<p>Even better, a few of the ads for the 8768 number, and one ad with the 8766
number, <em>do</em> give the name of a company, but an unfamiliar one: Aquarius
Enterprises.</p>
<p>Aquarius Enterprises was a "register tape advertising" or "receipt back
advertising" venture. In other words, they sold advertising on the backs of
receipts. Curiously, while Catalina mentions their 40-year history, Aquarius
Enterprises calls themselves "the most successful register tape advertising" for
"over 25 years"... in 1993. Are they the same company? Well, they used the same
phone number. Catalina is headquartered in St. Petersburg, Florida today, but
seems to have moved, as early articles describe then as Anaheim-based... rather
closer to the El Segundo address often used by Aquarius Enterprises.</p>
<p>Perhaps it is a coincidence of similar phone numbers and similar industries,
but I strongly suspect that Catalina was a spin-out of Aquarius Enterprises. I
tried finding shared employees, but there is remarkably little information
about Aquarius Enterprises outside of their classified ads for sales
associates. But then, once again, it's not an easy name to search for.</p>
<p>Whatever its origins, Catalina launched in 1985 with "Coupon $olutions."
Besides the cringeworthy name, this venture was remarkably similar to what
consumers will know them for today: Coupon $olutions consisted of software
that recorded a consumer's purchases at the POS, and then printed on-demand
targeted coupons.</p>
<p>Early articles about Catalina describe the system as relatively simple.
Coupons would be printed for "complimentary items." For example, the purchase
of baby food would result in an coupon for diapers. The coupons themselves were
also simple: printed in monochrome on tape with a distinctive printed edge.</p>
<p>Coupon $olutions debuted at two Boys markets in Los Angeles. It grew fast. By
1990, Catalina's coupon printers were installed in 3,300 grocery stores
nationwide. Newspaper coverage started to mention privacy concerns in the
1990s, waving them away with Catalina's assurances that there was no privacy
concern because they tracked only purchases and not the shopper's identity. Of
course, in the late '80s Catalina had trialed a shopper loyalty card program
that would rather change that situation, but it seems to have been
unsuccessful.</p>
<p>As time passed, Catalina expanded further into retail technology. They opened
their own clearinghouse service for coupons, and marketed their on-demand
coupon system to stores as an analytics product, since it provided real-time
reporting on purchases (in this era even large retailers would often not have
granular, fast reporting from their POS system).</p>
<p>The 1990s treated Catalina well, but they seem to have flown a little too
close to technology, and the dot com bust hit them as well. In the early '00s,
they weathered layoffs, an accounting probe, and a stock dive. Still, 2005
brought a big step forward: color.</p>
<p>Yes, we're finally back to the point. Catalina Marketing partnered with Epson
to introduce a special variant of the TM-C610 color receipt printer, called
the TM-C600. Called the CMC-6 by Catalina, the printer uses a full-width
inkjet head to produce 360 DPI full color on 57.5mm paper.</p>
<p>Lately, though, you may have noticed these printers yielding unsatisfactory
results. When I've gotten Checkout Coupons at all, they've been barely
legible or, increasingly, completely blank. Curious.</p>
<p>Catalina went bankrupt in 2018, and underwent a reorganization. The company
emerged, but apparently not by that much, as it went bankrupt once again in
2023. Catalina offers a fully managed service, meaning that they ship stores
new ink cartridges when remote monitoring of the printers indicates that it
will be needed. I have a suspicion that Catalina's second bankruptcy has
introduced some disruptions. And yet, in an article they claim:</p>
<blockquote>
<p>Catalina is assuring clients and shoppers that it’s still business as usual,
and ongoing promotions won’t be affected. “There will be no interruption in
Catalina’s ability to serve its customers or any impact on how it works with
them,” Catalina says.</p>
</blockquote>
<p>I'm not sure that this is working out, even a year into the bankruptcy process.
Safeway/Albertsons has apparently decided to remove the Catalina printers
entirely. Smith's (Kroger) doesn't seem to maintain them at all. Walgreens is
apparently more committed to the cause, as they are with the cooler screens,
but even there checkout coupons have become inconsistent.</p>
<p>Besides, I don't think even Catalina views the printers as very important any
more. They're relegated to a small corner of Catalina's website, with the vast
majority of their marketing material dedicated to analytics, targeting, and
digital marketing. Catalina seems to be a major player in the in-app digital
coupons now emphasized by a lot of grocers, although I've personally found
the system to be laughably unusable. But it's not surprising that you get a
laughably unusable app from an industry that churns out this kind of copy:</p>
<blockquote>
<p>84.51° currently delivers personalized promotional offers to Kroger’s
digitally engaged shoppers via its website, mobile app, and more broadly via
its Loyal Customer Mailer. Catalina Reach Extender is a complementary
solution to the way current offers are delivered and will expand the impact
of promotional offers by aligning those offers to the way customers shop –
in-store, online or both.</p>
</blockquote>
<p>As far as I can tell, this press release is just describing making digital
coupons (managed by a company that is, improbably, called 84.51°) also print
out on the Catalina printers. The ones that barely work any more. Well, that
was January of '23, they didn't know about the second bankruptcy yet.</p>
<p>Catalina may date to 1985, but it's sort of a case study in the advertising
industry. It's a huge, publicly traded company, with a market cap that's
reached at least $1.7 billion, and two bankruptcies. They write such obtuse
copy that it's hard to understand what exactly they do these days, which is
probably mainly a way to distract from the fact that their main business is now
collecting and selling consumer data. And I would say that no one likes them...
subreddits of retail employees are full of comments expressing relief when the
Catalina printers would break, since unplugging them would result in multiple
phone calls a day from Catalina investigating the "problem."</p>
<p>BUT: there are couponers.</p>
<p>That's right, there's a whole internet subculture that is obsessed with these
checkout coupons. They catalog the coupons on offer, and document the process
for requesting a replacement coupon from Catalina when the one you expected
failed to print. So very strange to me, a reminder of the many people out there
and their many strange hobbies.</p>
<p>Why would you ever waste your time on these coupons? I have real things to do,
like collecting thermal printers.</p> ]]></description>
	</item>

 	<item>
		<title>2024-05-06 matrix</title>
		<pubDate>Mon, 06 May 2024 00:00:00 GMT</pubDate>
		<link>https://computer.rip/2024-05-06-matrix.html</link>
		<guid>https://computer.rip/2024-05-06-matrix.html</guid>
		<description><![CDATA[ <p>For those of you who are members of the Matrix project, I wanted to let you
know that I am running for the Governing Board, and a bit about why. For those
of you who are not, I hope you will forgive the intrusion. Maybe you'll find my
opinions on the topic interesting anyway.</p>
<p>I am coming off of a period of intense involvement in an ill-fated government
commission, and I wanted to find another way to meaningfully contribute to the
governance of something I care about. Auspiciously, the newly constituted
Matrix foundation is forming a governing board. I am up for one of the
individual member seats.</p>
<h1>Why do I care?</h1>
<p>Instant messaging is a fascinating case study in the history of technology. It
is nearly as old as networked computing, and you could make a decent argument
that it is older, running only into dithering around the definitions. We've 
always wanted to communicate, and text has always been an obvious option. It
is probably because of the obviousness of instant messaging that it has
repeatedly been coopted by commercial interests.</p>
<p>You don't have to be very old to have lived through several iterations of this
process. I'm not quite the right person to remember ICQ fondly; for me it was
AIM. But what I remember most fondly is more obscure: XFire. It had an in-game
overlay <em>and</em> killcounter integration, both critical features for my computer
habits that consisted heavily of <em>Jedi Knight: Jedi Academy.</em> That isn't
actually important, I'm just reminiscing, but I think most people have a story
like this.</p>
<p>If you have read much of my back catalog you know that I am not always
optimistic about federated systems. They face a lot of challenges, which range
from the technical complexity of changing federated protocol specifications to
a whole category of opposing forces that can be vaguely chalked up as
capitalism. And yet, textual communications bring us what is probably
federation's greatest and most enduring success: email. Email is also a
cautionary tale in a lot of ways, but it gives us a cause for optimism.</p>
<p>The history of federated messaging is rather more varied. XMPP was, in its
heyday, nearly on track to mass adoption. High quality clients emerged, XMPP
was adopted by grassroots projects and then, as at least an implementation
detail, by Facebook and Google. We all know what happened. I think most people
today are too quick to blame XMPP's downfall on inconsistent implementation of
protocol extensions (XEPs) rather than complete cooption by two of the era's
largest internet companies, but to be clear, inconsistent implementation was
indeed a problem.</p>
<h1>Matrix and Me</h1>
<p>I have used Matrix as my main, day-to-day messaging solution since 2016. I have
also operated a homeserver with open registration for that entire span. In some
ways this has been a rather passive venture, but as the user count of that
homeserver has grown I've struggled more with performance and moderation
issues. A few months ago things tipped over and I had to spend a weekend doing
some serious work on both fronts. This lead me to pay a lot more attention to
the Matrix project and the state of the art.</p>
<p>I wish that I had been more involved in the Matrix project to date, but I try
very hard to avoid software engineering, and Matrix governance and community
efforts, the area that matters to me most, have often been hard for me to
follow. This situation has improved significantly recently, and I think that
the Matrix foundation deserves enormous credit for the work they have done to
pick up the level of community engagement.</p>
<p>Of course I come to the topic with some opinions. Who would expect anything
less?</p>
<h1>Polish over Features</h1>
<p>The Matrix project, especially as personified by Element, has added a huge
number of new features. It's hard to call this a bad thing, and some of them
have been notable successes. For example, E2E is a challenging feature to
deliver, but has indeed become table stakes for a messaging product that
attracts a privacy-minded userbase.</p>
<p>Still, there is one criticism of Matrix that has remained constant over its
entire lifespan, and its one that needs to be attended to: the level of
consistency, usability, and polish.</p>
<p>Polish is tricky in a federated system. It's more the domain of clients than
the protocol, but the protocol directly affects the situation by determining
how easy it is to develop and maintain high-quality clients. For many years it
was clear that the change rate in the Matrix protocol made it difficult to
develop a good client. Element often felt like the only complete client, and
even it was pretty rocky. Fortunately there has been a lot of progress; Element
has greatly improved and the stable of third-party clients like my own choice,
Nheko, has a lot to offer.</p>
<p>Still, there's a lot of progress to be made. Matrix competes directly with
commercial products that come from vendors with a heavy focus on usability and
user experience. It only takes one instance of the dreaded "Unable to decrypt"
for casual users to bounce. Element continues to be a de facto "primary"
implementation that can make the road more difficult for others.</p>
<p>I think that protocol changes should be evaluated conservatively, with an eye
towards providing a level of stability that enables multiple top-tier clients.
The Matrix Foundation should actively seek ways to support the enhancement and
maintenance of clients beyond Element, supporting the healthy ecosystem of
independent implementations that are required for an open protocol to be
sustainable.</p>
<h1>Moderation</h1>
<p>Moderation is one of the great struggles of the internet, if not the greatest.
Some advocates of federated systems opine that they make moderation easier or
more tractable. I disagree; while federation enables more flexibility in how
users experience moderation it makes many of the underlying problems more
difficult. Moderation decisions across the system are made in an ad-hoc,
distributed way. The rich network of homeservers presents many opportunities
for bad actors, including every poorly maintained (or unmaintained) node.</p>
<p>Matrix imposes a moderation challenge at two levels: within communities and
within homeservers. Relatively good tools exist at the community level, but
still, too many basic functions require introducing the Mjolnir moderation bot.
At the level of the homeserver, moderation tools are frustratingly limited.
The administration API is minimal in severely limiting ways and there do not
appear to be any complete implementations of a client for it.</p>
<p>I applaud the various efforts that have popped up, things like the community
moderation initiative's blocklist effort and the "awesome technologies"
Synapse administration tool. But we need more, and we need more in two ways.</p>
<p>First, we need technical progress. The in-protocol moderation capabilities of
Matrix should be improved over time with a north-star vision of eliminating
Mjolnir, an approach to community moderation that was carried over from IRC
but probably should have stayed there. The Synapse admin API should be
improved and better tooling around it developed.</p>
<p>Second, we need progress in governance. I would like to see an open initiative
to develop best practices for moderation of communities and homeservers. This
can include the development of shared blocklists through a documented,
auditable process (although not necessarily an open one, for reasons of user
privacy). I would like to see a sincere effort to advance the state of the art
in distributed moderation, bringing together diverse users to learn their
concern and developing tools to make consistent and active moderation the
default.</p>
<p>The number of independently operated homeservers in Matrix can be a strength,
but in this area it can be a weakness. ActivityPub, with its heavier
orientation towards public discussion, has served as a laboratory for abuse and
moderation issues. Matrix could learn a lot from the efforts going on in the
Mastodon community, for example, towards practical means of moderating across
instances.</p>
<p>For homeserver operators, moderation is an immense practical concern due to
risks from load and CSAM. The volume of CSAM traffic on Matrix, while not a
problem beyond solving, seems badly under-discussed and particularly calls for
some sort of distributed moderation program to relieve public homeserver
operators of ongoing whac-a-mole. Sometimes a graph is only as strong as its
weakest node---this is the kind of hard problem we have to take on to build a
sustainable future for federated systems, and we should take it on
enthusiastically.</p>
<p>I would like to see the Matrix project boldly take on moderation at multiple
levels. First, improving the moderation tools and capabilities of the Matrix
protocol should always be part of the discussion. Second, I would like to see
the Matrix Foundation support the development of improved moderation and abuse
tools, preferably including them as part of Synapse or providing a very easy
setup process so that good abuse management can be the norm rather than the
exception. Third, I would like the Matrix foundation to facilitate community
discussion around best practices, tools, and techniques for moderation.</p>
<p>Not everyone will agree on the way to perform moderation, or even the goals of
moderation. That's the nature of the internet, and more broadly of
communications. We can't let it stop us from trying. This can be one of the
hardest areas to build consensus, but that will always be the case, and so we
need to include the inherent social complexity of moderation as part of the
technical requirements. Once again: we need to be bold and take on the hard
problems, and this might be the hardest.</p>
<h1>Chat, First and Mostly</h1>
<p>One of the concerning trends I have seen in a lot of adjacent nonprofit tech
projects lately is dilution of mission. We could also call this "distractions."
Unfortunately, Matrix has not been immune. The most obvious example is <a href="https://thirdroom.io/preview">Third
Room</a>, the Matrix metaverse project. I want to
temper my criticism by saying that the level of effort devoted to Third Room
has evidently been low, but I think that the optical problem created by Third
Room (the appearance that Matrix has been capered, one might even say Zuck'd,
into a distracting focus on the latest trend) is certainly real. For a
community venture, appearances are important, and this means applying
discipline in how side projects are presented, especially in this era of so
many projects presaging their downfall with some buzzword-reaction initiative.</p>
<p>I might go just a bit further. I don't think that the VoIP features of Matrix
(voice and video communications) are a bad idea per se, but I think that that's
a complex problem space and the current landscape of instant messaging products
suggests that it's not a particularly important one. In other words, people
seem happy to do their voice/video chat in a different product than their text
chat. You could say that this presents an opportunity for Matrix: to double
down on providing a best-in-class textual messaging experience, without having
to expend significant resources on real-time media.</p>
<p>I wouldn't want to see existing features removed, but I think that features
other than core instant messaging should be deprioritized, at least in the
short term.</p>
<h1>Onboarding</h1>
<p>Sometimes caring a lot about onboarding can be kind of gross. It has the scent
of focusing on conversions. But it's a really important issue for IM, when the
onbarding experience of a lot of the other options is "you already have it."
The Matrix Foundation is well-positioned to demonstrate leadership in the
onbaording experience, across the protocol, clients, and public communications.
Let's make Matrix easy to get into.</p>
<h1>A Consistent Direction</h1>
<p>I don't want to dwell too long on how many times a certain prominent Matrix
client has been renamed, launched new App Store listings, etc. It's old news
and fortunately things seem to have settled down. Still, I think a lot of
reputational damage happened that has not fully been forgotten. This history
serves as a reminder that significant user-facing changes need to be made
carefully. New social applications in general, and especially federated ones,
have a bad reputation for churn. The most successful are often the most boring.
Let's think carefully about things, and look before we leap.</p>
<h1>What Do You Think?</h1>
<p>I have a lot of opinions and of course all of them are correct, but usually
only in my eccentric construction of reality. Your experience may vary. Please
feel free to reach out with your thought on the Matrix project, an offer that
stands whether I'm elected or not, because I love to talk about it.</p>
<p>And that concludes my stump speech. I'll be back again soon with a normal post
about some useless trivia. I think it might be about a specific kind of printer
that you've probably seen but not thought much about, other than slight
irritation. I'm also spending some time right now playing video games^w^w^w
working on a more ambitious writing project that is out of my normal lane but
you might still enjoy. It's about dogs. It's also very sad and I'm not entirely
sure what to think about it. You'll see what I mean if I ever finish.</p> ]]></description>
	</item>

</channel>
</rss> 